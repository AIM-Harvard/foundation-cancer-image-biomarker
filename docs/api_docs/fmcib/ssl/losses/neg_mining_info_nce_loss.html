<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fmcib.ssl.losses.neg_mining_info_nce_loss API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fmcib.ssl.losses.neg_mining_info_nce_loss</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Facebook, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import pprint

import numpy as np
import torch
from lightly.utils import dist
from loguru import logger
from torch import nn


class NegativeMiningInfoNCECriterion(nn.Module):
    &#34;&#34;&#34;
    The criterion corresponding to the SimCLR loss as defined in the paper
    https://arxiv.org/abs/2002.05709.

    Args:
        temperature (float): The temperature to be applied on the logits.
        buffer_params (dict): A dictionary containing the following keys:
            - world_size (int): Total number of trainers in training.
            - embedding_dim (int): Output dimensions of the features projects.
            - effective_batch_size (int): Total batch size used (includes positives).
    &#34;&#34;&#34;

    def __init__(
        self, embedding_dim, batch_size, world_size, gather_distributed=False, temperature: float = 0.1, balanced: bool = True
    ):
        &#34;&#34;&#34;
        Initialize the NegativeMiningInfoNCECriterion class.

        Args:
            embedding_dim (int): The dimension of the embedding space.
            batch_size (int): The size of the input batch.
            world_size (int): The number of distributed processes.
            gather_distributed (bool): Whether to gather distributed data.
            temperature (float): The temperature used in the computation.
            balanced (bool): Whether to use balanced sampling.

        Attributes:
            embedding_dim (int): The dimension of the embedding space.
            use_gpu (bool): Whether to use GPU for computations.
            temperature (float): The temperature used in the computation.
            num_pos (int): The number of positive samples.
            num_neg (int): The number of negative samples.
            criterion (nn.CrossEntropyLoss): The loss function.
            gather_distributed (bool): Whether to gather distributed data.
            world_size (int): The number of distributed processes.
            effective_batch_size (int): The effective batch size, taking into account world size and number of positive samples.
            pos_mask (None or Tensor): Mask for positive samples.
            neg_mask (None or Tensor): Mask for negative samples.
            balanced (bool): Whether to use balanced sampling.
            setup (bool): Whether the setup has been done.
        &#34;&#34;&#34;
        super(NegativeMiningInfoNCECriterion, self).__init__()
        self.embedding_dim = embedding_dim
        self.use_gpu = torch.cuda.is_available()
        self.temperature = temperature
        self.num_pos = 2

        # Same number of negatives as positives are loaded
        self.num_neg = self.num_pos
        self.criterion = nn.CrossEntropyLoss()
        self.gather_distributed = gather_distributed
        self.world_size = world_size
        self.effective_batch_size = batch_size * self.world_size * self.num_pos
        self.pos_mask = None
        self.neg_mask = None
        self.balanced = balanced
        self.setup = False

    def precompute_pos_neg_mask(self):
        &#34;&#34;&#34;
        Precompute the positive and negative masks to speed up the loss calculation.
        &#34;&#34;&#34;
        # computed once at the begining of training

        # total_images is x2 SimCLR Info-NCE loss
        # as we have negative samples for each positive sample

        total_images = self.effective_batch_size * self.num_neg
        world_size = self.world_size

        # Batch size computation is different from SimCLR paper
        batch_size = self.effective_batch_size // world_size
        orig_images = batch_size // self.num_pos
        rank = dist.rank()

        pos_mask = torch.zeros(batch_size * self.num_neg, total_images)
        neg_mask = torch.zeros(batch_size * self.num_neg, total_images)

        all_indices = np.arange(total_images)

        # Index for pairs of images (original + copy)
        pairs = orig_images * np.arange(self.num_pos)

        # Remove all indices associated with positive samples &amp; copies (for neg_mask)
        all_pos_members = []
        for _rank in range(world_size):
            all_pos_members += list(_rank * (batch_size * 2) + np.arange(batch_size))

        all_indices_pos_removed = np.delete(all_indices, all_pos_members)

        # Index of original positive images
        orig_members = torch.arange(orig_images)

        for anchor in np.arange(self.num_pos):
            for img_idx in range(orig_images):
                # delete_inds are spaced by batch_size for each rank as
                # all_indices_pos_removed (half of the indices) is deleted first
                delete_inds = batch_size * rank + img_idx + pairs
                neg_inds = torch.tensor(np.delete(all_indices_pos_removed, delete_inds)).long()
                neg_mask[anchor * orig_images + img_idx, neg_inds] = 1

            for pos in np.delete(np.arange(self.num_pos), anchor):
                # Pos_inds are spaced by batch_size * self.num_neg for each rank
                pos_inds = (batch_size * self.num_neg) * rank + pos * orig_images + orig_members
                pos_mask[
                    torch.arange(anchor * orig_images, (anchor + 1) * orig_images).long(),
                    pos_inds.long(),
                ] = 1

        self.pos_mask = pos_mask.cuda(non_blocking=True) if self.use_gpu else pos_mask
        self.neg_mask = neg_mask.cuda(non_blocking=True) if self.use_gpu else neg_mask

    def forward(self, out: torch.Tensor):
        &#34;&#34;&#34;
        Calculate the loss. Operates on embeddings tensor.
        &#34;&#34;&#34;
        if not self.setup:
            logger.info(f&#34;Running Negative Mining Info-NCE loss on Rank: {dist.rank()}&#34;)
            self.precompute_pos_neg_mask()
            self.setup = True

        pos0, pos1 = out[&#34;positive&#34;]
        neg0, neg1 = out[&#34;negative&#34;]
        embedding = torch.cat([pos0, pos1, neg0, neg1], dim=0)
        embedding = nn.functional.normalize(embedding, dim=1, p=2)
        assert embedding.ndim == 2
        assert embedding.shape[1] == int(self.embedding_dim)

        batch_size = embedding.shape[0]
        T = self.temperature
        num_pos = self.num_pos

        assert batch_size % num_pos == 0, &#34;Batch size should be divisible by num_pos&#34;
        assert batch_size == self.pos_mask.shape[0], &#34;Batch size should be equal to pos_mask shape&#34;

        # Step 1: gather all the embeddings. Shape example: 4096 x 128
        embeddings_buffer = self.gather_embeddings(embedding)

        # Step 2: matrix multiply: 64 x 128 with 4096 x 128 = 64 x 4096 and
        # divide by temperature.
        similarity = torch.exp(torch.mm(embedding, embeddings_buffer.t()) / T)

        pos = torch.sum(similarity * self.pos_mask, 1)
        neg = torch.sum(similarity * self.neg_mask, 1)

        # Ignore the negative samples as entries for loss calculation
        pos = pos[: (batch_size // 2)]
        neg = neg[: (batch_size // 2)]

        loss = -(torch.mean(torch.log(pos / (pos + neg))))
        return loss

    def __repr__(self):
        &#34;&#34;&#34;
        Return a string representation of the object.

        Returns:
            str: A formatted string representation of the object.

        Examples:
            The following example shows the string representation of the object:

            {
              &#39;name&#39;: &lt;object_name&gt;,
              &#39;temperature&#39;: &lt;temperature_value&gt;,
              &#39;num_negatives&#39;: &lt;num_negatives_value&gt;,
              &#39;num_pos&#39;: &lt;num_pos_value&gt;,
              &#39;dist_rank&#39;: &lt;dist_rank_value&gt;
            }

        Note:
            This function is intended to be used with the pprint module for pretty printing.
        &#34;&#34;&#34;
        num_negatives = self.effective_batch_size - 2
        T = self.temperature
        num_pos = self.num_pos
        repr_dict = {
            &#34;name&#34;: self._get_name(),
            &#34;temperature&#34;: T,
            &#34;num_negatives&#34;: num_negatives,
            &#34;num_pos&#34;: num_pos,
            &#34;dist_rank&#34;: dist.rank(),
        }
        return pprint.pformat(repr_dict, indent=2)

    def gather_embeddings(self, embedding: torch.Tensor):
        &#34;&#34;&#34;
        Do a gather over all embeddings, so we can compute the loss.
        Final shape is like: (batch_size * num_gpus) x embedding_dim
        &#34;&#34;&#34;
        if self.gather_distributed:
            embedding_gathered = torch.cat(dist.gather(embedding), 0)
        else:
            embedding_gathered = embedding
        return embedding_gathered</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion"><code class="flex name class">
<span>class <span class="ident">NegativeMiningInfoNCECriterion</span></span>
<span>(</span><span>embedding_dim, batch_size, world_size, gather_distributed=False, temperature: float = 0.1, balanced: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>The criterion corresponding to the SimCLR loss as defined in the paper
<a href="https://arxiv.org/abs/2002.05709.">https://arxiv.org/abs/2002.05709.</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code></dt>
<dd>The temperature to be applied on the logits.</dd>
<dt><strong><code>buffer_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary containing the following keys:
- world_size (int): Total number of trainers in training.
- embedding_dim (int): Output dimensions of the features projects.
- effective_batch_size (int): Total batch size used (includes positives).</dd>
</dl>
<p>Initialize the NegativeMiningInfoNCECriterion class.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the embedding space.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The size of the input batch.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of distributed processes.</dd>
<dt><strong><code>gather_distributed</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to gather distributed data.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code></dt>
<dd>The temperature used in the computation.</dd>
<dt><strong><code>balanced</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use balanced sampling.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the embedding space.</dd>
<dt><strong><code>use_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use GPU for computations.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code></dt>
<dd>The temperature used in the computation.</dd>
<dt><strong><code>num_pos</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of positive samples.</dd>
<dt><strong><code>num_neg</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of negative samples.</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>nn.CrossEntropyLoss</code></dt>
<dd>The loss function.</dd>
<dt><strong><code>gather_distributed</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to gather distributed data.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of distributed processes.</dd>
<dt><strong><code>effective_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The effective batch size, taking into account world size and number of positive samples.</dd>
<dt><strong><code>pos_mask</code></strong> :&ensp;<code>None</code> or <code>Tensor</code></dt>
<dd>Mask for positive samples.</dd>
<dt><strong><code>neg_mask</code></strong> :&ensp;<code>None</code> or <code>Tensor</code></dt>
<dd>Mask for negative samples.</dd>
<dt><strong><code>balanced</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use balanced sampling.</dd>
<dt><strong><code>setup</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the setup has been done.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NegativeMiningInfoNCECriterion(nn.Module):
    &#34;&#34;&#34;
    The criterion corresponding to the SimCLR loss as defined in the paper
    https://arxiv.org/abs/2002.05709.

    Args:
        temperature (float): The temperature to be applied on the logits.
        buffer_params (dict): A dictionary containing the following keys:
            - world_size (int): Total number of trainers in training.
            - embedding_dim (int): Output dimensions of the features projects.
            - effective_batch_size (int): Total batch size used (includes positives).
    &#34;&#34;&#34;

    def __init__(
        self, embedding_dim, batch_size, world_size, gather_distributed=False, temperature: float = 0.1, balanced: bool = True
    ):
        &#34;&#34;&#34;
        Initialize the NegativeMiningInfoNCECriterion class.

        Args:
            embedding_dim (int): The dimension of the embedding space.
            batch_size (int): The size of the input batch.
            world_size (int): The number of distributed processes.
            gather_distributed (bool): Whether to gather distributed data.
            temperature (float): The temperature used in the computation.
            balanced (bool): Whether to use balanced sampling.

        Attributes:
            embedding_dim (int): The dimension of the embedding space.
            use_gpu (bool): Whether to use GPU for computations.
            temperature (float): The temperature used in the computation.
            num_pos (int): The number of positive samples.
            num_neg (int): The number of negative samples.
            criterion (nn.CrossEntropyLoss): The loss function.
            gather_distributed (bool): Whether to gather distributed data.
            world_size (int): The number of distributed processes.
            effective_batch_size (int): The effective batch size, taking into account world size and number of positive samples.
            pos_mask (None or Tensor): Mask for positive samples.
            neg_mask (None or Tensor): Mask for negative samples.
            balanced (bool): Whether to use balanced sampling.
            setup (bool): Whether the setup has been done.
        &#34;&#34;&#34;
        super(NegativeMiningInfoNCECriterion, self).__init__()
        self.embedding_dim = embedding_dim
        self.use_gpu = torch.cuda.is_available()
        self.temperature = temperature
        self.num_pos = 2

        # Same number of negatives as positives are loaded
        self.num_neg = self.num_pos
        self.criterion = nn.CrossEntropyLoss()
        self.gather_distributed = gather_distributed
        self.world_size = world_size
        self.effective_batch_size = batch_size * self.world_size * self.num_pos
        self.pos_mask = None
        self.neg_mask = None
        self.balanced = balanced
        self.setup = False

    def precompute_pos_neg_mask(self):
        &#34;&#34;&#34;
        Precompute the positive and negative masks to speed up the loss calculation.
        &#34;&#34;&#34;
        # computed once at the begining of training

        # total_images is x2 SimCLR Info-NCE loss
        # as we have negative samples for each positive sample

        total_images = self.effective_batch_size * self.num_neg
        world_size = self.world_size

        # Batch size computation is different from SimCLR paper
        batch_size = self.effective_batch_size // world_size
        orig_images = batch_size // self.num_pos
        rank = dist.rank()

        pos_mask = torch.zeros(batch_size * self.num_neg, total_images)
        neg_mask = torch.zeros(batch_size * self.num_neg, total_images)

        all_indices = np.arange(total_images)

        # Index for pairs of images (original + copy)
        pairs = orig_images * np.arange(self.num_pos)

        # Remove all indices associated with positive samples &amp; copies (for neg_mask)
        all_pos_members = []
        for _rank in range(world_size):
            all_pos_members += list(_rank * (batch_size * 2) + np.arange(batch_size))

        all_indices_pos_removed = np.delete(all_indices, all_pos_members)

        # Index of original positive images
        orig_members = torch.arange(orig_images)

        for anchor in np.arange(self.num_pos):
            for img_idx in range(orig_images):
                # delete_inds are spaced by batch_size for each rank as
                # all_indices_pos_removed (half of the indices) is deleted first
                delete_inds = batch_size * rank + img_idx + pairs
                neg_inds = torch.tensor(np.delete(all_indices_pos_removed, delete_inds)).long()
                neg_mask[anchor * orig_images + img_idx, neg_inds] = 1

            for pos in np.delete(np.arange(self.num_pos), anchor):
                # Pos_inds are spaced by batch_size * self.num_neg for each rank
                pos_inds = (batch_size * self.num_neg) * rank + pos * orig_images + orig_members
                pos_mask[
                    torch.arange(anchor * orig_images, (anchor + 1) * orig_images).long(),
                    pos_inds.long(),
                ] = 1

        self.pos_mask = pos_mask.cuda(non_blocking=True) if self.use_gpu else pos_mask
        self.neg_mask = neg_mask.cuda(non_blocking=True) if self.use_gpu else neg_mask

    def forward(self, out: torch.Tensor):
        &#34;&#34;&#34;
        Calculate the loss. Operates on embeddings tensor.
        &#34;&#34;&#34;
        if not self.setup:
            logger.info(f&#34;Running Negative Mining Info-NCE loss on Rank: {dist.rank()}&#34;)
            self.precompute_pos_neg_mask()
            self.setup = True

        pos0, pos1 = out[&#34;positive&#34;]
        neg0, neg1 = out[&#34;negative&#34;]
        embedding = torch.cat([pos0, pos1, neg0, neg1], dim=0)
        embedding = nn.functional.normalize(embedding, dim=1, p=2)
        assert embedding.ndim == 2
        assert embedding.shape[1] == int(self.embedding_dim)

        batch_size = embedding.shape[0]
        T = self.temperature
        num_pos = self.num_pos

        assert batch_size % num_pos == 0, &#34;Batch size should be divisible by num_pos&#34;
        assert batch_size == self.pos_mask.shape[0], &#34;Batch size should be equal to pos_mask shape&#34;

        # Step 1: gather all the embeddings. Shape example: 4096 x 128
        embeddings_buffer = self.gather_embeddings(embedding)

        # Step 2: matrix multiply: 64 x 128 with 4096 x 128 = 64 x 4096 and
        # divide by temperature.
        similarity = torch.exp(torch.mm(embedding, embeddings_buffer.t()) / T)

        pos = torch.sum(similarity * self.pos_mask, 1)
        neg = torch.sum(similarity * self.neg_mask, 1)

        # Ignore the negative samples as entries for loss calculation
        pos = pos[: (batch_size // 2)]
        neg = neg[: (batch_size // 2)]

        loss = -(torch.mean(torch.log(pos / (pos + neg))))
        return loss

    def __repr__(self):
        &#34;&#34;&#34;
        Return a string representation of the object.

        Returns:
            str: A formatted string representation of the object.

        Examples:
            The following example shows the string representation of the object:

            {
              &#39;name&#39;: &lt;object_name&gt;,
              &#39;temperature&#39;: &lt;temperature_value&gt;,
              &#39;num_negatives&#39;: &lt;num_negatives_value&gt;,
              &#39;num_pos&#39;: &lt;num_pos_value&gt;,
              &#39;dist_rank&#39;: &lt;dist_rank_value&gt;
            }

        Note:
            This function is intended to be used with the pprint module for pretty printing.
        &#34;&#34;&#34;
        num_negatives = self.effective_batch_size - 2
        T = self.temperature
        num_pos = self.num_pos
        repr_dict = {
            &#34;name&#34;: self._get_name(),
            &#34;temperature&#34;: T,
            &#34;num_negatives&#34;: num_negatives,
            &#34;num_pos&#34;: num_pos,
            &#34;dist_rank&#34;: dist.rank(),
        }
        return pprint.pformat(repr_dict, indent=2)

    def gather_embeddings(self, embedding: torch.Tensor):
        &#34;&#34;&#34;
        Do a gather over all embeddings, so we can compute the loss.
        Final shape is like: (batch_size * num_gpus) x embedding_dim
        &#34;&#34;&#34;
        if self.gather_distributed:
            embedding_gathered = torch.cat(dist.gather(embedding), 0)
        else:
            embedding_gathered = embedding
        return embedding_gathered</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, out: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the loss. Operates on embeddings tensor.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, out: torch.Tensor):
    &#34;&#34;&#34;
    Calculate the loss. Operates on embeddings tensor.
    &#34;&#34;&#34;
    if not self.setup:
        logger.info(f&#34;Running Negative Mining Info-NCE loss on Rank: {dist.rank()}&#34;)
        self.precompute_pos_neg_mask()
        self.setup = True

    pos0, pos1 = out[&#34;positive&#34;]
    neg0, neg1 = out[&#34;negative&#34;]
    embedding = torch.cat([pos0, pos1, neg0, neg1], dim=0)
    embedding = nn.functional.normalize(embedding, dim=1, p=2)
    assert embedding.ndim == 2
    assert embedding.shape[1] == int(self.embedding_dim)

    batch_size = embedding.shape[0]
    T = self.temperature
    num_pos = self.num_pos

    assert batch_size % num_pos == 0, &#34;Batch size should be divisible by num_pos&#34;
    assert batch_size == self.pos_mask.shape[0], &#34;Batch size should be equal to pos_mask shape&#34;

    # Step 1: gather all the embeddings. Shape example: 4096 x 128
    embeddings_buffer = self.gather_embeddings(embedding)

    # Step 2: matrix multiply: 64 x 128 with 4096 x 128 = 64 x 4096 and
    # divide by temperature.
    similarity = torch.exp(torch.mm(embedding, embeddings_buffer.t()) / T)

    pos = torch.sum(similarity * self.pos_mask, 1)
    neg = torch.sum(similarity * self.neg_mask, 1)

    # Ignore the negative samples as entries for loss calculation
    pos = pos[: (batch_size // 2)]
    neg = neg[: (batch_size // 2)]

    loss = -(torch.mean(torch.log(pos / (pos + neg))))
    return loss</code></pre>
</details>
</dd>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.gather_embeddings"><code class="name flex">
<span>def <span class="ident">gather_embeddings</span></span>(<span>self, embedding: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Do a gather over all embeddings, so we can compute the loss.
Final shape is like: (batch_size * num_gpus) x embedding_dim</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather_embeddings(self, embedding: torch.Tensor):
    &#34;&#34;&#34;
    Do a gather over all embeddings, so we can compute the loss.
    Final shape is like: (batch_size * num_gpus) x embedding_dim
    &#34;&#34;&#34;
    if self.gather_distributed:
        embedding_gathered = torch.cat(dist.gather(embedding), 0)
    else:
        embedding_gathered = embedding
    return embedding_gathered</code></pre>
</details>
</dd>
<dt id="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.precompute_pos_neg_mask"><code class="name flex">
<span>def <span class="ident">precompute_pos_neg_mask</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Precompute the positive and negative masks to speed up the loss calculation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def precompute_pos_neg_mask(self):
    &#34;&#34;&#34;
    Precompute the positive and negative masks to speed up the loss calculation.
    &#34;&#34;&#34;
    # computed once at the begining of training

    # total_images is x2 SimCLR Info-NCE loss
    # as we have negative samples for each positive sample

    total_images = self.effective_batch_size * self.num_neg
    world_size = self.world_size

    # Batch size computation is different from SimCLR paper
    batch_size = self.effective_batch_size // world_size
    orig_images = batch_size // self.num_pos
    rank = dist.rank()

    pos_mask = torch.zeros(batch_size * self.num_neg, total_images)
    neg_mask = torch.zeros(batch_size * self.num_neg, total_images)

    all_indices = np.arange(total_images)

    # Index for pairs of images (original + copy)
    pairs = orig_images * np.arange(self.num_pos)

    # Remove all indices associated with positive samples &amp; copies (for neg_mask)
    all_pos_members = []
    for _rank in range(world_size):
        all_pos_members += list(_rank * (batch_size * 2) + np.arange(batch_size))

    all_indices_pos_removed = np.delete(all_indices, all_pos_members)

    # Index of original positive images
    orig_members = torch.arange(orig_images)

    for anchor in np.arange(self.num_pos):
        for img_idx in range(orig_images):
            # delete_inds are spaced by batch_size for each rank as
            # all_indices_pos_removed (half of the indices) is deleted first
            delete_inds = batch_size * rank + img_idx + pairs
            neg_inds = torch.tensor(np.delete(all_indices_pos_removed, delete_inds)).long()
            neg_mask[anchor * orig_images + img_idx, neg_inds] = 1

        for pos in np.delete(np.arange(self.num_pos), anchor):
            # Pos_inds are spaced by batch_size * self.num_neg for each rank
            pos_inds = (batch_size * self.num_neg) * rank + pos * orig_images + orig_members
            pos_mask[
                torch.arange(anchor * orig_images, (anchor + 1) * orig_images).long(),
                pos_inds.long(),
            ] = 1

    self.pos_mask = pos_mask.cuda(non_blocking=True) if self.use_gpu else pos_mask
    self.neg_mask = neg_mask.cuda(non_blocking=True) if self.use_gpu else neg_mask</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fmcib.ssl.losses" href="index.html">fmcib.ssl.losses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion">NegativeMiningInfoNCECriterion</a></code></h4>
<ul class="">
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.call_super_init" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.call_super_init">call_super_init</a></code></li>
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.dump_patches" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.dump_patches">dump_patches</a></code></li>
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.forward" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.forward">forward</a></code></li>
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.gather_embeddings" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.gather_embeddings">gather_embeddings</a></code></li>
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.precompute_pos_neg_mask" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.precompute_pos_neg_mask">precompute_pos_neg_mask</a></code></li>
<li><code><a title="fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.training" href="#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>