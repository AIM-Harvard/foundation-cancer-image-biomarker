{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation-walkthrough","title":"Documentation Walkthrough","text":"<p>Getting Started This section of the documentation explains how you can install our package and get started with a working example. </p> <p>Note</p> <p>We also provide quickstart examples that run in a free-cloud based environment (through Google Colab) so you can get familiar with our workflows, without having to download anything on your local machine!!</p> <p>Replication Guide If you would like to pre-train a foundation model on your own unannotated data or would like to replicate the training and evaluation from our study, see here. </p> <p>Tutorials We provide comprehensive tutorials that use the foundation model for cancer imaging biomarkers and compare against other popularly used methods. If you would like to build your own study using our foundation model, these set of tutorials are highly recommended as the starting point. </p> <p>API Docs  This is for the more advanced user who would like to deep-dive into different methods and classes provided by our package. </p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the <code>MIT</code> license.  See LICENSE for more details.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Code development, testing, and documentation: Suraj Pai, Ibrahim Hadzic  Framework used for building the code: project-lighter</p> <p>project-lighter was developed internally within our lab by Ibrahim Hadzic and Suraj Pai. </p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The code and data of this repository are provided to promote reproducible research. They are not intended for clinical care or commercial use. The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.</p>"},{"location":"#code-citation","title":"Code Citation","text":"<p>Please cite the  DOIs if you use our code in your project. </p> <pre><code>@software{suraj_pai_2024_10535536,\n  author       = {Suraj Pai and\n                  Ibrahim Hadzic},\n  title        = {{AIM-Harvard/foundation-cancer-image-biomarker: \n                   v0.0.1}},\n  month        = jan,\n  year         = 2024,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.10535536},\n  url          = {https://doi.org/10.5281/zenodo.10535536}\n}\n</code></pre>"},{"location":"getting-started/cloud-quick-start/","title":"Cloud Quick Start w Google Colab","text":"<p>We provide two notebooks that can be run in google colab to promote the use of our model by the community,</p> <ol> <li> <p>A reproducibility notebook that allows users to recreate our analysis for NSCLC prognostication, from downloading the data to obtaining results matching our manuscript.  </p> </li> <li> <p>A bring your own use case notebook that shows you how you can use the foundation model for your data.  </p> </li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Our package is offered through a very simple pip install, you can run: <pre><code>pip install foundation-cancer-image-biomarker\n</code></pre> The package provides simple ways to interact with the foundation model through the means of several utility scripts and functions. </p> <p>If you would like to install the bleeding edge version, please add  <pre><code>pip install foundation-cancer-image-biomarker --pre\n</code></pre></p> <p>Once you have installed the package, you can move to our Quick Start guide.</p>"},{"location":"getting-started/installation/#notes","title":"Notes","text":"<p>We recommend using Python 3.8 on a Linux machine since that is the environment we have tested on. However, we use Poetry to manage our dependencies which should make it compatible with Python versions above 3.8 on all platforms. We suggest using Conda to create an isolated virtual environment for the project dependencies. To download Conda, follow the instructions here: https://conda.io/projects/conda/en/latest/user-guide/install/index.html</p>"},{"location":"getting-started/installation/#hardware","title":"Hardware","text":"<p>The <code>foundation-cancer-image-biomarker</code> package can operate on a standard computer hardware. For using our foundation models for inference, which should be most of the use-cases, the system requirements are minimal as the batch size can be reduced to accomodate low memory sizes. We expect the user to have atleast 4GB of RAM and 4-cores to ensure smooth operation. A GPU with atleast 4GB of RAM will also be a plus as the inference can be ofloaded onto the GPU. </p> <p>For reproducing the training, we recommend a system with atleast 12 GB GPU VRAM, 4+ cores and 12 GB RAM. Note that batch-size, mixed-precision and number of workers can be adjusted to fit several training constraints. </p>"},{"location":"getting-started/installation/#software","title":"Software","text":"<p>We have tested this system on Ubuntu 20.04 and 22.04. We expect it to work on Widows and Mac systems in inference mode without any hinderance. For reproducing the training, we support only Linux systems due to CUDA and Pytorch setups. </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":""},{"location":"getting-started/quick-start/#extracting-features-from-the-foundation-model-recommended","title":"Extracting Features from the Foundation Model (Recommended)","text":"<p>Step 1: Install all our dependencies:</p> <pre><code>pip install foundation-cancer-image-biomarker\n</code></pre> <p>Fore more info: See detailed Install instructions</p> <p>Step 2:  Generate a CSV file with the path to your images and seed points (in physical coordinates),</p> image_path coordX coordY coordZ /content/data/dummy_0.nii.gz 55.0 119.0 27.0 <p>Step 3:  Run this in your code environment, <pre><code>from fmcib.run import get_features\n\nfeature_df = get_features(\"csv_path_here\")\n</code></pre></p> <p>This will preprocess your data, download our pre-trained model and execute inference on whatever device you have (CPU/GPU) and return a dataframe with the features.</p> <p>Note</p> <p>By default, the weights are downloaded in the current working directory and are named <code>model_weights.torch</code>. You can use these downloaded weights for all future uses. If you would like to run this script in a different location and do not want to download the weights, simply copy or symlink the weights!</p> <p>If you want to download the weights separately, you can run download it from here and place it in the current working directory.</p> <p>You can test to check if the global coordinates are where you expect by using our provided <code>visualize_seed_point</code> function.  We expect the coordinates in the LPS coordinate system (like ITK) but if you have it in RAS, you can negate the X and Y coordinates and that should work with our system. See here</p> <pre><code>from fmcib.visualization.verify_io import visualize_seed_point\nimport pandas as pd\n\nfeature_df = pd.read_csv(\"csv_path_here\")\nvisualize_seed_point(feature_df.iloc[0]) # Visualize annotations on the 0th row. \n</code></pre>"},{"location":"getting-started/quick-start/#fine-tuning-the-foundation-model","title":"Fine-tuning the Foundation Model","text":"<p>There are several different ways you can fine-tune the provided foundation model.</p>"},{"location":"getting-started/quick-start/#using-your-own-fine-tuning-pipeline","title":"Using your own fine-tuning pipeline","text":"<p>If you have your own fine-tuning pipeline and would like to use the model along with its weights, you can access the model with loaded weights as below,</p> <pre><code>from fmcib.models import fmcib_model \n\nmodel = fmcib_model()\n</code></pre> <p>Note that for best performance, using similar data-preprocessing methods are recommended, these can be accessed using, </p> <p><pre><code>from fmcib.preprocessing import preprocess\nfrom fmcib.models import fmcib_model\nimport pandas as pd\n\nmodel = fmcib_model(eval_mode=False) # By default the model is in eval mode. Set to false if you want to train it \n\ndf = pd.read_csv(\"csv_path_here\")\n\nimage = preprocess(df.iloc[0])\nout = model(image)\n</code></pre> The above shows a full pipeline of using the transforms to get the image in the expected pre-processed input to the model. The CSV format is the same as expected for the feature extraction as shown above. </p>"},{"location":"getting-started/quick-start/#using-project-lighter-to-fine-tune-the-fm","title":"Using <code>project-lighter</code> to fine-tune the FM","text":"<p>This is the easiest way to fine-tune the FM on your data. First, begin by preparing the CSV in the format similar to the feature extraction. You might have noticed by now that this format is followed across the study. Once you have this CSV, you can follow the instructions for adapting the FM in the replication guide as shown here. Instead of using the csv path from the specific datasets in the study, provide your own csv path. </p>"},{"location":"getting-started/repository-structure/","title":"Repository structure","text":""},{"location":"getting-started/repository-structure/#repository-structure","title":"Repository Structure","text":"<p>This code repository includes the Python package containing all our source code: <code>fmcib</code>, as well as the code to reproduce experiments and analyses presented in our paper. The pipelines to reproduce our experiments and analyses are independent of the python package and will be shared to the public upon publication.</p> <p>Here is the structure of the repository:</p> <pre><code>\u251c\u2500\u2500 fmcib/          # The main Python package\n\u251c\u2500\u2500 data/           # Downloading and preprocessing data\n\u251c\u2500\u2500 experiments/    # Reproduce paper experiments\n\u251c\u2500\u2500 models/         # Download and test final models\n\u251c\u2500\u2500 outputs/        # Outputs from pipelines in the study\n\u251c\u2500\u2500 analysis/       # Reproducible analysis for statistics\n\u251c\u2500\u2500 additional_requirements.txt\n\u2514\u2500\u2500 README.pdf\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fmcib<ul> <li>run</li> <li>callbacks<ul> <li>utils</li> <li>prediction_saver</li> </ul> </li> <li>datasets<ul> <li>ssl_radiomics_dataset</li> <li>utils</li> </ul> </li> <li>optimizers<ul> <li>lars</li> </ul> </li> <li>models<ul> <li>load_model</li> <li>models_genesis</li> <li>autoencoder</li> </ul> </li> <li>utils<ul> <li>download_utils</li> <li>idc_helper</li> </ul> </li> <li>ssl<ul> <li>losses<ul> <li>ntxent_mined_loss</li> <li>ntxent_loss</li> <li>neg_mining_info_nce_loss</li> <li>swav_loss</li> <li>nnclr_loss</li> </ul> </li> <li>modules<ul> <li>exneg_simclr</li> <li>swav</li> <li>simclr</li> <li>nnclr</li> </ul> </li> </ul> </li> <li>transforms<ul> <li>multicrop</li> <li>duplicate</li> <li>med3d</li> <li>random_resized_crop</li> </ul> </li> <li>visualization<ul> <li>verify_io</li> </ul> </li> <li>preprocessing<ul> <li>seed_based_crop</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/run/","title":"run","text":""},{"location":"reference/run/#fmcib.run.get_features","title":"<code>get_features(csv_path, weights_path=None, spatial_size=(50, 50, 50), precropped=False, **kwargs)</code>","text":"<p>Extracts features from images specified in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the CSV file containing image paths.</p> required <code>weights_path</code> <code>str</code> <p>Path to the pre-trained weights file. Default is None.</p> <code>None</code> <code>spatial_size</code> <code>tuple</code> <p>Spatial size of the input images. Default is (50, 50, 50).</p> <code>(50, 50, 50)</code> <code>precropped</code> <code>bool</code> <p>Whether the images are already pre-cropped. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments to be passed to the dataloader.</p> <code>{}</code> <p>Returns:     pandas.DataFrame: DataFrame containing the original data from the CSV file along with the extracted features.</p> Source code in <code>fmcib/run.py</code> <pre><code>def get_features(csv_path, weights_path=None, spatial_size=(50, 50, 50), precropped=False, **kwargs):\n    \"\"\"\n    Extracts features from images specified in a CSV file.\n\n    Args:\n        csv_path (str): Path to the CSV file containing image paths.\n        weights_path (str, optional): Path to the pre-trained weights file. Default is None.\n        spatial_size (tuple, optional): Spatial size of the input images. Default is (50, 50, 50).\n        precropped (bool, optional): Whether the images are already pre-cropped. Default is False.\n        **kwargs: Additional arguments to be passed to the dataloader.\n    Returns:\n        pandas.DataFrame: DataFrame containing the original data from the CSV file along with the extracted features.\n    \"\"\"\n    logger.info(\"Loading CSV file ...\")\n    df = pd.read_csv(csv_path)\n    dataloader = get_dataloader(csv_path, spatial_size=spatial_size, precropped=precropped, **kwargs)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    if weights_path is None:\n        model = fmcib_model().to(device)\n    else:\n        logger.warning(\n            \"Loading custom model provided from weights file. If this is not intended, please do not provide the weights_path argument.\"\n        )\n        trunk = resnet50(\n            pretrained=False,\n            n_input_channels=1,\n            widen_factor=2,\n            conv1_t_stride=2,\n            feed_forward=False,\n            bias_downsample=True,\n        )\n        model = LoadModel(trunk=trunk, weights_path=weights_path).to(device)\n\n    feature_list = []\n    logger.info(\"Running inference over batches ...\")\n\n    model.eval()\n    for batch in tqdm(dataloader, total=len(dataloader)):\n        feature = model(batch.to(device)).detach().cpu().numpy()\n        feature_list.append(feature)\n\n    features = np.concatenate(feature_list, axis=0)\n    # Flatten features into a list\n    features = features.reshape(-1, 4096)\n\n    # Add the features to the dataframe\n    df = pd.concat([df, pd.DataFrame(features, columns=[f\"pred_{idx}\" for idx in range(4096)])], axis=1)\n    return df\n</code></pre>"},{"location":"reference/callbacks/","title":"callbacks","text":""},{"location":"reference/callbacks/prediction_saver/","title":"prediction_saver","text":""},{"location":"reference/callbacks/prediction_saver/#fmcib.callbacks.prediction_saver.SavePredictions","title":"<code>SavePredictions</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>A class that saves model predictions.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to save the output CSV file.</p> <code>save_preview_samples</code> <code>bool</code> <p>If True, save preview images.</p> <code>keys</code> <code>List[str]</code> <p>A list of keys.</p> Source code in <code>fmcib/callbacks/prediction_saver.py</code> <pre><code>class SavePredictions(BasePredictionWriter):\n    \"\"\"\n    A class that saves model predictions.\n\n    Attributes:\n        path (str): The path to save the output CSV file.\n        save_preview_samples (bool): If True, save preview images.\n        keys (List[str]): A list of keys.\n    \"\"\"\n\n    def __init__(self, path: str, save_preview_samples: bool = False, keys: List[str] = None):\n        \"\"\"\n        Initialize an instance of the class.\n\n        Args:\n            path (str): The path to save the output CSV file.\n            save_preview_samples (bool, optional): A flag indicating whether to save preview samples. Defaults to False.\n            keys (List[str], optional): A list of keys. Defaults to None.\n\n        Raises:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__(\"epoch\")\n        self.output_csv = Path(path)\n        self.keys = keys\n        self.save_preview_samples = save_preview_samples\n        self.output_csv.parent.mkdir(parents=True, exist_ok=True)\n\n    def save_preview_image(self, data, tag):\n        \"\"\"\n        Save a preview image to a specified directory.\n\n        Args:\n            self (object): The object calling the function. (self in Python)\n            data (tuple): A tuple containing the image data and its corresponding tag.\n            tag (str): The tag for the image.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        self.output_dir = self.output_csv.parent / f\"previews_{self.output_csv.stem}\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        image, _ = data\n        image = handle_image(image)\n        fp = self.output_dir / f\"{tag}.png\"\n        torchvision.utils.save_image(image, fp)\n\n    def write_on_epoch_end(\n        self,\n        trainer,\n        pl_module: \"LightningModule\",\n        predictions: List[Any],\n        batch_indices: List[Any],\n    ):\n        \"\"\"\n        Write predictions on epoch end.\n\n        Args:\n            self: The instance of the class.\n            trainer: The trainer object.\n            pl_module (LightningModule): The Lightning module.\n            predictions (List[Any]): A list of prediction values.\n            batch_indices (List[Any]): A list of batch indices.\n\n        Raises:\n            AssertionError: If 'predict' is not present in pl_module.datasets.\n            AssertionError: If 'data' is not defined in pl_module.datasets.\n\n        Returns:\n            None\n        \"\"\"\n        rows = []\n        assert \"predict\" in pl_module.datasets, \"`data` not defined\"\n        dataset = pl_module.datasets[\"predict\"]\n        predictions = [pred for batch_pred in predictions for pred in batch_pred[\"pred\"]]\n\n        for idx, (row, pred) in enumerate(zip(dataset.get_rows(), predictions)):\n            for i, v in enumerate(pred):\n                row[f\"pred_{i}\"] = v.item()\n\n            rows.append(row)\n\n            # Save image previews\n            if idx &lt;= self.save_preview_samples:\n                input = dataset[idx]\n                self.save_preview_image(input, idx)\n\n        df = pd.DataFrame(rows)\n        df.to_csv(self.output_csv)\n</code></pre>"},{"location":"reference/callbacks/prediction_saver/#fmcib.callbacks.prediction_saver.SavePredictions.__init__","title":"<code>__init__(path, save_preview_samples=False, keys=None)</code>","text":"<p>Initialize an instance of the class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the output CSV file.</p> required <code>save_preview_samples</code> <code>bool</code> <p>A flag indicating whether to save preview samples. Defaults to False.</p> <code>False</code> <code>keys</code> <code>List[str]</code> <p>A list of keys. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/callbacks/prediction_saver.py</code> <pre><code>def __init__(self, path: str, save_preview_samples: bool = False, keys: List[str] = None):\n    \"\"\"\n    Initialize an instance of the class.\n\n    Args:\n        path (str): The path to save the output CSV file.\n        save_preview_samples (bool, optional): A flag indicating whether to save preview samples. Defaults to False.\n        keys (List[str], optional): A list of keys. Defaults to None.\n\n    Raises:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__(\"epoch\")\n    self.output_csv = Path(path)\n    self.keys = keys\n    self.save_preview_samples = save_preview_samples\n    self.output_csv.parent.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/callbacks/prediction_saver/#fmcib.callbacks.prediction_saver.SavePredictions.save_preview_image","title":"<code>save_preview_image(data, tag)</code>","text":"<p>Save a preview image to a specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The object calling the function. (self in Python)</p> required <code>data</code> <code>tuple</code> <p>A tuple containing the image data and its corresponding tag.</p> required <code>tag</code> <code>str</code> <p>The tag for the image.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/callbacks/prediction_saver.py</code> <pre><code>def save_preview_image(self, data, tag):\n    \"\"\"\n    Save a preview image to a specified directory.\n\n    Args:\n        self (object): The object calling the function. (self in Python)\n        data (tuple): A tuple containing the image data and its corresponding tag.\n        tag (str): The tag for the image.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    self.output_dir = self.output_csv.parent / f\"previews_{self.output_csv.stem}\"\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n    image, _ = data\n    image = handle_image(image)\n    fp = self.output_dir / f\"{tag}.png\"\n    torchvision.utils.save_image(image, fp)\n</code></pre>"},{"location":"reference/callbacks/prediction_saver/#fmcib.callbacks.prediction_saver.SavePredictions.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer, pl_module, predictions, batch_indices)</code>","text":"<p>Write predictions on epoch end.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The instance of the class.</p> required <code>trainer</code> <p>The trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module.</p> required <code>predictions</code> <code>List[Any]</code> <p>A list of prediction values.</p> required <code>batch_indices</code> <code>List[Any]</code> <p>A list of batch indices.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If 'predict' is not present in pl_module.datasets.</p> <code>AssertionError</code> <p>If 'data' is not defined in pl_module.datasets.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/callbacks/prediction_saver.py</code> <pre><code>def write_on_epoch_end(\n    self,\n    trainer,\n    pl_module: \"LightningModule\",\n    predictions: List[Any],\n    batch_indices: List[Any],\n):\n    \"\"\"\n    Write predictions on epoch end.\n\n    Args:\n        self: The instance of the class.\n        trainer: The trainer object.\n        pl_module (LightningModule): The Lightning module.\n        predictions (List[Any]): A list of prediction values.\n        batch_indices (List[Any]): A list of batch indices.\n\n    Raises:\n        AssertionError: If 'predict' is not present in pl_module.datasets.\n        AssertionError: If 'data' is not defined in pl_module.datasets.\n\n    Returns:\n        None\n    \"\"\"\n    rows = []\n    assert \"predict\" in pl_module.datasets, \"`data` not defined\"\n    dataset = pl_module.datasets[\"predict\"]\n    predictions = [pred for batch_pred in predictions for pred in batch_pred[\"pred\"]]\n\n    for idx, (row, pred) in enumerate(zip(dataset.get_rows(), predictions)):\n        for i, v in enumerate(pred):\n            row[f\"pred_{i}\"] = v.item()\n\n        rows.append(row)\n\n        # Save image previews\n        if idx &lt;= self.save_preview_samples:\n            input = dataset[idx]\n            self.save_preview_image(input, idx)\n\n    df = pd.DataFrame(rows)\n    df.to_csv(self.output_csv)\n</code></pre>"},{"location":"reference/callbacks/utils/","title":"utils","text":""},{"location":"reference/callbacks/utils/#fmcib.callbacks.utils.decollate","title":"<code>decollate(data)</code>","text":"<p>Decollate a list of tensors into a list of values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>A list of batch tensors.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values from the input tensors.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the input is not a list of tensors.</p> Source code in <code>fmcib/callbacks/utils.py</code> <pre><code>def decollate(data: List[torch.Tensor]):\n    \"\"\"\n    Decollate a list of tensors into a list of values.\n\n    Args:\n        data (list): A list of batch tensors.\n\n    Returns:\n        list: A list of values from the input tensors.\n\n    Raises:\n        AssertionError: If the input is not a list of tensors.\n    \"\"\"\n    assert isinstance(data, list), \"Decollate only implemented for list of `batch` tensors\"\n\n    out = []\n    for d in data:\n        # Handles both cases: multiple elements and single element\n        # https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html\n        d = d.tolist()\n\n        out += d\n    return out\n</code></pre>"},{"location":"reference/callbacks/utils/#fmcib.callbacks.utils.handle_image","title":"<code>handle_image(image)</code>","text":"<p>Handle image according to specific requirements.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>tensor</code> <p>An image tensor.</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>The processed image tensor, based on the input conditions.</p> Source code in <code>fmcib/callbacks/utils.py</code> <pre><code>def handle_image(image):\n    \"\"\"\n    Handle image according to specific requirements.\n\n    Args:\n        image (tensor): An image tensor.\n\n    Returns:\n        tensor: The processed image tensor, based on the input conditions.\n\n    Raises:\n        None.\n    \"\"\"\n    image = image.squeeze()\n    if image.dim() == 3:\n        return image[image.shape[0] // 2]\n    else:\n        return image\n</code></pre>"},{"location":"reference/datasets/","title":"datasets","text":""},{"location":"reference/datasets/#fmcib.datasets.create_dummy_row","title":"<code>create_dummy_row(size, output_filename)</code>","text":"<p>Function to create a dummy row with path to an image and seed point corresponding to the image</p> Source code in <code>fmcib/datasets/__init__.py</code> <pre><code>def create_dummy_row(size, output_filename):\n    \"\"\"\n    Function to create a dummy row with path to an image and seed point corresponding to the image\n    \"\"\"\n\n    # Create a np array initialized with random values between -1024 and 2048\n    np_image = np.random.randint(-1024, 2048, size, dtype=np.int16)\n\n    # Create an itk image from the numpy array\n    itk_image = sitk.GetImageFromArray(np_image)\n\n    # Save itk image to file with the given output filename\n    sitk.WriteImage(itk_image, output_filename)\n\n    x, y, z = generate_random_seed_point(itk_image.GetSize())\n\n    # Convert to global coordinates\n    x, y, z = itk_image.TransformContinuousIndexToPhysicalPoint((x, y, z))\n\n    return {\n        \"image_path\": output_filename,\n        \"PatientID\": random.randint(0, 100000),\n        \"coordX\": x,\n        \"coordY\": y,\n        \"coordZ\": z,\n        \"label\": random.randint(0, 1),\n    }\n</code></pre>"},{"location":"reference/datasets/#fmcib.datasets.generate_random_seed_point","title":"<code>generate_random_seed_point(image_size)</code>","text":"<p>Function to generate a random x, y, z coordinate within the image</p> Source code in <code>fmcib/datasets/__init__.py</code> <pre><code>def generate_random_seed_point(image_size):\n    \"\"\"\n    Function to generate a random x, y, z coordinate within the image\n    \"\"\"\n    x = random.randint(0, image_size[0] - 1)\n    y = random.randint(0, image_size[1] - 1)\n    z = random.randint(0, image_size[2] - 1)\n\n    return (x, y, z)\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/","title":"ssl_radiomics_dataset","text":""},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset","title":"<code>SSLRadiomicsDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for SSL Radiomics dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the dataset.</p> required <code>label</code> <code>str</code> <p>The label column name in the dataset annotations. Default is None.</p> <code>None</code> <code>radius</code> <code>int</code> <p>The radius around the centroid for positive patch extraction. Default is 25.</p> <code>25</code> <code>orient</code> <code>bool</code> <p>Whether to orient the images to LPI orientation. Default is False.</p> <code>False</code> <code>resample_spacing</code> <code>float or tuple</code> <p>The desired spacing for resampling the images. Default is None.</p> <code>None</code> <code>enable_negatives</code> <code>bool</code> <p>Whether to include negative samples. Default is True.</p> <code>True</code> <code>transform</code> <code>callable</code> <p>A function/transform to apply on the images. Default is None.</p> <code>None</code> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>class SSLRadiomicsDataset(Dataset):\n    \"\"\"\n    Dataset class for SSL Radiomics dataset.\n\n    Args:\n        path (str): The path to the dataset.\n        label (str, optional): The label column name in the dataset annotations. Default is None.\n        radius (int, optional): The radius around the centroid for positive patch extraction. Default is 25.\n        orient (bool, optional): Whether to orient the images to LPI orientation. Default is False.\n        resample_spacing (float or tuple, optional): The desired spacing for resampling the images. Default is None.\n        enable_negatives (bool, optional): Whether to include negative samples. Default is True.\n        transform (callable, optional): A function/transform to apply on the images. Default is None.\n    \"\"\"\n\n    def __init__(\n        self,\n        path,\n        label=None,\n        radius=25,\n        orient=False,\n        resample_spacing=None,\n        enable_negatives=True,\n        transform=None,\n        orient_patch=True,\n        input_is_target=False,\n    ):\n        \"\"\"\n        Creates an instance of the SSLRadiomicsDataset class with the given parameters.\n\n        Args:\n            path (str): The path to the dataset.\n            label (Optional[str]): The label to use for the dataset. Defaults to None.\n            radius (int): The radius parameter. Defaults to 25.\n            orient (bool): True if the dataset should be oriented, False otherwise. Defaults to False.\n            resample_spacing (Optional[...]): The resample spacing parameter. Defaults to None.\n            enable_negatives (bool): True if negatives are enabled, False otherwise. Defaults to True.\n            transform: The transformation to apply to the dataset. Defaults to None.\n            orient_patch (bool): True if the patch should be oriented, False otherwise. Defaults to True.\n            input_is_target (bool): True if the input is the target, False otherwise. Defaults to False.\n\n        Raises:\n            None.\n\n        Returns:\n            None.\n        \"\"\"\n        monai.data.set_track_meta(False)\n        sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(1)\n        super(SSLRadiomicsDataset, self).__init__()\n        self._path = Path(path)\n\n        self.radius = radius\n        self.orient = orient\n        self.resample_spacing = resample_spacing\n        self.label = label\n        self.enable_negatives = enable_negatives\n        self.transform = transform\n        self.orient_patch = orient_patch\n        self.input_is_target = input_is_target\n        self.annotations = pd.read_csv(self._path)\n        self._num_samples = len(self.annotations)  # set the length of the dataset\n\n    def get_rows(self):\n        \"\"\"\n        Get the rows of the annotations as a list of dictionaries.\n\n        Returns:\n            list of dict: The rows of the annotations as dictionaries.\n        \"\"\"\n        return self.annotations.to_dict(orient=\"records\")\n\n    def get_labels(self):\n        \"\"\"\n        Function to get labels for when they are available in the dataset.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n\n        labels = self.annotations[self.label].values\n        assert not np.any(labels == -1), \"All labels must be specified\"\n        return labels\n\n    def __len__(self):\n        \"\"\"\n        Size of the dataset.\n        \"\"\"\n        return self._num_samples\n\n    def get_negative_sample(self, image):\n        \"\"\"\n        Extract a negative sample from the image background with no overlap to the positive sample.\n\n        Parameters:\n            image: Image to extract sample\n            positive_patch_idx: Index of the positive patch in [(xmin, xmax), (ymin, ymax), (zmin, zmax)]\n        \"\"\"\n        positive_patch_size = [self.radius * 2] * 3\n        valid_patch_size = monai.data.utils.get_valid_patch_size(image.GetSize(), positive_patch_size)\n\n        def get_random_patch():\n            \"\"\"\n            Get a random patch from an image.\n\n            Returns:\n                list: A list containing the start and end indices of the random patch.\n            \"\"\"\n            random_patch_idx = [\n                [x.start, x.stop] for x in monai.data.utils.get_random_patch(image.GetSize(), valid_patch_size)\n            ]\n            return random_patch_idx\n\n        random_patch_idx = get_random_patch()\n\n        # escape_count = 0\n        # while is_overlapping(positive_patch_idx, random_patch_idx):\n        #     if escape_count &gt;= 3:\n        #         logger.warning(\"Random patch has overlap with positive patch\")\n        #         return None\n\n        #     random_patch_idx = get_random_patch()\n        #     escape_count += 1\n\n        random_patch = slice_image(image, random_patch_idx)\n        random_patch = sitk.DICOMOrient(random_patch, \"LPS\") if self.orient_patch else random_patch\n        negative_array = sitk.GetArrayFromImage(random_patch)\n\n        negative_tensor = negative_array if self.transform is None else self.transform(negative_array)\n        return negative_tensor\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        Implement how to load the data corresponding to the idx element in the dataset from your data source.\n        \"\"\"\n\n        # Get a row from the CSV file\n        row = self.annotations.iloc[idx]\n        image_path = row[\"image_path\"]\n        image = sitk.ReadImage(str(image_path))\n        image = resample_image_to_spacing(image, self.resample_spacing, -1024) if self.resample_spacing is not None else image\n\n        centroid = (row[\"coordX\"], row[\"coordY\"], row[\"coordZ\"])\n        centroid = image.TransformPhysicalPointToContinuousIndex(centroid)\n        centroid = [int(d) for d in centroid]\n\n        # Orient all images to LPI orientation\n        image = sitk.DICOMOrient(image, \"LPI\") if self.orient else image\n\n        # Extract positive with a specified radius around centroid\n        patch_idx = [(c - self.radius, c + self.radius) for c in centroid]\n        patch_image = slice_image(image, patch_idx)\n\n        patch_image = sitk.DICOMOrient(patch_image, \"LPS\") if self.orient_patch else patch_image\n\n        array = sitk.GetArrayFromImage(patch_image)\n        tensor = array if self.transform is None else self.transform(array)\n\n        if self.label is not None:\n            target = int(row[self.label])\n        elif self.input_is_target:\n            target = tensor.clone()\n        else:\n            target = None\n\n        if self.enable_negatives:\n            return {\"positive\": tensor, \"negative\": self.get_negative_sample(image)}, target\n\n        return tensor, target\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Implement how to load the data corresponding to the idx element in the dataset from your data source.</p> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def __getitem__(self, idx: int):\n    \"\"\"\n    Implement how to load the data corresponding to the idx element in the dataset from your data source.\n    \"\"\"\n\n    # Get a row from the CSV file\n    row = self.annotations.iloc[idx]\n    image_path = row[\"image_path\"]\n    image = sitk.ReadImage(str(image_path))\n    image = resample_image_to_spacing(image, self.resample_spacing, -1024) if self.resample_spacing is not None else image\n\n    centroid = (row[\"coordX\"], row[\"coordY\"], row[\"coordZ\"])\n    centroid = image.TransformPhysicalPointToContinuousIndex(centroid)\n    centroid = [int(d) for d in centroid]\n\n    # Orient all images to LPI orientation\n    image = sitk.DICOMOrient(image, \"LPI\") if self.orient else image\n\n    # Extract positive with a specified radius around centroid\n    patch_idx = [(c - self.radius, c + self.radius) for c in centroid]\n    patch_image = slice_image(image, patch_idx)\n\n    patch_image = sitk.DICOMOrient(patch_image, \"LPS\") if self.orient_patch else patch_image\n\n    array = sitk.GetArrayFromImage(patch_image)\n    tensor = array if self.transform is None else self.transform(array)\n\n    if self.label is not None:\n        target = int(row[self.label])\n    elif self.input_is_target:\n        target = tensor.clone()\n    else:\n        target = None\n\n    if self.enable_negatives:\n        return {\"positive\": tensor, \"negative\": self.get_negative_sample(image)}, target\n\n    return tensor, target\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.__init__","title":"<code>__init__(path, label=None, radius=25, orient=False, resample_spacing=None, enable_negatives=True, transform=None, orient_patch=True, input_is_target=False)</code>","text":"<p>Creates an instance of the SSLRadiomicsDataset class with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the dataset.</p> required <code>label</code> <code>Optional[str]</code> <p>The label to use for the dataset. Defaults to None.</p> <code>None</code> <code>radius</code> <code>int</code> <p>The radius parameter. Defaults to 25.</p> <code>25</code> <code>orient</code> <code>bool</code> <p>True if the dataset should be oriented, False otherwise. Defaults to False.</p> <code>False</code> <code>resample_spacing</code> <code>Optional[...]</code> <p>The resample spacing parameter. Defaults to None.</p> <code>None</code> <code>enable_negatives</code> <code>bool</code> <p>True if negatives are enabled, False otherwise. Defaults to True.</p> <code>True</code> <code>transform</code> <p>The transformation to apply to the dataset. Defaults to None.</p> <code>None</code> <code>orient_patch</code> <code>bool</code> <p>True if the patch should be oriented, False otherwise. Defaults to True.</p> <code>True</code> <code>input_is_target</code> <code>bool</code> <p>True if the input is the target, False otherwise. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def __init__(\n    self,\n    path,\n    label=None,\n    radius=25,\n    orient=False,\n    resample_spacing=None,\n    enable_negatives=True,\n    transform=None,\n    orient_patch=True,\n    input_is_target=False,\n):\n    \"\"\"\n    Creates an instance of the SSLRadiomicsDataset class with the given parameters.\n\n    Args:\n        path (str): The path to the dataset.\n        label (Optional[str]): The label to use for the dataset. Defaults to None.\n        radius (int): The radius parameter. Defaults to 25.\n        orient (bool): True if the dataset should be oriented, False otherwise. Defaults to False.\n        resample_spacing (Optional[...]): The resample spacing parameter. Defaults to None.\n        enable_negatives (bool): True if negatives are enabled, False otherwise. Defaults to True.\n        transform: The transformation to apply to the dataset. Defaults to None.\n        orient_patch (bool): True if the patch should be oriented, False otherwise. Defaults to True.\n        input_is_target (bool): True if the input is the target, False otherwise. Defaults to False.\n\n    Raises:\n        None.\n\n    Returns:\n        None.\n    \"\"\"\n    monai.data.set_track_meta(False)\n    sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(1)\n    super(SSLRadiomicsDataset, self).__init__()\n    self._path = Path(path)\n\n    self.radius = radius\n    self.orient = orient\n    self.resample_spacing = resample_spacing\n    self.label = label\n    self.enable_negatives = enable_negatives\n    self.transform = transform\n    self.orient_patch = orient_patch\n    self.input_is_target = input_is_target\n    self.annotations = pd.read_csv(self._path)\n    self._num_samples = len(self.annotations)  # set the length of the dataset\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.__len__","title":"<code>__len__()</code>","text":"<p>Size of the dataset.</p> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Size of the dataset.\n    \"\"\"\n    return self._num_samples\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.get_labels","title":"<code>get_labels()</code>","text":"<p>Function to get labels for when they are available in the dataset.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def get_labels(self):\n    \"\"\"\n    Function to get labels for when they are available in the dataset.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n\n    labels = self.annotations[self.label].values\n    assert not np.any(labels == -1), \"All labels must be specified\"\n    return labels\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.get_negative_sample","title":"<code>get_negative_sample(image)</code>","text":"<p>Extract a negative sample from the image background with no overlap to the positive sample.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>Image to extract sample</p> required <code>positive_patch_idx</code> <p>Index of the positive patch in [(xmin, xmax), (ymin, ymax), (zmin, zmax)]</p> required Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def get_negative_sample(self, image):\n    \"\"\"\n    Extract a negative sample from the image background with no overlap to the positive sample.\n\n    Parameters:\n        image: Image to extract sample\n        positive_patch_idx: Index of the positive patch in [(xmin, xmax), (ymin, ymax), (zmin, zmax)]\n    \"\"\"\n    positive_patch_size = [self.radius * 2] * 3\n    valid_patch_size = monai.data.utils.get_valid_patch_size(image.GetSize(), positive_patch_size)\n\n    def get_random_patch():\n        \"\"\"\n        Get a random patch from an image.\n\n        Returns:\n            list: A list containing the start and end indices of the random patch.\n        \"\"\"\n        random_patch_idx = [\n            [x.start, x.stop] for x in monai.data.utils.get_random_patch(image.GetSize(), valid_patch_size)\n        ]\n        return random_patch_idx\n\n    random_patch_idx = get_random_patch()\n\n    # escape_count = 0\n    # while is_overlapping(positive_patch_idx, random_patch_idx):\n    #     if escape_count &gt;= 3:\n    #         logger.warning(\"Random patch has overlap with positive patch\")\n    #         return None\n\n    #     random_patch_idx = get_random_patch()\n    #     escape_count += 1\n\n    random_patch = slice_image(image, random_patch_idx)\n    random_patch = sitk.DICOMOrient(random_patch, \"LPS\") if self.orient_patch else random_patch\n    negative_array = sitk.GetArrayFromImage(random_patch)\n\n    negative_tensor = negative_array if self.transform is None else self.transform(negative_array)\n    return negative_tensor\n</code></pre>"},{"location":"reference/datasets/ssl_radiomics_dataset/#fmcib.datasets.ssl_radiomics_dataset.SSLRadiomicsDataset.get_rows","title":"<code>get_rows()</code>","text":"<p>Get the rows of the annotations as a list of dictionaries.</p> <p>Returns:</p> Type Description <p>list of dict: The rows of the annotations as dictionaries.</p> Source code in <code>fmcib/datasets/ssl_radiomics_dataset.py</code> <pre><code>def get_rows(self):\n    \"\"\"\n    Get the rows of the annotations as a list of dictionaries.\n\n    Returns:\n        list of dict: The rows of the annotations as dictionaries.\n    \"\"\"\n    return self.annotations.to_dict(orient=\"records\")\n</code></pre>"},{"location":"reference/datasets/utils/","title":"utils","text":""},{"location":"reference/datasets/utils/#fmcib.datasets.utils.is_overlapping","title":"<code>is_overlapping(patch1, patch2)</code>","text":"<p>Check if two patches are overlapping.</p> <p>Parameters:</p> Name Type Description Default <code>patch1</code> <code>list of tuples</code> <p>A list of tuples representing the ranges of each axis in patch1.</p> required <code>patch2</code> <code>list of tuples</code> <p>A list of tuples representing the ranges of each axis in patch2.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the two patches overlap, False otherwise.</p> Note <p>This function assumes that each patch is represented by a list of tuples, where each tuple represents the range of an axis in the patch. The range of an axis is represented by a tuple (start, end), where start is the start value of the range and end is the end value of the range. The patches are considered overlapping if there is any overlap in the ranges of their axes.</p> Source code in <code>fmcib/datasets/utils.py</code> <pre><code>def is_overlapping(patch1, patch2):\n    \"\"\"\n    Check if two patches are overlapping.\n\n    Args:\n        patch1 (list of tuples): A list of tuples representing the ranges of each axis in patch1.\n        patch2 (list of tuples): A list of tuples representing the ranges of each axis in patch2.\n\n    Returns:\n        bool: True if the two patches overlap, False otherwise.\n\n    Note:\n        This function assumes that each patch is represented by a list of tuples, where each tuple represents the range of an axis in the patch.\n        The range of an axis is represented by a tuple (start, end), where start is the start value of the range and end is the end value of the range.\n        The patches are considered overlapping if there is any overlap in the ranges of their axes.\n    \"\"\"\n    overlap_by_axis = [max(axis1[0], axis2[0]) &lt; min(axis1[1], axis2[1]) for axis1, axis2 in zip(patch1, patch2)]\n\n    return np.all(overlap_by_axis)\n</code></pre>"},{"location":"reference/datasets/utils/#fmcib.datasets.utils.resample_image_to_spacing","title":"<code>resample_image_to_spacing(image, new_spacing, default_value, interpolator='linear')</code>","text":"<p>Resample an image to a new spacing.</p> Source code in <code>fmcib/datasets/utils.py</code> <pre><code>def resample_image_to_spacing(image, new_spacing, default_value, interpolator=\"linear\"):\n    \"\"\"\n    Resample an image to a new spacing.\n    \"\"\"\n    assert interpolator in SITK_INTERPOLATOR_DICT, (\n        f\"Interpolator '{interpolator}' not part of SimpleITK. \"\n        f\"Please choose one of the following {list(SITK_INTERPOLATOR_DICT.keys())}.\"\n    )\n\n    assert image.GetDimension() == len(new_spacing), (\n        f\"Input is {image.GetDimension()}-dimensional while \" f\"the new spacing is {len(new_spacing)}-dimensional.\"\n    )\n\n    interpolator = SITK_INTERPOLATOR_DICT[interpolator]\n    spacing = image.GetSpacing()\n    size = image.GetSize()\n    new_size = [int(round(siz * spac / n_spac)) for siz, spac, n_spac in zip(size, spacing, new_spacing)]\n    return sitk.Resample(\n        image,\n        new_size,  # size\n        sitk.Transform(),  # transform\n        interpolator,  # interpolator\n        image.GetOrigin(),  # outputOrigin\n        new_spacing,  # outputSpacing\n        image.GetDirection(),  # outputDirection\n        default_value,  # defaultPixelValue\n        image.GetPixelID(),\n    )  # outputPixelType\n</code></pre>"},{"location":"reference/datasets/utils/#fmcib.datasets.utils.slice_image","title":"<code>slice_image(image, patch_idx)</code>","text":"<p>Slice an image.</p> Source code in <code>fmcib/datasets/utils.py</code> <pre><code>def slice_image(image, patch_idx):\n    \"\"\"\n    Slice an image.\n    \"\"\"\n\n    start, stop = zip(*patch_idx)\n    slice_filter = sitk.SliceImageFilter()\n    slice_filter.SetStart(start)\n    slice_filter.SetStop(stop)\n    return slice_filter.Execute(image)\n</code></pre>"},{"location":"reference/models/","title":"models","text":""},{"location":"reference/models/autoencoder/","title":"autoencoder","text":""},{"location":"reference/models/autoencoder/#fmcib.models.autoencoder.CustomAE","title":"<code>CustomAE</code>","text":"<p>               Bases: <code>AutoEncoder</code></p> <p>A custom AutoEncoder class.</p> <p>Inherits from AutoEncoder.</p> <p>Attributes:</p> Name Type Description <code>padding</code> <code>int</code> <p>The padding size for the convolutional layers.</p> <code>decoder</code> <code>bool</code> <p>Determines if the decoder part of the network is included.</p> <code>kwargs</code> <code>bool</code> <p>Additional keyword arguments passed to the parent class.</p> <p>Methods:</p> Name Description <code>_get_encode_layer</code> <p>Returns a single layer of the encoder part of the network.</p> <code>_get_decode_layer</code> <p>Returns a single layer of the decoder part of the network.</p> Source code in <code>fmcib/models/autoencoder.py</code> <pre><code>class CustomAE(AutoEncoder):\n    \"\"\"\n    A custom AutoEncoder class.\n\n    Inherits from AutoEncoder.\n\n    Attributes:\n        padding (int): The padding size for the convolutional layers.\n        decoder (bool, optional): Determines if the decoder part of the network is included.\n        kwargs: Additional keyword arguments passed to the parent class.\n\n    Methods:\n        _get_encode_layer(in_channels, out_channels, strides, is_last): Returns a single layer of the encoder part of the network.\n        _get_decode_layer(in_channels, out_channels, strides, is_last): Returns a single layer of the decoder part of the network.\n    \"\"\"\n\n    def __init__(self, padding, decoder=True, **kwargs):\n        \"\"\"\n        Initialize the object.\n\n        Args:\n            padding (int): Padding value.\n            decoder (bool, optional): If True, use a decoder. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        Attributes:\n            padding (int): Padding value.\n\n        Raises:\n            None\n        \"\"\"\n        self.padding = padding\n        super().__init__(**kwargs)\n        if not decoder:\n            self.decode = nn.Sequential(nn.AvgPool3d(3), nn.Flatten())\n\n    def _get_encode_layer(self, in_channels: int, out_channels: int, strides: int, is_last: bool) -&gt; nn.Module:\n        \"\"\"\n        Returns a single layer of the encoder part of the network.\n        \"\"\"\n        mod: nn.Module\n        if self.num_res_units &gt; 0:\n            mod = ResidualUnit(\n                spatial_dims=self.dimensions,\n                in_channels=in_channels,\n                out_channels=out_channels,\n                strides=strides,\n                kernel_size=self.kernel_size,\n                padding=self.padding,\n                subunits=self.num_res_units,\n                act=self.act,\n                norm=self.norm,\n                dropout=self.dropout,\n                bias=self.bias,\n                last_conv_only=is_last,\n            )\n            return mod\n        mod = Convolution(\n            spatial_dims=self.dimensions,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            strides=strides,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            act=self.act,\n            norm=self.norm,\n            dropout=self.dropout,\n            bias=self.bias,\n            conv_only=is_last,\n        )\n        return mod\n\n    def _get_decode_layer(self, in_channels: int, out_channels: int, strides: int, is_last: bool) -&gt; nn.Sequential:\n        \"\"\"\n        Returns a single layer of the decoder part of the network.\n        \"\"\"\n        decode = nn.Sequential()\n\n        conv = Convolution(\n            spatial_dims=self.dimensions,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            strides=strides,\n            kernel_size=self.up_kernel_size,\n            padding=self.padding,\n            act=self.act,\n            norm=self.norm,\n            dropout=self.dropout,\n            bias=self.bias,\n            conv_only=is_last and self.num_res_units == 0,\n            is_transposed=True,\n        )\n\n        decode.add_module(\"conv\", conv)\n\n        if self.num_res_units &gt; 0:\n            ru = ResidualUnit(\n                spatial_dims=self.dimensions,\n                in_channels=out_channels,\n                out_channels=out_channels,\n                padding=self.padding,\n                strides=strides,\n                kernel_size=self.kernel_size,\n                subunits=1,\n                act=self.act,\n                norm=self.norm,\n                dropout=self.dropout,\n                bias=self.bias,\n                last_conv_only=is_last,\n            )\n\n            decode.add_module(\"resunit\", ru)\n\n        return decode\n</code></pre>"},{"location":"reference/models/autoencoder/#fmcib.models.autoencoder.CustomAE.__init__","title":"<code>__init__(padding, decoder=True, **kwargs)</code>","text":"<p>Initialize the object.</p> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>int</code> <p>Padding value.</p> required <code>decoder</code> <code>bool</code> <p>If True, use a decoder. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>padding</code> <code>int</code> <p>Padding value.</p> Source code in <code>fmcib/models/autoencoder.py</code> <pre><code>def __init__(self, padding, decoder=True, **kwargs):\n    \"\"\"\n    Initialize the object.\n\n    Args:\n        padding (int): Padding value.\n        decoder (bool, optional): If True, use a decoder. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    Attributes:\n        padding (int): Padding value.\n\n    Raises:\n        None\n    \"\"\"\n    self.padding = padding\n    super().__init__(**kwargs)\n    if not decoder:\n        self.decode = nn.Sequential(nn.AvgPool3d(3), nn.Flatten())\n</code></pre>"},{"location":"reference/models/autoencoder/#fmcib.models.autoencoder.CustomAE._get_decode_layer","title":"<code>_get_decode_layer(in_channels, out_channels, strides, is_last)</code>","text":"<p>Returns a single layer of the decoder part of the network.</p> Source code in <code>fmcib/models/autoencoder.py</code> <pre><code>def _get_decode_layer(self, in_channels: int, out_channels: int, strides: int, is_last: bool) -&gt; nn.Sequential:\n    \"\"\"\n    Returns a single layer of the decoder part of the network.\n    \"\"\"\n    decode = nn.Sequential()\n\n    conv = Convolution(\n        spatial_dims=self.dimensions,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        strides=strides,\n        kernel_size=self.up_kernel_size,\n        padding=self.padding,\n        act=self.act,\n        norm=self.norm,\n        dropout=self.dropout,\n        bias=self.bias,\n        conv_only=is_last and self.num_res_units == 0,\n        is_transposed=True,\n    )\n\n    decode.add_module(\"conv\", conv)\n\n    if self.num_res_units &gt; 0:\n        ru = ResidualUnit(\n            spatial_dims=self.dimensions,\n            in_channels=out_channels,\n            out_channels=out_channels,\n            padding=self.padding,\n            strides=strides,\n            kernel_size=self.kernel_size,\n            subunits=1,\n            act=self.act,\n            norm=self.norm,\n            dropout=self.dropout,\n            bias=self.bias,\n            last_conv_only=is_last,\n        )\n\n        decode.add_module(\"resunit\", ru)\n\n    return decode\n</code></pre>"},{"location":"reference/models/autoencoder/#fmcib.models.autoencoder.CustomAE._get_encode_layer","title":"<code>_get_encode_layer(in_channels, out_channels, strides, is_last)</code>","text":"<p>Returns a single layer of the encoder part of the network.</p> Source code in <code>fmcib/models/autoencoder.py</code> <pre><code>def _get_encode_layer(self, in_channels: int, out_channels: int, strides: int, is_last: bool) -&gt; nn.Module:\n    \"\"\"\n    Returns a single layer of the encoder part of the network.\n    \"\"\"\n    mod: nn.Module\n    if self.num_res_units &gt; 0:\n        mod = ResidualUnit(\n            spatial_dims=self.dimensions,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            strides=strides,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            subunits=self.num_res_units,\n            act=self.act,\n            norm=self.norm,\n            dropout=self.dropout,\n            bias=self.bias,\n            last_conv_only=is_last,\n        )\n        return mod\n    mod = Convolution(\n        spatial_dims=self.dimensions,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        strides=strides,\n        kernel_size=self.kernel_size,\n        padding=self.padding,\n        act=self.act,\n        norm=self.norm,\n        dropout=self.dropout,\n        bias=self.bias,\n        conv_only=is_last,\n    )\n    return mod\n</code></pre>"},{"location":"reference/models/load_model/","title":"load_model","text":""},{"location":"reference/models/load_model/#fmcib.models.load_model.LoadModel","title":"<code>LoadModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing a loaded model.</p> <p>Parameters:</p> Name Type Description Default <code>trunk</code> <code>Module</code> <p>The trunk of the model. Defaults to None.</p> <code>None</code> <code>weights_path</code> <code>str</code> <p>The path to the weights file. Defaults to None.</p> <code>None</code> <code>heads</code> <code>list</code> <p>The list of head layers in the model. Defaults to [].</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>trunk</code> <code>Module</code> <p>The trunk of the model.</p> <code>heads</code> <code>Sequential</code> <p>The concatenated head layers of the model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>torch.Tensor) -&gt; torch.Tensor: Forward pass through the model.</p> <code>load</code> <p>Load the pretrained model weights.</p> Source code in <code>fmcib/models/load_model.py</code> <pre><code>class LoadModel(nn.Module):\n    \"\"\"\n    A class representing a loaded model.\n\n    Args:\n        trunk (nn.Module, optional): The trunk of the model. Defaults to None.\n        weights_path (str, optional): The path to the weights file. Defaults to None.\n        heads (list, optional): The list of head layers in the model. Defaults to [].\n\n    Attributes:\n        trunk (nn.Module): The trunk of the model.\n        heads (nn.Sequential): The concatenated head layers of the model.\n\n    Methods:\n        forward(x: torch.Tensor) -&gt; torch.Tensor: Forward pass through the model.\n        load(weights): Load the pretrained model weights.\n    \"\"\"\n\n    def __init__(self, trunk=None, weights_path=None, heads=[]) -&gt; None:\n        \"\"\"\n        Initialize the model.\n\n        Args:\n            trunk (optional): The trunk of the model.\n            weights_path (optional): The path to the weights file.\n            heads (list, optional): A list of layer sizes for the heads of the model.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super().__init__()\n        self.trunk = trunk\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        head_layers = []\n        for idx in range(len(heads) - 1):\n            current_layers = []\n            current_layers.append(nn.Linear(heads[idx], heads[idx + 1], bias=True))\n\n            if idx != (len(heads) - 2):\n                current_layers.append(nn.ReLU(inplace=True))\n\n            head_layers.append(nn.Sequential(*current_layers))\n\n        if len(head_layers):\n            self.heads = nn.Sequential(*head_layers)\n        else:\n            self.heads = nn.Identity()\n\n        if weights_path is not None:\n            self.load(weights_path)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Forward pass of the neural network.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor.\n        \"\"\"\n        out = self.trunk(x)\n        out = self.heads(out)\n        return out\n\n    def load(self, weights):\n        \"\"\"\n        Load pretrained model weights from a file.\n\n        Args:\n            weights (str): The path to the file containing the pretrained model weights.\n\n        Raises:\n            KeyError: If the input weights file does not contain the expected keys.\n            Exception: If there is an error when loading the pretrained heads.\n\n        Returns:\n            None.\n\n        Note:\n            This function assumes that the pretrained model weights file is in the format expected by the model architecture.\n\n        Warnings:\n            - Missing keys: This warning message indicates the keys in the pretrained model weights file that are missing from the current model.\n            - Unexpected keys: This warning message indicates the keys in the pretrained model weights file that are not expected by the current model.\n\n        Raises the appropriate warnings and logs informational messages.\n        \"\"\"\n        pretrained_model = torch.load(weights, map_location=self.device, weights_only=True)\n\n        if \"trunk_state_dict\" in pretrained_model:  # Loading ViSSL pretrained model\n            trained_trunk = pretrained_model[\"trunk_state_dict\"]\n            msg = self.trunk.load_state_dict(trained_trunk, strict=False)\n            logger.warning(f\"Model Trunk - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n        # Load trained heads\n        if \"head_state_dict\" in pretrained_model:\n            trained_heads = pretrained_model[\"head_state_dict\"]\n\n            try:\n                msg = self.heads.load_state_dict(trained_heads, strict=False)\n            except Exception as e:\n                logger.error(f\"Failed to load trained heads with error {e}. This is expected if the models do not match!\")\n            logger.warning(f\"Model Head - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n        # Loading Lighter and other pretrained model\n        if \"state_dict\" in pretrained_model:\n            trained_model = pretrained_model[\"state_dict\"]\n\n            # match the keys (https://github.com/Project-MONAI/MONAI/issues/6811)\n            weights = {key.replace(\"module.\", \"\"): value for key, value in trained_model.items()}\n            weights = {key.replace(\"model.trunk.\", \"\"): value for key, value in trained_model.items()}\n            msg = self.trunk.load_state_dict(weights, strict=False)\n            logger.warning(f\"Model Trunk - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n            weights = {\n                key.replace(\"model.heads.\", \"\"): value for key, value in trained_model.items() if key.startswith(\"model.heads\")\n            }\n            msg = self.heads.load_state_dict(weights, strict=False)\n            logger.warning(f\"Model Head - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n        logger.info(f\"Loaded pretrained model weights \\n\")\n</code></pre>"},{"location":"reference/models/load_model/#fmcib.models.load_model.LoadModel.__init__","title":"<code>__init__(trunk=None, weights_path=None, heads=[])</code>","text":"<p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>trunk</code> <code>optional</code> <p>The trunk of the model.</p> <code>None</code> <code>weights_path</code> <code>optional</code> <p>The path to the weights file.</p> <code>None</code> <code>heads</code> <code>list</code> <p>A list of layer sizes for the heads of the model.</p> <code>[]</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>fmcib/models/load_model.py</code> <pre><code>def __init__(self, trunk=None, weights_path=None, heads=[]) -&gt; None:\n    \"\"\"\n    Initialize the model.\n\n    Args:\n        trunk (optional): The trunk of the model.\n        weights_path (optional): The path to the weights file.\n        heads (list, optional): A list of layer sizes for the heads of the model.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super().__init__()\n    self.trunk = trunk\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    head_layers = []\n    for idx in range(len(heads) - 1):\n        current_layers = []\n        current_layers.append(nn.Linear(heads[idx], heads[idx + 1], bias=True))\n\n        if idx != (len(heads) - 2):\n            current_layers.append(nn.ReLU(inplace=True))\n\n        head_layers.append(nn.Sequential(*current_layers))\n\n    if len(head_layers):\n        self.heads = nn.Sequential(*head_layers)\n    else:\n        self.heads = nn.Identity()\n\n    if weights_path is not None:\n        self.load(weights_path)\n</code></pre>"},{"location":"reference/models/load_model/#fmcib.models.load_model.LoadModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor.</p> Source code in <code>fmcib/models/load_model.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n    Forward pass of the neural network.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor.\n    \"\"\"\n    out = self.trunk(x)\n    out = self.heads(out)\n    return out\n</code></pre>"},{"location":"reference/models/load_model/#fmcib.models.load_model.LoadModel.load","title":"<code>load(weights)</code>","text":"<p>Load pretrained model weights from a file.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>The path to the file containing the pretrained model weights.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the input weights file does not contain the expected keys.</p> <code>Exception</code> <p>If there is an error when loading the pretrained heads.</p> <p>Returns:</p> Type Description <p>None.</p> Note <p>This function assumes that the pretrained model weights file is in the format expected by the model architecture.</p> <p>Warns:</p> Type Description <code>- Missing keys</code> <p>This warning message indicates the keys in the pretrained model weights file that are missing from the current model.</p> <code>- Unexpected keys</code> <p>This warning message indicates the keys in the pretrained model weights file that are not expected by the current model.</p> <p>Raises the appropriate warnings and logs informational messages.</p> Source code in <code>fmcib/models/load_model.py</code> <pre><code>def load(self, weights):\n    \"\"\"\n    Load pretrained model weights from a file.\n\n    Args:\n        weights (str): The path to the file containing the pretrained model weights.\n\n    Raises:\n        KeyError: If the input weights file does not contain the expected keys.\n        Exception: If there is an error when loading the pretrained heads.\n\n    Returns:\n        None.\n\n    Note:\n        This function assumes that the pretrained model weights file is in the format expected by the model architecture.\n\n    Warnings:\n        - Missing keys: This warning message indicates the keys in the pretrained model weights file that are missing from the current model.\n        - Unexpected keys: This warning message indicates the keys in the pretrained model weights file that are not expected by the current model.\n\n    Raises the appropriate warnings and logs informational messages.\n    \"\"\"\n    pretrained_model = torch.load(weights, map_location=self.device, weights_only=True)\n\n    if \"trunk_state_dict\" in pretrained_model:  # Loading ViSSL pretrained model\n        trained_trunk = pretrained_model[\"trunk_state_dict\"]\n        msg = self.trunk.load_state_dict(trained_trunk, strict=False)\n        logger.warning(f\"Model Trunk - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n    # Load trained heads\n    if \"head_state_dict\" in pretrained_model:\n        trained_heads = pretrained_model[\"head_state_dict\"]\n\n        try:\n            msg = self.heads.load_state_dict(trained_heads, strict=False)\n        except Exception as e:\n            logger.error(f\"Failed to load trained heads with error {e}. This is expected if the models do not match!\")\n        logger.warning(f\"Model Head - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n    # Loading Lighter and other pretrained model\n    if \"state_dict\" in pretrained_model:\n        trained_model = pretrained_model[\"state_dict\"]\n\n        # match the keys (https://github.com/Project-MONAI/MONAI/issues/6811)\n        weights = {key.replace(\"module.\", \"\"): value for key, value in trained_model.items()}\n        weights = {key.replace(\"model.trunk.\", \"\"): value for key, value in trained_model.items()}\n        msg = self.trunk.load_state_dict(weights, strict=False)\n        logger.warning(f\"Model Trunk - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n        weights = {\n            key.replace(\"model.heads.\", \"\"): value for key, value in trained_model.items() if key.startswith(\"model.heads\")\n        }\n        msg = self.heads.load_state_dict(weights, strict=False)\n        logger.warning(f\"Model Head - Missing keys: {msg[0]} and unexpected keys: {msg[1]}\")\n\n    logger.info(f\"Loaded pretrained model weights \\n\")\n</code></pre>"},{"location":"reference/models/models_genesis/","title":"models_genesis","text":""},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.ContBatchNorm3d","title":"<code>ContBatchNorm3d</code>","text":"<p>               Bases: <code>_BatchNorm</code></p> <p>A class representing a 3D contextual batch normalization layer.</p> <p>Attributes:</p> Name Type Description <code>running_mean</code> <code>Tensor</code> <p>The running mean of the batch normalization.</p> <code>running_var</code> <code>Tensor</code> <p>The running variance of the batch normalization.</p> <code>weight</code> <code>Tensor</code> <p>The learnable weights of the batch normalization.</p> <code>bias</code> <code>Tensor</code> <p>The learnable bias of the batch normalization.</p> <code>momentum</code> <code>float</code> <p>The momentum for updating the running statistics.</p> <code>eps</code> <code>float</code> <p>Small value added to the denominator for numerical stability.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class ContBatchNorm3d(nn.modules.batchnorm._BatchNorm):\n    \"\"\"\n    A class representing a 3D contextual batch normalization layer.\n\n    Attributes:\n        running_mean (torch.Tensor): The running mean of the batch normalization.\n        running_var (torch.Tensor): The running variance of the batch normalization.\n        weight (torch.Tensor): The learnable weights of the batch normalization.\n        bias (torch.Tensor): The learnable bias of the batch normalization.\n        momentum (float): The momentum for updating the running statistics.\n        eps (float): Small value added to the denominator for numerical stability.\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        \"\"\"\n        Check if the input tensor is 5-dimensional.\n\n        Args:\n            input (torch.Tensor): Input tensor to check the dimensionality.\n\n        Raises:\n            ValueError: If the input tensor is not 5-dimensional.\n        \"\"\"\n        if input.dim() != 5:\n            raise ValueError(\"expected 5D input (got {}D input)\".format(input.dim()))\n        # super(ContBatchNorm3d, self)._check_input_dim(input)\n\n    def forward(self, input):\n        \"\"\"\n        Apply forward pass for the input through batch normalization layer.\n\n        Args:\n            input (Tensor): Input tensor to be normalized.\n\n        Returns:\n            Tensor: Normalized output tensor.\n\n        Raises:\n            ValueError: If the dimensions of the input tensor do not match the expected input dimensions.\n        \"\"\"\n        self._check_input_dim(input)\n        return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, True, self.momentum, self.eps)\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.ContBatchNorm3d._check_input_dim","title":"<code>_check_input_dim(input)</code>","text":"<p>Check if the input tensor is 5-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to check the dimensionality.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input tensor is not 5-dimensional.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def _check_input_dim(self, input):\n    \"\"\"\n    Check if the input tensor is 5-dimensional.\n\n    Args:\n        input (torch.Tensor): Input tensor to check the dimensionality.\n\n    Raises:\n        ValueError: If the input tensor is not 5-dimensional.\n    \"\"\"\n    if input.dim() != 5:\n        raise ValueError(\"expected 5D input (got {}D input)\".format(input.dim()))\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.ContBatchNorm3d.forward","title":"<code>forward(input)</code>","text":"<p>Apply forward pass for the input through batch normalization layer.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to be normalized.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Normalized output tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dimensions of the input tensor do not match the expected input dimensions.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, input):\n    \"\"\"\n    Apply forward pass for the input through batch normalization layer.\n\n    Args:\n        input (Tensor): Input tensor to be normalized.\n\n    Returns:\n        Tensor: Normalized output tensor.\n\n    Raises:\n        ValueError: If the dimensions of the input tensor do not match the expected input dimensions.\n    \"\"\"\n    self._check_input_dim(input)\n    return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, True, self.momentum, self.eps)\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.DownTransition","title":"<code>DownTransition</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing a down transition module in a neural network.</p> <p>Attributes:</p> Name Type Description <code>in_channel</code> <code>int</code> <p>The number of input channels.</p> <code>depth</code> <code>int</code> <p>The depth of the down transition module.</p> <code>act</code> <code>Module</code> <p>The activation function used in the module.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class DownTransition(nn.Module):\n    \"\"\"\n    A class representing a down transition module in a neural network.\n\n    Attributes:\n        in_channel (int): The number of input channels.\n        depth (int): The depth of the down transition module.\n        act (nn.Module): The activation function used in the module.\n    \"\"\"\n\n    def __init__(self, in_channel, depth, act):\n        \"\"\"\n        Initialize a DownTransition object.\n\n        Args:\n            in_channel (int): The number of channels in the input.\n            depth (int): The depth of the DownTransition.\n            act (function): The activation function.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super(DownTransition, self).__init__()\n        self.ops = _make_nConv(in_channel, depth, act)\n        self.maxpool = nn.MaxPool3d(2)\n        self.current_depth = depth\n\n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass through the neural network.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            tuple: A tuple containing two tensors. The first tensor is the output of the forward pass. The second tensor is the output before applying the max pooling operation.\n\n        Raises:\n            None\n        \"\"\"\n        if self.current_depth == 3:\n            out = self.ops(x)\n            out_before_pool = out\n        else:\n            out_before_pool = self.ops(x)\n            out = self.maxpool(out_before_pool)\n        return out, out_before_pool\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.DownTransition.__init__","title":"<code>__init__(in_channel, depth, act)</code>","text":"<p>Initialize a DownTransition object.</p> <p>Parameters:</p> Name Type Description Default <code>in_channel</code> <code>int</code> <p>The number of channels in the input.</p> required <code>depth</code> <code>int</code> <p>The depth of the DownTransition.</p> required <code>act</code> <code>function</code> <p>The activation function.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def __init__(self, in_channel, depth, act):\n    \"\"\"\n    Initialize a DownTransition object.\n\n    Args:\n        in_channel (int): The number of channels in the input.\n        depth (int): The depth of the DownTransition.\n        act (function): The activation function.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super(DownTransition, self).__init__()\n    self.ops = _make_nConv(in_channel, depth, act)\n    self.maxpool = nn.MaxPool3d(2)\n    self.current_depth = depth\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.DownTransition.forward","title":"<code>forward(x)</code>","text":"<p>Perform a forward pass through the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing two tensors. The first tensor is the output of the forward pass. The second tensor is the output before applying the max pooling operation.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Perform a forward pass through the neural network.\n\n    Args:\n        x (Tensor): The input tensor.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is the output of the forward pass. The second tensor is the output before applying the max pooling operation.\n\n    Raises:\n        None\n    \"\"\"\n    if self.current_depth == 3:\n        out = self.ops(x)\n        out_before_pool = out\n    else:\n        out_before_pool = self.ops(x)\n        out = self.maxpool(out_before_pool)\n    return out, out_before_pool\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.LUConv","title":"<code>LUConv</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing a LUConv module.</p> <p>This module performs a convolution operation on the input data with a specified number of input channels and output channels. The convolution is followed by batch normalization and an activation function.</p> <p>Attributes:</p> Name Type Description <code>in_chan</code> <code>int</code> <p>The number of input channels.</p> <code>out_chan</code> <code>int</code> <p>The number of output channels.</p> <code>act</code> <code>str</code> <p>The activation function to be applied. Can be one of 'relu', 'prelu', or 'elu'.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class LUConv(nn.Module):\n    \"\"\"\n    A class representing a LUConv module.\n\n    This module performs a convolution operation on the input data with a specified number of input channels and output channels.\n    The convolution is followed by batch normalization and an activation function.\n\n    Attributes:\n        in_chan (int): The number of input channels.\n        out_chan (int): The number of output channels.\n        act (str): The activation function to be applied. Can be one of 'relu', 'prelu', or 'elu'.\n    \"\"\"\n\n    def __init__(self, in_chan, out_chan, act):\n        \"\"\"\n        Initialize a LUConv layer.\n\n        Args:\n            in_chan (int): Number of input channels.\n            out_chan (int): Number of output channels.\n            act (str): Activation function. Options: 'relu', 'prelu', 'elu'.\n\n        Returns:\n            None\n\n        Raises:\n            TypeError: If the activation function is not one of the specified options.\n        \"\"\"\n        super(LUConv, self).__init__()\n        self.conv1 = nn.Conv3d(in_chan, out_chan, kernel_size=3, padding=1)\n        self.bn1 = ContBatchNorm3d(out_chan)\n\n        if act == \"relu\":\n            self.activation = nn.ReLU(out_chan)\n        elif act == \"prelu\":\n            self.activation = nn.PReLU(out_chan)\n        elif act == \"elu\":\n            self.activation = nn.ELU(inplace=True)\n        else:\n            raise\n\n    def forward(self, x):\n        \"\"\"\n        Apply forward pass through the neural network.\n\n        Args:\n            x (Tensor): Input tensor to the network.\n\n        Returns:\n            Tensor: Output tensor after passing through the network.\n        \"\"\"\n        out = self.activation(self.bn1(self.conv1(x)))\n        return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.LUConv.__init__","title":"<code>__init__(in_chan, out_chan, act)</code>","text":"<p>Initialize a LUConv layer.</p> <p>Parameters:</p> Name Type Description Default <code>in_chan</code> <code>int</code> <p>Number of input channels.</p> required <code>out_chan</code> <code>int</code> <p>Number of output channels.</p> required <code>act</code> <code>str</code> <p>Activation function. Options: 'relu', 'prelu', 'elu'.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the activation function is not one of the specified options.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def __init__(self, in_chan, out_chan, act):\n    \"\"\"\n    Initialize a LUConv layer.\n\n    Args:\n        in_chan (int): Number of input channels.\n        out_chan (int): Number of output channels.\n        act (str): Activation function. Options: 'relu', 'prelu', 'elu'.\n\n    Returns:\n        None\n\n    Raises:\n        TypeError: If the activation function is not one of the specified options.\n    \"\"\"\n    super(LUConv, self).__init__()\n    self.conv1 = nn.Conv3d(in_chan, out_chan, kernel_size=3, padding=1)\n    self.bn1 = ContBatchNorm3d(out_chan)\n\n    if act == \"relu\":\n        self.activation = nn.ReLU(out_chan)\n    elif act == \"prelu\":\n        self.activation = nn.PReLU(out_chan)\n    elif act == \"elu\":\n        self.activation = nn.ELU(inplace=True)\n    else:\n        raise\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.LUConv.forward","title":"<code>forward(x)</code>","text":"<p>Apply forward pass through the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to the network.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Output tensor after passing through the network.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Apply forward pass through the neural network.\n\n    Args:\n        x (Tensor): Input tensor to the network.\n\n    Returns:\n        Tensor: Output tensor after passing through the network.\n    \"\"\"\n    out = self.activation(self.bn1(self.conv1(x)))\n    return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.OutputTransition","title":"<code>OutputTransition</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing the output transition in a neural network.</p> <p>Attributes:</p> Name Type Description <code>inChans</code> <code>int</code> <p>The number of input channels.</p> <code>n_labels</code> <code>int</code> <p>The number of output labels.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class OutputTransition(nn.Module):\n    \"\"\"\n    A class representing the output transition in a neural network.\n\n    Attributes:\n        inChans (int): The number of input channels.\n        n_labels (int): The number of output labels.\n    \"\"\"\n\n    def __init__(self, inChans, n_labels):\n        \"\"\"\n        Initialize the OutputTransition class.\n\n        Args:\n            inChans (int): Number of input channels.\n            n_labels (int): Number of output labels.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super(OutputTransition, self).__init__()\n        self.final_conv = nn.Conv3d(inChans, n_labels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through a neural network model.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            Tensor: The output tensor after passing through the model.\n        \"\"\"\n        out = self.sigmoid(self.final_conv(x))\n        return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.OutputTransition.__init__","title":"<code>__init__(inChans, n_labels)</code>","text":"<p>Initialize the OutputTransition class.</p> <p>Parameters:</p> Name Type Description Default <code>inChans</code> <code>int</code> <p>Number of input channels.</p> required <code>n_labels</code> <code>int</code> <p>Number of output labels.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def __init__(self, inChans, n_labels):\n    \"\"\"\n    Initialize the OutputTransition class.\n\n    Args:\n        inChans (int): Number of input channels.\n        n_labels (int): Number of output labels.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super(OutputTransition, self).__init__()\n    self.final_conv = nn.Conv3d(inChans, n_labels, kernel_size=1)\n    self.sigmoid = nn.Sigmoid()\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.OutputTransition.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through a neural network model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The output tensor after passing through the model.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass through a neural network model.\n\n    Args:\n        x (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The output tensor after passing through the model.\n    \"\"\"\n    out = self.sigmoid(self.final_conv(x))\n    return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UNet3D","title":"<code>UNet3D</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing a 3D UNet model for segmentation.</p> <p>Attributes:</p> Name Type Description <code>n_class</code> <code>int</code> <p>The number of classes for segmentation.</p> <code>act</code> <code>str</code> <p>The activation function type used in the model.</p> <code>decoder</code> <code>bool</code> <p>Whether to include the decoder part in the model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the model.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class UNet3D(nn.Module):\n    # the number of convolutions in each layer corresponds\n    # to what is in the actual prototxt, not the intent\n    \"\"\"\n    A class representing a 3D UNet model for segmentation.\n\n    Attributes:\n        n_class (int): The number of classes for segmentation.\n        act (str): The activation function type used in the model.\n        decoder (bool): Whether to include the decoder part in the model.\n\n    Methods:\n        forward(x): Forward pass of the model.\n    \"\"\"\n\n    def __init__(self, n_class=1, act=\"relu\", decoder=True):\n        \"\"\"\n        Initialize a 3D UNet neural network model.\n\n        Args:\n            n_class (int): The number of output classes. Defaults to 1.\n            act (str): The activation function to use. Defaults to 'relu'.\n            decoder (bool): Whether to include the decoder layers. Defaults to True.\n\n        Attributes:\n            decoder (bool): Whether the model includes decoder layers.\n            down_tr64 (DownTransition): The first down transition layer.\n            down_tr128 (DownTransition): The second down transition layer.\n            down_tr256 (DownTransition): The third down transition layer.\n            down_tr512 (DownTransition): The fourth down transition layer.\n            up_tr256 (UpTransition): The first up transition layer. (Only exists if `decoder` is True)\n            up_tr128 (UpTransition): The second up transition layer. (Only exists if `decoder` is True)\n            up_tr64 (UpTransition): The third up transition layer. (Only exists if `decoder` is True)\n            out_tr (OutputTransition): The output transition layer. (Only exists if `decoder` is True)\n            avg_pool (nn.AvgPool3d): The average pooling layer. (Only exists if `decoder` is False)\n            flatten (nn.Flatten): The flattening layer. (Only exists if `decoder` is False)\n        \"\"\"\n        super(UNet3D, self).__init__()\n\n        self.decoder = decoder\n\n        self.down_tr64 = DownTransition(1, 0, act)\n        self.down_tr128 = DownTransition(64, 1, act)\n        self.down_tr256 = DownTransition(128, 2, act)\n        self.down_tr512 = DownTransition(256, 3, act)\n\n        if self.decoder:\n            self.up_tr256 = UpTransition(512, 512, 2, act)\n            self.up_tr128 = UpTransition(256, 256, 1, act)\n            self.up_tr64 = UpTransition(128, 128, 0, act)\n            self.out_tr = OutputTransition(64, n_class)\n        else:\n            self.avg_pool = nn.AvgPool3d(3, stride=2)\n            self.flatten = nn.Flatten()\n\n    def forward(self, x):\n        \"\"\"\n        Perform forward pass through the neural network.\n\n        Args:\n            x (Tensor): Input tensor to the network.\n\n        Returns:\n            Tensor: Output tensor from the network.\n\n        Note: This function performs a series of operations to downsample the input tensor, followed by upsampling if the 'decoder' flag is set. If the 'decoder' flag is not set, the output tensor goes through average pooling and flattening.\n\n        Raises:\n            None.\n        \"\"\"\n        self.out64, self.skip_out64 = self.down_tr64(x)\n        self.out128, self.skip_out128 = self.down_tr128(self.out64)\n        self.out256, self.skip_out256 = self.down_tr256(self.out128)\n        self.out512, self.skip_out512 = self.down_tr512(self.out256)\n\n        if self.decoder:\n            self.out_up_256 = self.up_tr256(self.out512, self.skip_out256)\n            self.out_up_128 = self.up_tr128(self.out_up_256, self.skip_out128)\n            self.out_up_64 = self.up_tr64(self.out_up_128, self.skip_out64)\n            self.out = self.out_tr(self.out_up_64)\n        else:\n            self.out = self.avg_pool(self.out512)\n            self.out = self.flatten(self.out)\n\n        return self.out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UNet3D.__init__","title":"<code>__init__(n_class=1, act='relu', decoder=True)</code>","text":"<p>Initialize a 3D UNet neural network model.</p> <p>Parameters:</p> Name Type Description Default <code>n_class</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> <code>act</code> <code>str</code> <p>The activation function to use. Defaults to 'relu'.</p> <code>'relu'</code> <code>decoder</code> <code>bool</code> <p>Whether to include the decoder layers. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>decoder</code> <code>bool</code> <p>Whether the model includes decoder layers.</p> <code>down_tr64</code> <code>DownTransition</code> <p>The first down transition layer.</p> <code>down_tr128</code> <code>DownTransition</code> <p>The second down transition layer.</p> <code>down_tr256</code> <code>DownTransition</code> <p>The third down transition layer.</p> <code>down_tr512</code> <code>DownTransition</code> <p>The fourth down transition layer.</p> <code>up_tr256</code> <code>UpTransition</code> <p>The first up transition layer. (Only exists if <code>decoder</code> is True)</p> <code>up_tr128</code> <code>UpTransition</code> <p>The second up transition layer. (Only exists if <code>decoder</code> is True)</p> <code>up_tr64</code> <code>UpTransition</code> <p>The third up transition layer. (Only exists if <code>decoder</code> is True)</p> <code>out_tr</code> <code>OutputTransition</code> <p>The output transition layer. (Only exists if <code>decoder</code> is True)</p> <code>avg_pool</code> <code>AvgPool3d</code> <p>The average pooling layer. (Only exists if <code>decoder</code> is False)</p> <code>flatten</code> <code>Flatten</code> <p>The flattening layer. (Only exists if <code>decoder</code> is False)</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def __init__(self, n_class=1, act=\"relu\", decoder=True):\n    \"\"\"\n    Initialize a 3D UNet neural network model.\n\n    Args:\n        n_class (int): The number of output classes. Defaults to 1.\n        act (str): The activation function to use. Defaults to 'relu'.\n        decoder (bool): Whether to include the decoder layers. Defaults to True.\n\n    Attributes:\n        decoder (bool): Whether the model includes decoder layers.\n        down_tr64 (DownTransition): The first down transition layer.\n        down_tr128 (DownTransition): The second down transition layer.\n        down_tr256 (DownTransition): The third down transition layer.\n        down_tr512 (DownTransition): The fourth down transition layer.\n        up_tr256 (UpTransition): The first up transition layer. (Only exists if `decoder` is True)\n        up_tr128 (UpTransition): The second up transition layer. (Only exists if `decoder` is True)\n        up_tr64 (UpTransition): The third up transition layer. (Only exists if `decoder` is True)\n        out_tr (OutputTransition): The output transition layer. (Only exists if `decoder` is True)\n        avg_pool (nn.AvgPool3d): The average pooling layer. (Only exists if `decoder` is False)\n        flatten (nn.Flatten): The flattening layer. (Only exists if `decoder` is False)\n    \"\"\"\n    super(UNet3D, self).__init__()\n\n    self.decoder = decoder\n\n    self.down_tr64 = DownTransition(1, 0, act)\n    self.down_tr128 = DownTransition(64, 1, act)\n    self.down_tr256 = DownTransition(128, 2, act)\n    self.down_tr512 = DownTransition(256, 3, act)\n\n    if self.decoder:\n        self.up_tr256 = UpTransition(512, 512, 2, act)\n        self.up_tr128 = UpTransition(256, 256, 1, act)\n        self.up_tr64 = UpTransition(128, 128, 0, act)\n        self.out_tr = OutputTransition(64, n_class)\n    else:\n        self.avg_pool = nn.AvgPool3d(3, stride=2)\n        self.flatten = nn.Flatten()\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UNet3D.forward","title":"<code>forward(x)</code>","text":"<p>Perform forward pass through the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to the network.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Output tensor from the network.</p> <p>Note: This function performs a series of operations to downsample the input tensor, followed by upsampling if the 'decoder' flag is set. If the 'decoder' flag is not set, the output tensor goes through average pooling and flattening.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Perform forward pass through the neural network.\n\n    Args:\n        x (Tensor): Input tensor to the network.\n\n    Returns:\n        Tensor: Output tensor from the network.\n\n    Note: This function performs a series of operations to downsample the input tensor, followed by upsampling if the 'decoder' flag is set. If the 'decoder' flag is not set, the output tensor goes through average pooling and flattening.\n\n    Raises:\n        None.\n    \"\"\"\n    self.out64, self.skip_out64 = self.down_tr64(x)\n    self.out128, self.skip_out128 = self.down_tr128(self.out64)\n    self.out256, self.skip_out256 = self.down_tr256(self.out128)\n    self.out512, self.skip_out512 = self.down_tr512(self.out256)\n\n    if self.decoder:\n        self.out_up_256 = self.up_tr256(self.out512, self.skip_out256)\n        self.out_up_128 = self.up_tr128(self.out_up_256, self.skip_out128)\n        self.out_up_64 = self.up_tr64(self.out_up_128, self.skip_out64)\n        self.out = self.out_tr(self.out_up_64)\n    else:\n        self.out = self.avg_pool(self.out512)\n        self.out = self.flatten(self.out)\n\n    return self.out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UpTransition","title":"<code>UpTransition</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class representing an up transition layer in a neural network.</p> <p>Attributes:</p> Name Type Description <code>inChans</code> <code>int</code> <p>The number of input channels.</p> <code>outChans</code> <code>int</code> <p>The number of output channels.</p> <code>depth</code> <code>int</code> <p>The depth of the layer.</p> <code>act</code> <code>str</code> <p>The activation function to be applied.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>class UpTransition(nn.Module):\n    \"\"\"\n    A class representing an up transition layer in a neural network.\n\n    Attributes:\n        inChans (int): The number of input channels.\n        outChans (int): The number of output channels.\n        depth (int): The depth of the layer.\n        act (str): The activation function to be applied.\n    \"\"\"\n\n    def __init__(self, inChans, outChans, depth, act):\n        \"\"\"\n        Initialize the UpTransition module.\n\n        Args:\n            inChans (int): The number of input channels.\n            outChans (int): The number of output channels.\n            depth (int): The depth of the module.\n            act (nn.Module): The activation function to be used.\n\n        Returns:\n            None.\n\n        Raises:\n            None.\n        \"\"\"\n        super(UpTransition, self).__init__()\n        self.depth = depth\n        self.up_conv = nn.ConvTranspose3d(inChans, outChans, kernel_size=2, stride=2)\n        self.ops = _make_nConv(inChans + outChans // 2, depth, act, double_chnnel=True)\n\n    def forward(self, x, skip_x):\n        \"\"\"\n        Forward pass of the neural network.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            skip_x (torch.Tensor): Tensor to be concatenated with the upsampled convolution output.\n\n        Returns:\n            torch.Tensor: The output tensor after passing through the network.\n        \"\"\"\n        out_up_conv = self.up_conv(x)\n        concat = torch.cat((out_up_conv, skip_x), 1)\n        out = self.ops(concat)\n        return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UpTransition.__init__","title":"<code>__init__(inChans, outChans, depth, act)</code>","text":"<p>Initialize the UpTransition module.</p> <p>Parameters:</p> Name Type Description Default <code>inChans</code> <code>int</code> <p>The number of input channels.</p> required <code>outChans</code> <code>int</code> <p>The number of output channels.</p> required <code>depth</code> <code>int</code> <p>The depth of the module.</p> required <code>act</code> <code>Module</code> <p>The activation function to be used.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def __init__(self, inChans, outChans, depth, act):\n    \"\"\"\n    Initialize the UpTransition module.\n\n    Args:\n        inChans (int): The number of input channels.\n        outChans (int): The number of output channels.\n        depth (int): The depth of the module.\n        act (nn.Module): The activation function to be used.\n\n    Returns:\n        None.\n\n    Raises:\n        None.\n    \"\"\"\n    super(UpTransition, self).__init__()\n    self.depth = depth\n    self.up_conv = nn.ConvTranspose3d(inChans, outChans, kernel_size=2, stride=2)\n    self.ops = _make_nConv(inChans + outChans // 2, depth, act, double_chnnel=True)\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis.UpTransition.forward","title":"<code>forward(x, skip_x)</code>","text":"<p>Forward pass of the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>skip_x</code> <code>Tensor</code> <p>Tensor to be concatenated with the upsampled convolution output.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor after passing through the network.</p> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def forward(self, x, skip_x):\n    \"\"\"\n    Forward pass of the neural network.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n        skip_x (torch.Tensor): Tensor to be concatenated with the upsampled convolution output.\n\n    Returns:\n        torch.Tensor: The output tensor after passing through the network.\n    \"\"\"\n    out_up_conv = self.up_conv(x)\n    concat = torch.cat((out_up_conv, skip_x), 1)\n    out = self.ops(concat)\n    return out\n</code></pre>"},{"location":"reference/models/models_genesis/#fmcib.models.models_genesis._make_nConv","title":"<code>_make_nConv(in_channel, depth, act, double_chnnel=False)</code>","text":"<p>Make a two-layer convolutional neural network module.</p> <p>Parameters:</p> Name Type Description Default <code>in_channel</code> <code>int</code> <p>The number of input channels.</p> required <code>depth</code> <code>int</code> <p>The depth of the network.</p> required <code>act</code> <p>Activation function to be used in the network.</p> required <code>double_channel</code> <code>bool</code> <p>If True, double the number of channels in the network. Defaults to False.</p> required <p>Returns:</p> Type Description <p>nn.Sequential: A sequential module representing the two-layer convolutional network.</p> Note <ul> <li>If double_channel is True, the first layer will have 32 * 2 ** (depth + 1) channels and the second layer will have the same number of channels.</li> <li>If double_channel is False, the first layer will have 32 * 2 ** depth channels and the second layer will have 32 * 2 ** depth * 2 channels.</li> </ul> Source code in <code>fmcib/models/models_genesis.py</code> <pre><code>def _make_nConv(in_channel, depth, act, double_chnnel=False):\n    \"\"\"\n    Make a two-layer convolutional neural network module.\n\n    Args:\n        in_channel (int): The number of input channels.\n        depth (int): The depth of the network.\n        act: Activation function to be used in the network.\n        double_channel (bool, optional): If True, double the number of channels in the network. Defaults to False.\n\n    Returns:\n        nn.Sequential: A sequential module representing the two-layer convolutional network.\n\n    Note:\n        - If double_channel is True, the first layer will have 32 * 2 ** (depth + 1) channels and the second layer will have the same number of channels.\n        - If double_channel is False, the first layer will have 32 * 2 ** depth channels and the second layer will have 32 * 2 ** depth * 2 channels.\n    \"\"\"\n    if double_chnnel:\n        layer1 = LUConv(in_channel, 32 * (2 ** (depth + 1)), act)\n        layer2 = LUConv(32 * (2 ** (depth + 1)), 32 * (2 ** (depth + 1)), act)\n    else:\n        layer1 = LUConv(in_channel, 32 * (2**depth), act)\n        layer2 = LUConv(32 * (2**depth), 32 * (2**depth) * 2, act)\n\n    return nn.Sequential(layer1, layer2)\n</code></pre>"},{"location":"reference/optimizers/","title":"optimizers","text":""},{"location":"reference/optimizers/lars/","title":"lars","text":"References <ul> <li>https://arxiv.org/pdf/1708.03888.pdf</li> <li>https://github.com/pytorch/pytorch/blob/1.6/torch/optim/sgd.py</li> </ul>"},{"location":"reference/optimizers/lars/#fmcib.optimizers.lars.LARS","title":"<code>LARS</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Extends SGD in PyTorch with LARS scaling from the paper <code>Large batch training of Convolutional Networks &lt;https://arxiv.org/pdf/1708.03888.pdf&gt;</code>_. Args:     params (iterable): iterable of parameters to optimize or dicts defining         parameter groups     lr (float): learning rate     momentum (float, optional): momentum factor (default: 0)     weight_decay (float, optional): weight decay (L2 penalty) (default: 0)     dampening (float, optional): dampening for momentum (default: 0)     nesterov (bool, optional): enables Nesterov momentum (default: False)     trust_coefficient (float, optional): trust coefficient for computing LR (default: 0.001)     eps (float, optional): eps for division denominator (default: 1e-8)</p> Example <p>model = torch.nn.Linear(10, 1) input = torch.Tensor(10) target = torch.Tensor([1.]) loss_fn = lambda input, target: (input - target) ** 2</p> <p>.. note::     The application of momentum in the SGD part is modified according to     the PyTorch standards. LARS scaling fits into the equation in the     following fashion.</p> <pre><code>.. math::\n    \begin{aligned}\n        g_{t+1} &amp; =     ext{lars_lr} * (\beta * p_{t} + g_{t+1}), \\\n        v_{t+1} &amp; = \\mu * v_{t} + g_{t+1}, \\\n        p_{t+1} &amp; = p_{t} -     ext{lr} * v_{t+1},\n    \\end{aligned}\n\nwhere :math:`p`, :math:`g`, :math:`v`, :math:`\\mu` and :math:`\beta` denote the\nparameters, gradient, velocity, momentum, and weight decay respectively.\nThe :math:`lars_lr` is defined by Eq. 6 in the paper.\nThe Nesterov version is analogously modified.\n</code></pre> <p>.. warning::     Parameters with weight decay set to 0 will automatically be excluded from     layer-wise LR scaling. This is to ensure consistency with papers like SimCLR     and BYOL.</p> Source code in <code>fmcib/optimizers/lars.py</code> <pre><code>class LARS(Optimizer):\n    \"\"\"Extends SGD in PyTorch with LARS scaling from the paper\n    `Large batch training of Convolutional Networks &lt;https://arxiv.org/pdf/1708.03888.pdf&gt;`_.\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float): learning rate\n        momentum (float, optional): momentum factor (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        dampening (float, optional): dampening for momentum (default: 0)\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\n        trust_coefficient (float, optional): trust coefficient for computing LR (default: 0.001)\n        eps (float, optional): eps for division denominator (default: 1e-8)\n\n    Example:\n        &gt;&gt;&gt; model = torch.nn.Linear(10, 1)\n        &gt;&gt;&gt; input = torch.Tensor(10)\n        &gt;&gt;&gt; target = torch.Tensor([1.])\n        &gt;&gt;&gt; loss_fn = lambda input, target: (input - target) ** 2\n        &gt;&gt;&gt; #\n        &gt;&gt;&gt; optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9)\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss_fn(model(input), target).backward()\n        &gt;&gt;&gt; optimizer.step()\n\n    .. note::\n        The application of momentum in the SGD part is modified according to\n        the PyTorch standards. LARS scaling fits into the equation in the\n        following fashion.\n\n        .. math::\n            \\begin{aligned}\n                g_{t+1} &amp; = \\text{lars_lr} * (\\beta * p_{t} + g_{t+1}), \\\\\n                v_{t+1} &amp; = \\\\mu * v_{t} + g_{t+1}, \\\\\n                p_{t+1} &amp; = p_{t} - \\text{lr} * v_{t+1},\n            \\\\end{aligned}\n\n        where :math:`p`, :math:`g`, :math:`v`, :math:`\\\\mu` and :math:`\\beta` denote the\n        parameters, gradient, velocity, momentum, and weight decay respectively.\n        The :math:`lars_lr` is defined by Eq. 6 in the paper.\n        The Nesterov version is analogously modified.\n\n    .. warning::\n        Parameters with weight decay set to 0 will automatically be excluded from\n        layer-wise LR scaling. This is to ensure consistency with papers like SimCLR\n        and BYOL.\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=required,\n        momentum=0,\n        dampening=0,\n        weight_decay=0,\n        nesterov=False,\n        trust_coefficient=0.001,\n        eps=1e-8,\n    ):\n        \"\"\"\n        Initialize an optimizer with the given parameters.\n\n        Args:\n            params (iterable): Iterable of parameters to optimize.\n            lr (float, optional): Learning rate. Default is required.\n            momentum (float, optional): Momentum factor. Default is 0.\n            dampening (float, optional): Dampening for momentum. Default is 0.\n            weight_decay (float, optional): Weight decay factor. Default is 0.\n            nesterov (bool, optional): Use Nesterov momentum. Default is False.\n            trust_coefficient (float, optional): Trust coefficient. Default is 0.001.\n            eps (float, optional): Small value for numerical stability. Default is 1e-08.\n\n        Raises:\n            ValueError: If an invalid value is provided for lr, momentum, or weight_decay.\n            ValueError: If nesterov momentum is enabled without providing a momentum and zero dampening.\n        \"\"\"\n        if lr is not required and lr &lt; 0.0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if momentum &lt; 0.0:\n            raise ValueError(f\"Invalid momentum value: {momentum}\")\n        if weight_decay &lt; 0.0:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            trust_coefficient=trust_coefficient,\n            eps=eps,\n        )\n        if nesterov and (momentum &lt;= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        \"\"\"\n        Set the state of the optimizer.\n\n        Args:\n            state (dict): A dictionary containing the state of the optimizer.\n\n        Returns:\n            None\n\n        Note:\n            This method is an override of the `__setstate__` method of the superclass. It sets the state of the optimizer using the provided dictionary. Additionally, it sets the `nesterov` parameter in each group of the optimizer to `False` if it is not already present.\n        \"\"\"\n        super().__setstate__(state)\n\n        for group in self.param_groups:\n            group.setdefault(\"nesterov\", False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"\n        Performs a single optimization step.\n\n        Parameters:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        # exclude scaling for params with 0 weight decay\n        for group in self.param_groups:\n            weight_decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad\n                p_norm = torch.norm(p.data)\n                g_norm = torch.norm(p.grad.data)\n\n                # lars scaling + weight decay part\n                if weight_decay != 0:\n                    if p_norm != 0 and g_norm != 0:\n                        lars_lr = p_norm / (g_norm + p_norm * weight_decay + group[\"eps\"])\n                        lars_lr *= group[\"trust_coefficient\"]\n\n                        d_p = d_p.add(p, alpha=weight_decay)\n                        d_p *= lars_lr\n\n                # sgd part\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \"momentum_buffer\" not in param_state:\n                        buf = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                    else:\n                        buf = param_state[\"momentum_buffer\"]\n                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                    if nesterov:\n                        d_p = d_p.add(buf, alpha=momentum)\n                    else:\n                        d_p = buf\n\n                p.add_(d_p, alpha=-group[\"lr\"])\n\n        return loss\n</code></pre>"},{"location":"reference/optimizers/lars/#fmcib.optimizers.lars.LARS--_1","title":"lars","text":"<p>optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9) optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step()</p>"},{"location":"reference/optimizers/lars/#fmcib.optimizers.lars.LARS.__init__","title":"<code>__init__(params, lr=required, momentum=0, dampening=0, weight_decay=0, nesterov=False, trust_coefficient=0.001, eps=1e-08)</code>","text":"<p>Initialize an optimizer with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>iterable</code> <p>Iterable of parameters to optimize.</p> required <code>lr</code> <code>float</code> <p>Learning rate. Default is required.</p> <code>required</code> <code>momentum</code> <code>float</code> <p>Momentum factor. Default is 0.</p> <code>0</code> <code>dampening</code> <code>float</code> <p>Dampening for momentum. Default is 0.</p> <code>0</code> <code>weight_decay</code> <code>float</code> <p>Weight decay factor. Default is 0.</p> <code>0</code> <code>nesterov</code> <code>bool</code> <p>Use Nesterov momentum. Default is False.</p> <code>False</code> <code>trust_coefficient</code> <code>float</code> <p>Trust coefficient. Default is 0.001.</p> <code>0.001</code> <code>eps</code> <code>float</code> <p>Small value for numerical stability. Default is 1e-08.</p> <code>1e-08</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid value is provided for lr, momentum, or weight_decay.</p> <code>ValueError</code> <p>If nesterov momentum is enabled without providing a momentum and zero dampening.</p> Source code in <code>fmcib/optimizers/lars.py</code> <pre><code>def __init__(\n    self,\n    params,\n    lr=required,\n    momentum=0,\n    dampening=0,\n    weight_decay=0,\n    nesterov=False,\n    trust_coefficient=0.001,\n    eps=1e-8,\n):\n    \"\"\"\n    Initialize an optimizer with the given parameters.\n\n    Args:\n        params (iterable): Iterable of parameters to optimize.\n        lr (float, optional): Learning rate. Default is required.\n        momentum (float, optional): Momentum factor. Default is 0.\n        dampening (float, optional): Dampening for momentum. Default is 0.\n        weight_decay (float, optional): Weight decay factor. Default is 0.\n        nesterov (bool, optional): Use Nesterov momentum. Default is False.\n        trust_coefficient (float, optional): Trust coefficient. Default is 0.001.\n        eps (float, optional): Small value for numerical stability. Default is 1e-08.\n\n    Raises:\n        ValueError: If an invalid value is provided for lr, momentum, or weight_decay.\n        ValueError: If nesterov momentum is enabled without providing a momentum and zero dampening.\n    \"\"\"\n    if lr is not required and lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if momentum &lt; 0.0:\n        raise ValueError(f\"Invalid momentum value: {momentum}\")\n    if weight_decay &lt; 0.0:\n        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n    defaults = dict(\n        lr=lr,\n        momentum=momentum,\n        dampening=dampening,\n        weight_decay=weight_decay,\n        nesterov=nesterov,\n        trust_coefficient=trust_coefficient,\n        eps=eps,\n    )\n    if nesterov and (momentum &lt;= 0 or dampening != 0):\n        raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/optimizers/lars/#fmcib.optimizers.lars.LARS.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Set the state of the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>A dictionary containing the state of the optimizer.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>This method is an override of the <code>__setstate__</code> method of the superclass. It sets the state of the optimizer using the provided dictionary. Additionally, it sets the <code>nesterov</code> parameter in each group of the optimizer to <code>False</code> if it is not already present.</p> Source code in <code>fmcib/optimizers/lars.py</code> <pre><code>def __setstate__(self, state):\n    \"\"\"\n    Set the state of the optimizer.\n\n    Args:\n        state (dict): A dictionary containing the state of the optimizer.\n\n    Returns:\n        None\n\n    Note:\n        This method is an override of the `__setstate__` method of the superclass. It sets the state of the optimizer using the provided dictionary. Additionally, it sets the `nesterov` parameter in each group of the optimizer to `False` if it is not already present.\n    \"\"\"\n    super().__setstate__(state)\n\n    for group in self.param_groups:\n        group.setdefault(\"nesterov\", False)\n</code></pre>"},{"location":"reference/optimizers/lars/#fmcib.optimizers.lars.LARS.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> <p>Parameters:</p> Name Type Description Default <code>closure</code> <code>callable</code> <p>A closure that reevaluates the model and returns the loss.</p> <code>None</code> Source code in <code>fmcib/optimizers/lars.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure=None):\n    \"\"\"\n    Performs a single optimization step.\n\n    Parameters:\n        closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    # exclude scaling for params with 0 weight decay\n    for group in self.param_groups:\n        weight_decay = group[\"weight_decay\"]\n        momentum = group[\"momentum\"]\n        dampening = group[\"dampening\"]\n        nesterov = group[\"nesterov\"]\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n\n            d_p = p.grad\n            p_norm = torch.norm(p.data)\n            g_norm = torch.norm(p.grad.data)\n\n            # lars scaling + weight decay part\n            if weight_decay != 0:\n                if p_norm != 0 and g_norm != 0:\n                    lars_lr = p_norm / (g_norm + p_norm * weight_decay + group[\"eps\"])\n                    lars_lr *= group[\"trust_coefficient\"]\n\n                    d_p = d_p.add(p, alpha=weight_decay)\n                    d_p *= lars_lr\n\n            # sgd part\n            if momentum != 0:\n                param_state = self.state[p]\n                if \"momentum_buffer\" not in param_state:\n                    buf = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                else:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                if nesterov:\n                    d_p = d_p.add(buf, alpha=momentum)\n                else:\n                    d_p = buf\n\n            p.add_(d_p, alpha=-group[\"lr\"])\n\n    return loss\n</code></pre>"},{"location":"reference/preprocessing/","title":"preprocessing","text":""},{"location":"reference/preprocessing/seed_based_crop/","title":"seed_based_crop","text":"<p>Author: Suraj Pai Email: bspai@bwh.harvard.edu This script contains two classes: 1. SeedBasedPatchCropd 2. SeedBasedPatchCrop</p>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCrop","title":"<code>SeedBasedPatchCrop</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A class representing a seed-based patch crop transformation.</p> <p>Attributes:</p> Name Type Description <code>roi_size</code> <p>Tuple indicating the size of the region of interest (ROI).</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Crop a patch from the input image centered around the seed coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <p>Tuple indicating the size of the region of interest (ROI).</p> required <p>Returns:</p> Name Type Description <code>NdarrayOrTensor</code> <p>Cropped patch of shape (C, Ph, Pw, Pd), where (Ph, Pw, Pd) is the patch size.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the input image has dimensions other than (C, H, W, D)</p> <code>AssertionError</code> <p>If the coordinates are invalid (e.g., min_h &gt;= max_h)</p> Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>class SeedBasedPatchCrop(Transform):\n    \"\"\"\n    A class representing a seed-based patch crop transformation.\n\n    Attributes:\n        roi_size: Tuple indicating the size of the region of interest (ROI).\n\n    Methods:\n        __call__: Crop a patch from the input image centered around the seed coordinate.\n\n    Args:\n        roi_size: Tuple indicating the size of the region of interest (ROI).\n\n    Returns:\n        NdarrayOrTensor: Cropped patch of shape (C, Ph, Pw, Pd), where (Ph, Pw, Pd) is the patch size.\n\n    Raises:\n        AssertionError: If the input image has dimensions other than (C, H, W, D)\n        AssertionError: If the coordinates are invalid (e.g., min_h &gt;= max_h)\n    \"\"\"\n\n    def __init__(self, roi_size) -&gt; None:\n        \"\"\"\n        Initialize SeedBasedPatchCrop class.\n\n        Args:\n            roi_size (tuple): Tuple indicating the size of the region of interest (ROI).\n        \"\"\"\n        super().__init__()\n        self.roi_size = roi_size\n\n    def __call__(self, img: NdarrayOrTensor, center: tuple, global_coordinates=False) -&gt; NdarrayOrTensor:\n        \"\"\"\n        Crop a patch from the input image centered around the seed coordinate.\n\n        Args:\n            img (NdarrayOrTensor): Image to crop, with dimensions (C, H, W, D). C is the number of channels.\n            center (tuple): Seed coordinate around which to crop the patch (X, Y, Z).\n            global_coordinates (bool): If True, seed coordinate is in global space; otherwise, local space.\n\n        Returns:\n            NdarrayOrTensor: Cropped patch of shape (C, Ph, Pw, Pd), where (Ph, Pw, Pd) is the patch size.\n        \"\"\"\n        assert len(img.shape) == 4, \"Input image must have dimensions: (C, H, W, D)\"\n        C, H, W, D = img.shape\n        Ph, Pw, Pd = self.roi_size\n\n        # If global coordinates, convert to local coordinates\n        if global_coordinates:\n            center = np.linalg.inv(np.array(img.affine)) @ np.array(center + (1,))\n            center = [int(x) for x in center[:3]]\n\n        # Calculate and clamp the ranges for cropping\n        min_h, max_h = max(center[0] - Ph // 2, 0), min(center[0] + Ph // 2, H)\n        min_w, max_w = max(center[1] - Pw // 2, 0), min(center[1] + Pw // 2, W)\n        min_d, max_d = max(center[2] - Pd // 2, 0), min(center[2] + Pd // 2, D)\n\n        # Check if coordinates are valid\n        assert min_h &lt; max_h, \"Invalid coordinates: min_h &gt;= max_h\"\n        assert min_w &lt; max_w, \"Invalid coordinates: min_w &gt;= max_w\"\n        assert min_d &lt; max_d, \"Invalid coordinates: min_d &gt;= max_d\"\n\n        # Crop the patch from the image\n        patch = img[:, min_h:max_h, min_w:max_w, min_d:max_d]\n\n        return patch\n</code></pre>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCrop.__call__","title":"<code>__call__(img, center, global_coordinates=False)</code>","text":"<p>Crop a patch from the input image centered around the seed coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>NdarrayOrTensor</code> <p>Image to crop, with dimensions (C, H, W, D). C is the number of channels.</p> required <code>center</code> <code>tuple</code> <p>Seed coordinate around which to crop the patch (X, Y, Z).</p> required <code>global_coordinates</code> <code>bool</code> <p>If True, seed coordinate is in global space; otherwise, local space.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>NdarrayOrTensor</code> <code>NdarrayOrTensor</code> <p>Cropped patch of shape (C, Ph, Pw, Pd), where (Ph, Pw, Pd) is the patch size.</p> Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>def __call__(self, img: NdarrayOrTensor, center: tuple, global_coordinates=False) -&gt; NdarrayOrTensor:\n    \"\"\"\n    Crop a patch from the input image centered around the seed coordinate.\n\n    Args:\n        img (NdarrayOrTensor): Image to crop, with dimensions (C, H, W, D). C is the number of channels.\n        center (tuple): Seed coordinate around which to crop the patch (X, Y, Z).\n        global_coordinates (bool): If True, seed coordinate is in global space; otherwise, local space.\n\n    Returns:\n        NdarrayOrTensor: Cropped patch of shape (C, Ph, Pw, Pd), where (Ph, Pw, Pd) is the patch size.\n    \"\"\"\n    assert len(img.shape) == 4, \"Input image must have dimensions: (C, H, W, D)\"\n    C, H, W, D = img.shape\n    Ph, Pw, Pd = self.roi_size\n\n    # If global coordinates, convert to local coordinates\n    if global_coordinates:\n        center = np.linalg.inv(np.array(img.affine)) @ np.array(center + (1,))\n        center = [int(x) for x in center[:3]]\n\n    # Calculate and clamp the ranges for cropping\n    min_h, max_h = max(center[0] - Ph // 2, 0), min(center[0] + Ph // 2, H)\n    min_w, max_w = max(center[1] - Pw // 2, 0), min(center[1] + Pw // 2, W)\n    min_d, max_d = max(center[2] - Pd // 2, 0), min(center[2] + Pd // 2, D)\n\n    # Check if coordinates are valid\n    assert min_h &lt; max_h, \"Invalid coordinates: min_h &gt;= max_h\"\n    assert min_w &lt; max_w, \"Invalid coordinates: min_w &gt;= max_w\"\n    assert min_d &lt; max_d, \"Invalid coordinates: min_d &gt;= max_d\"\n\n    # Crop the patch from the image\n    patch = img[:, min_h:max_h, min_w:max_w, min_d:max_d]\n\n    return patch\n</code></pre>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCrop.__init__","title":"<code>__init__(roi_size)</code>","text":"<p>Initialize SeedBasedPatchCrop class.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <code>tuple</code> <p>Tuple indicating the size of the region of interest (ROI).</p> required Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>def __init__(self, roi_size) -&gt; None:\n    \"\"\"\n    Initialize SeedBasedPatchCrop class.\n\n    Args:\n        roi_size (tuple): Tuple indicating the size of the region of interest (ROI).\n    \"\"\"\n    super().__init__()\n    self.roi_size = roi_size\n</code></pre>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCropd","title":"<code>SeedBasedPatchCropd</code>","text":"<p>               Bases: <code>MapTransform</code></p> <p>A class representing a seed-based patch crop transformation.</p> <p>Inherits from MapTransform.</p> <p>Attributes:</p> Name Type Description <code>keys</code> <code>list</code> <p>List of keys for images in the input data dictionary.</p> <code>roi_size</code> <code>tuple</code> <p>Tuple indicating the size of the region of interest (ROI).</p> <code>allow_missing_keys</code> <code>bool</code> <p>If True, do not raise an error if some keys in the input data dictionary are missing.</p> <code>coord_orientation</code> <code>str</code> <p>Coordinate system (RAS or LPS) of input coordinates.</p> <code>global_coordinates</code> <code>bool</code> <p>If True, coordinates are in global coordinates; otherwise, local coordinates.</p> Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>class SeedBasedPatchCropd(MapTransform):\n    \"\"\"\n    A class representing a seed-based patch crop transformation.\n\n    Inherits from MapTransform.\n\n    Attributes:\n        keys (list): List of keys for images in the input data dictionary.\n        roi_size (tuple): Tuple indicating the size of the region of interest (ROI).\n        allow_missing_keys (bool): If True, do not raise an error if some keys in the input data dictionary are missing.\n        coord_orientation (str): Coordinate system (RAS or LPS) of input coordinates.\n        global_coordinates (bool): If True, coordinates are in global coordinates; otherwise, local coordinates.\n    \"\"\"\n\n    def __init__(self, keys, roi_size, allow_missing_keys=False, coord_orientation=\"RAS\", global_coordinates=True) -&gt; None:\n        \"\"\"\n        Initialize SeedBasedPatchCropd class.\n\n        Args:\n            keys (List): List of keys for images in the input data dictionary.\n            roi_size (Tuple): Tuple indicating the size of the region of interest (ROI).\n            allow_missing_keys (bool): If True, do not raise an error if some keys in the input data dictionary are missing.\n            coord_orientation (str): Coordinate system (RAS or LPS) of input coordinates.\n            global_coordinates (bool): If True, coordinates are in global coordinates; otherwise, local coordinates.\n        \"\"\"\n        super().__init__(keys=keys, allow_missing_keys=allow_missing_keys)\n        self.coord_orientation = coord_orientation\n        self.global_coordinates = global_coordinates\n        self.cropper = SeedBasedPatchCrop(roi_size=roi_size)\n\n    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -&gt; Dict[Hashable, NdarrayOrTensor]:\n        \"\"\"\n        Apply transformation to given data.\n\n        Args:\n            data (dict): Dictionary with image keys and required center coordinates.\n\n        Returns:\n            dict: Dictionary with cropped patches for each key in the input data dictionary.\n        \"\"\"\n        d = dict(data)\n\n        assert \"coordX\" in d.keys(), \"coordX not found in data\"\n        assert \"coordY\" in d.keys(), \"coordY not found in data\"\n        assert \"coordZ\" in d.keys(), \"coordZ not found in data\"\n\n        # Convert coordinates to RAS orientation to match image orientation\n        if self.coord_orientation == \"RAS\":\n            center = (d[\"coordX\"], d[\"coordY\"], d[\"coordZ\"])\n        elif self.coord_orientation == \"LPS\":\n            center = (-d[\"coordX\"], -d[\"coordY\"], d[\"coordZ\"])\n\n        for key in self.key_iterator(d):\n            d[key] = self.cropper(d[key], center=center, global_coordinates=self.global_coordinates)\n        return d\n</code></pre>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCropd.__call__","title":"<code>__call__(data)</code>","text":"<p>Apply transformation to given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary with image keys and required center coordinates.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[Hashable, NdarrayOrTensor]</code> <p>Dictionary with cropped patches for each key in the input data dictionary.</p> Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -&gt; Dict[Hashable, NdarrayOrTensor]:\n    \"\"\"\n    Apply transformation to given data.\n\n    Args:\n        data (dict): Dictionary with image keys and required center coordinates.\n\n    Returns:\n        dict: Dictionary with cropped patches for each key in the input data dictionary.\n    \"\"\"\n    d = dict(data)\n\n    assert \"coordX\" in d.keys(), \"coordX not found in data\"\n    assert \"coordY\" in d.keys(), \"coordY not found in data\"\n    assert \"coordZ\" in d.keys(), \"coordZ not found in data\"\n\n    # Convert coordinates to RAS orientation to match image orientation\n    if self.coord_orientation == \"RAS\":\n        center = (d[\"coordX\"], d[\"coordY\"], d[\"coordZ\"])\n    elif self.coord_orientation == \"LPS\":\n        center = (-d[\"coordX\"], -d[\"coordY\"], d[\"coordZ\"])\n\n    for key in self.key_iterator(d):\n        d[key] = self.cropper(d[key], center=center, global_coordinates=self.global_coordinates)\n    return d\n</code></pre>"},{"location":"reference/preprocessing/seed_based_crop/#fmcib.preprocessing.seed_based_crop.SeedBasedPatchCropd.__init__","title":"<code>__init__(keys, roi_size, allow_missing_keys=False, coord_orientation='RAS', global_coordinates=True)</code>","text":"<p>Initialize SeedBasedPatchCropd class.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List</code> <p>List of keys for images in the input data dictionary.</p> required <code>roi_size</code> <code>Tuple</code> <p>Tuple indicating the size of the region of interest (ROI).</p> required <code>allow_missing_keys</code> <code>bool</code> <p>If True, do not raise an error if some keys in the input data dictionary are missing.</p> <code>False</code> <code>coord_orientation</code> <code>str</code> <p>Coordinate system (RAS or LPS) of input coordinates.</p> <code>'RAS'</code> <code>global_coordinates</code> <code>bool</code> <p>If True, coordinates are in global coordinates; otherwise, local coordinates.</p> <code>True</code> Source code in <code>fmcib/preprocessing/seed_based_crop.py</code> <pre><code>def __init__(self, keys, roi_size, allow_missing_keys=False, coord_orientation=\"RAS\", global_coordinates=True) -&gt; None:\n    \"\"\"\n    Initialize SeedBasedPatchCropd class.\n\n    Args:\n        keys (List): List of keys for images in the input data dictionary.\n        roi_size (Tuple): Tuple indicating the size of the region of interest (ROI).\n        allow_missing_keys (bool): If True, do not raise an error if some keys in the input data dictionary are missing.\n        coord_orientation (str): Coordinate system (RAS or LPS) of input coordinates.\n        global_coordinates (bool): If True, coordinates are in global coordinates; otherwise, local coordinates.\n    \"\"\"\n    super().__init__(keys=keys, allow_missing_keys=allow_missing_keys)\n    self.coord_orientation = coord_orientation\n    self.global_coordinates = global_coordinates\n    self.cropper = SeedBasedPatchCrop(roi_size=roi_size)\n</code></pre>"},{"location":"reference/ssl/","title":"ssl","text":""},{"location":"reference/ssl/losses/","title":"losses","text":""},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/","title":"neg_mining_info_nce_loss","text":""},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion","title":"<code>NegativeMiningInfoNCECriterion</code>","text":"<p>               Bases: <code>Module</code></p> <p>The criterion corresponding to the SimCLR loss as defined in the paper https://arxiv.org/abs/2002.05709.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature to be applied on the logits.</p> <code>0.1</code> <code>buffer_params</code> <code>dict</code> <p>A dictionary containing the following keys: - world_size (int): Total number of trainers in training. - embedding_dim (int): Output dimensions of the features projects. - effective_batch_size (int): Total batch size used (includes positives).</p> required Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>class NegativeMiningInfoNCECriterion(nn.Module):\n    \"\"\"\n    The criterion corresponding to the SimCLR loss as defined in the paper\n    https://arxiv.org/abs/2002.05709.\n\n    Args:\n        temperature (float): The temperature to be applied on the logits.\n        buffer_params (dict): A dictionary containing the following keys:\n            - world_size (int): Total number of trainers in training.\n            - embedding_dim (int): Output dimensions of the features projects.\n            - effective_batch_size (int): Total batch size used (includes positives).\n    \"\"\"\n\n    def __init__(\n        self, embedding_dim, batch_size, world_size, gather_distributed=False, temperature: float = 0.1, balanced: bool = True\n    ):\n        \"\"\"\n        Initialize the NegativeMiningInfoNCECriterion class.\n\n        Args:\n            embedding_dim (int): The dimension of the embedding space.\n            batch_size (int): The size of the input batch.\n            world_size (int): The number of distributed processes.\n            gather_distributed (bool): Whether to gather distributed data.\n            temperature (float): The temperature used in the computation.\n            balanced (bool): Whether to use balanced sampling.\n\n        Attributes:\n            embedding_dim (int): The dimension of the embedding space.\n            use_gpu (bool): Whether to use GPU for computations.\n            temperature (float): The temperature used in the computation.\n            num_pos (int): The number of positive samples.\n            num_neg (int): The number of negative samples.\n            criterion (nn.CrossEntropyLoss): The loss function.\n            gather_distributed (bool): Whether to gather distributed data.\n            world_size (int): The number of distributed processes.\n            effective_batch_size (int): The effective batch size, taking into account world size and number of positive samples.\n            pos_mask (None or Tensor): Mask for positive samples.\n            neg_mask (None or Tensor): Mask for negative samples.\n            balanced (bool): Whether to use balanced sampling.\n            setup (bool): Whether the setup has been done.\n        \"\"\"\n        super(NegativeMiningInfoNCECriterion, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.use_gpu = torch.cuda.is_available()\n        self.temperature = temperature\n        self.num_pos = 2\n\n        # Same number of negatives as positives are loaded\n        self.num_neg = self.num_pos\n        self.criterion = nn.CrossEntropyLoss()\n        self.gather_distributed = gather_distributed\n        self.world_size = world_size\n        self.effective_batch_size = batch_size * self.world_size * self.num_pos\n        self.pos_mask = None\n        self.neg_mask = None\n        self.balanced = balanced\n        self.setup = False\n\n    def precompute_pos_neg_mask(self):\n        \"\"\"\n        Precompute the positive and negative masks to speed up the loss calculation.\n        \"\"\"\n        # computed once at the begining of training\n\n        # total_images is x2 SimCLR Info-NCE loss\n        # as we have negative samples for each positive sample\n\n        total_images = self.effective_batch_size * self.num_neg\n        world_size = self.world_size\n\n        # Batch size computation is different from SimCLR paper\n        batch_size = self.effective_batch_size // world_size\n        orig_images = batch_size // self.num_pos\n        rank = dist.rank()\n\n        pos_mask = torch.zeros(batch_size * self.num_neg, total_images)\n        neg_mask = torch.zeros(batch_size * self.num_neg, total_images)\n\n        all_indices = np.arange(total_images)\n\n        # Index for pairs of images (original + copy)\n        pairs = orig_images * np.arange(self.num_pos)\n\n        # Remove all indices associated with positive samples &amp; copies (for neg_mask)\n        all_pos_members = []\n        for _rank in range(world_size):\n            all_pos_members += list(_rank * (batch_size * 2) + np.arange(batch_size))\n\n        all_indices_pos_removed = np.delete(all_indices, all_pos_members)\n\n        # Index of original positive images\n        orig_members = torch.arange(orig_images)\n\n        for anchor in np.arange(self.num_pos):\n            for img_idx in range(orig_images):\n                # delete_inds are spaced by batch_size for each rank as\n                # all_indices_pos_removed (half of the indices) is deleted first\n                delete_inds = batch_size * rank + img_idx + pairs\n                neg_inds = torch.tensor(np.delete(all_indices_pos_removed, delete_inds)).long()\n                neg_mask[anchor * orig_images + img_idx, neg_inds] = 1\n\n            for pos in np.delete(np.arange(self.num_pos), anchor):\n                # Pos_inds are spaced by batch_size * self.num_neg for each rank\n                pos_inds = (batch_size * self.num_neg) * rank + pos * orig_images + orig_members\n                pos_mask[\n                    torch.arange(anchor * orig_images, (anchor + 1) * orig_images).long(),\n                    pos_inds.long(),\n                ] = 1\n\n        self.pos_mask = pos_mask.cuda(non_blocking=True) if self.use_gpu else pos_mask\n        self.neg_mask = neg_mask.cuda(non_blocking=True) if self.use_gpu else neg_mask\n\n    def forward(self, out: torch.Tensor):\n        \"\"\"\n        Calculate the loss. Operates on embeddings tensor.\n        \"\"\"\n        if not self.setup:\n            logger.info(f\"Running Negative Mining Info-NCE loss on Rank: {dist.rank()}\")\n            self.precompute_pos_neg_mask()\n            self.setup = True\n\n        pos0, pos1 = out[\"positive\"]\n        neg0, neg1 = out[\"negative\"]\n        embedding = torch.cat([pos0, pos1, neg0, neg1], dim=0)\n        embedding = nn.functional.normalize(embedding, dim=1, p=2)\n        assert embedding.ndim == 2\n        assert embedding.shape[1] == int(self.embedding_dim)\n\n        batch_size = embedding.shape[0]\n        T = self.temperature\n        num_pos = self.num_pos\n\n        assert batch_size % num_pos == 0, \"Batch size should be divisible by num_pos\"\n        assert batch_size == self.pos_mask.shape[0], \"Batch size should be equal to pos_mask shape\"\n\n        # Step 1: gather all the embeddings. Shape example: 4096 x 128\n        embeddings_buffer = self.gather_embeddings(embedding)\n\n        # Step 2: matrix multiply: 64 x 128 with 4096 x 128 = 64 x 4096 and\n        # divide by temperature.\n        similarity = torch.exp(torch.mm(embedding, embeddings_buffer.t()) / T)\n\n        pos = torch.sum(similarity * self.pos_mask, 1)\n        neg = torch.sum(similarity * self.neg_mask, 1)\n\n        # Ignore the negative samples as entries for loss calculation\n        pos = pos[: (batch_size // 2)]\n        neg = neg[: (batch_size // 2)]\n\n        loss = -(torch.mean(torch.log(pos / (pos + neg))))\n        return loss\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the object.\n\n        Returns:\n            str: A formatted string representation of the object.\n\n        Examples:\n            The following example shows the string representation of the object:\n\n            {\n              'name': &lt;object_name&gt;,\n              'temperature': &lt;temperature_value&gt;,\n              'num_negatives': &lt;num_negatives_value&gt;,\n              'num_pos': &lt;num_pos_value&gt;,\n              'dist_rank': &lt;dist_rank_value&gt;\n            }\n\n        Note:\n            This function is intended to be used with the pprint module for pretty printing.\n        \"\"\"\n        num_negatives = self.effective_batch_size - 2\n        T = self.temperature\n        num_pos = self.num_pos\n        repr_dict = {\n            \"name\": self._get_name(),\n            \"temperature\": T,\n            \"num_negatives\": num_negatives,\n            \"num_pos\": num_pos,\n            \"dist_rank\": dist.rank(),\n        }\n        return pprint.pformat(repr_dict, indent=2)\n\n    def gather_embeddings(self, embedding: torch.Tensor):\n        \"\"\"\n        Do a gather over all embeddings, so we can compute the loss.\n        Final shape is like: (batch_size * num_gpus) x embedding_dim\n        \"\"\"\n        if self.gather_distributed:\n            embedding_gathered = torch.cat(dist.gather(embedding), 0)\n        else:\n            embedding_gathered = embedding\n        return embedding_gathered\n</code></pre>"},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.__init__","title":"<code>__init__(embedding_dim, batch_size, world_size, gather_distributed=False, temperature=0.1, balanced=True)</code>","text":"<p>Initialize the NegativeMiningInfoNCECriterion class.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>The dimension of the embedding space.</p> required <code>batch_size</code> <code>int</code> <p>The size of the input batch.</p> required <code>world_size</code> <code>int</code> <p>The number of distributed processes.</p> required <code>gather_distributed</code> <code>bool</code> <p>Whether to gather distributed data.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature used in the computation.</p> <code>0.1</code> <code>balanced</code> <code>bool</code> <p>Whether to use balanced sampling.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>embedding_dim</code> <code>int</code> <p>The dimension of the embedding space.</p> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU for computations.</p> <code>temperature</code> <code>float</code> <p>The temperature used in the computation.</p> <code>num_pos</code> <code>int</code> <p>The number of positive samples.</p> <code>num_neg</code> <code>int</code> <p>The number of negative samples.</p> <code>criterion</code> <code>CrossEntropyLoss</code> <p>The loss function.</p> <code>gather_distributed</code> <code>bool</code> <p>Whether to gather distributed data.</p> <code>world_size</code> <code>int</code> <p>The number of distributed processes.</p> <code>effective_batch_size</code> <code>int</code> <p>The effective batch size, taking into account world size and number of positive samples.</p> <code>pos_mask</code> <code>None or Tensor</code> <p>Mask for positive samples.</p> <code>neg_mask</code> <code>None or Tensor</code> <p>Mask for negative samples.</p> <code>balanced</code> <code>bool</code> <p>Whether to use balanced sampling.</p> <code>setup</code> <code>bool</code> <p>Whether the setup has been done.</p> Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>def __init__(\n    self, embedding_dim, batch_size, world_size, gather_distributed=False, temperature: float = 0.1, balanced: bool = True\n):\n    \"\"\"\n    Initialize the NegativeMiningInfoNCECriterion class.\n\n    Args:\n        embedding_dim (int): The dimension of the embedding space.\n        batch_size (int): The size of the input batch.\n        world_size (int): The number of distributed processes.\n        gather_distributed (bool): Whether to gather distributed data.\n        temperature (float): The temperature used in the computation.\n        balanced (bool): Whether to use balanced sampling.\n\n    Attributes:\n        embedding_dim (int): The dimension of the embedding space.\n        use_gpu (bool): Whether to use GPU for computations.\n        temperature (float): The temperature used in the computation.\n        num_pos (int): The number of positive samples.\n        num_neg (int): The number of negative samples.\n        criterion (nn.CrossEntropyLoss): The loss function.\n        gather_distributed (bool): Whether to gather distributed data.\n        world_size (int): The number of distributed processes.\n        effective_batch_size (int): The effective batch size, taking into account world size and number of positive samples.\n        pos_mask (None or Tensor): Mask for positive samples.\n        neg_mask (None or Tensor): Mask for negative samples.\n        balanced (bool): Whether to use balanced sampling.\n        setup (bool): Whether the setup has been done.\n    \"\"\"\n    super(NegativeMiningInfoNCECriterion, self).__init__()\n    self.embedding_dim = embedding_dim\n    self.use_gpu = torch.cuda.is_available()\n    self.temperature = temperature\n    self.num_pos = 2\n\n    # Same number of negatives as positives are loaded\n    self.num_neg = self.num_pos\n    self.criterion = nn.CrossEntropyLoss()\n    self.gather_distributed = gather_distributed\n    self.world_size = world_size\n    self.effective_batch_size = batch_size * self.world_size * self.num_pos\n    self.pos_mask = None\n    self.neg_mask = None\n    self.balanced = balanced\n    self.setup = False\n</code></pre>"},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A formatted string representation of the object.</p> <p>Examples:</p> <p>The following example shows the string representation of the object:</p> <p>{   'name': ,   'temperature': ,   'num_negatives': ,   'num_pos': ,   'dist_rank':  } Note <p>This function is intended to be used with the pprint module for pretty printing.</p> Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Return a string representation of the object.\n\n    Returns:\n        str: A formatted string representation of the object.\n\n    Examples:\n        The following example shows the string representation of the object:\n\n        {\n          'name': &lt;object_name&gt;,\n          'temperature': &lt;temperature_value&gt;,\n          'num_negatives': &lt;num_negatives_value&gt;,\n          'num_pos': &lt;num_pos_value&gt;,\n          'dist_rank': &lt;dist_rank_value&gt;\n        }\n\n    Note:\n        This function is intended to be used with the pprint module for pretty printing.\n    \"\"\"\n    num_negatives = self.effective_batch_size - 2\n    T = self.temperature\n    num_pos = self.num_pos\n    repr_dict = {\n        \"name\": self._get_name(),\n        \"temperature\": T,\n        \"num_negatives\": num_negatives,\n        \"num_pos\": num_pos,\n        \"dist_rank\": dist.rank(),\n    }\n    return pprint.pformat(repr_dict, indent=2)\n</code></pre>"},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.forward","title":"<code>forward(out)</code>","text":"<p>Calculate the loss. Operates on embeddings tensor.</p> Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>def forward(self, out: torch.Tensor):\n    \"\"\"\n    Calculate the loss. Operates on embeddings tensor.\n    \"\"\"\n    if not self.setup:\n        logger.info(f\"Running Negative Mining Info-NCE loss on Rank: {dist.rank()}\")\n        self.precompute_pos_neg_mask()\n        self.setup = True\n\n    pos0, pos1 = out[\"positive\"]\n    neg0, neg1 = out[\"negative\"]\n    embedding = torch.cat([pos0, pos1, neg0, neg1], dim=0)\n    embedding = nn.functional.normalize(embedding, dim=1, p=2)\n    assert embedding.ndim == 2\n    assert embedding.shape[1] == int(self.embedding_dim)\n\n    batch_size = embedding.shape[0]\n    T = self.temperature\n    num_pos = self.num_pos\n\n    assert batch_size % num_pos == 0, \"Batch size should be divisible by num_pos\"\n    assert batch_size == self.pos_mask.shape[0], \"Batch size should be equal to pos_mask shape\"\n\n    # Step 1: gather all the embeddings. Shape example: 4096 x 128\n    embeddings_buffer = self.gather_embeddings(embedding)\n\n    # Step 2: matrix multiply: 64 x 128 with 4096 x 128 = 64 x 4096 and\n    # divide by temperature.\n    similarity = torch.exp(torch.mm(embedding, embeddings_buffer.t()) / T)\n\n    pos = torch.sum(similarity * self.pos_mask, 1)\n    neg = torch.sum(similarity * self.neg_mask, 1)\n\n    # Ignore the negative samples as entries for loss calculation\n    pos = pos[: (batch_size // 2)]\n    neg = neg[: (batch_size // 2)]\n\n    loss = -(torch.mean(torch.log(pos / (pos + neg))))\n    return loss\n</code></pre>"},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.gather_embeddings","title":"<code>gather_embeddings(embedding)</code>","text":"<p>Do a gather over all embeddings, so we can compute the loss. Final shape is like: (batch_size * num_gpus) x embedding_dim</p> Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>def gather_embeddings(self, embedding: torch.Tensor):\n    \"\"\"\n    Do a gather over all embeddings, so we can compute the loss.\n    Final shape is like: (batch_size * num_gpus) x embedding_dim\n    \"\"\"\n    if self.gather_distributed:\n        embedding_gathered = torch.cat(dist.gather(embedding), 0)\n    else:\n        embedding_gathered = embedding\n    return embedding_gathered\n</code></pre>"},{"location":"reference/ssl/losses/neg_mining_info_nce_loss/#fmcib.ssl.losses.neg_mining_info_nce_loss.NegativeMiningInfoNCECriterion.precompute_pos_neg_mask","title":"<code>precompute_pos_neg_mask()</code>","text":"<p>Precompute the positive and negative masks to speed up the loss calculation.</p> Source code in <code>fmcib/ssl/losses/neg_mining_info_nce_loss.py</code> <pre><code>def precompute_pos_neg_mask(self):\n    \"\"\"\n    Precompute the positive and negative masks to speed up the loss calculation.\n    \"\"\"\n    # computed once at the begining of training\n\n    # total_images is x2 SimCLR Info-NCE loss\n    # as we have negative samples for each positive sample\n\n    total_images = self.effective_batch_size * self.num_neg\n    world_size = self.world_size\n\n    # Batch size computation is different from SimCLR paper\n    batch_size = self.effective_batch_size // world_size\n    orig_images = batch_size // self.num_pos\n    rank = dist.rank()\n\n    pos_mask = torch.zeros(batch_size * self.num_neg, total_images)\n    neg_mask = torch.zeros(batch_size * self.num_neg, total_images)\n\n    all_indices = np.arange(total_images)\n\n    # Index for pairs of images (original + copy)\n    pairs = orig_images * np.arange(self.num_pos)\n\n    # Remove all indices associated with positive samples &amp; copies (for neg_mask)\n    all_pos_members = []\n    for _rank in range(world_size):\n        all_pos_members += list(_rank * (batch_size * 2) + np.arange(batch_size))\n\n    all_indices_pos_removed = np.delete(all_indices, all_pos_members)\n\n    # Index of original positive images\n    orig_members = torch.arange(orig_images)\n\n    for anchor in np.arange(self.num_pos):\n        for img_idx in range(orig_images):\n            # delete_inds are spaced by batch_size for each rank as\n            # all_indices_pos_removed (half of the indices) is deleted first\n            delete_inds = batch_size * rank + img_idx + pairs\n            neg_inds = torch.tensor(np.delete(all_indices_pos_removed, delete_inds)).long()\n            neg_mask[anchor * orig_images + img_idx, neg_inds] = 1\n\n        for pos in np.delete(np.arange(self.num_pos), anchor):\n            # Pos_inds are spaced by batch_size * self.num_neg for each rank\n            pos_inds = (batch_size * self.num_neg) * rank + pos * orig_images + orig_members\n            pos_mask[\n                torch.arange(anchor * orig_images, (anchor + 1) * orig_images).long(),\n                pos_inds.long(),\n            ] = 1\n\n    self.pos_mask = pos_mask.cuda(non_blocking=True) if self.use_gpu else pos_mask\n    self.neg_mask = neg_mask.cuda(non_blocking=True) if self.use_gpu else neg_mask\n</code></pre>"},{"location":"reference/ssl/losses/nnclr_loss/","title":"nnclr_loss","text":""},{"location":"reference/ssl/losses/nnclr_loss/#fmcib.ssl.losses.nnclr_loss.NNCLRLoss","title":"<code>NNCLRLoss</code>","text":"<p>               Bases: <code>NTXentLoss</code></p> <p>A class representing the NNCLRLoss.</p> <p>This class extends the NTXentLoss class and implements a symmetric loss function for NNCLR.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>The temperature for the loss function. Default is 0.1.</p> <code>gather_distributed</code> <code>bool</code> <p>A flag indicating whether the distributed gathering is used. Default is False.</p> Source code in <code>fmcib/ssl/losses/nnclr_loss.py</code> <pre><code>class NNCLRLoss(NTXentLoss):\n    \"\"\"\n    A class representing the NNCLRLoss.\n\n    This class extends the NTXentLoss class and implements a symmetric loss function for NNCLR.\n\n    Attributes:\n        temperature (float): The temperature for the loss function. Default is 0.1.\n        gather_distributed (bool): A flag indicating whether the distributed gathering is used. Default is False.\n    \"\"\"\n\n    def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n        \"\"\"\n        Initialize a new instance of the class.\n\n        Args:\n            temperature (float): The temperature to use for initialization. Default value is 0.1.\n            gather_distributed (bool): Whether to use gather distributed mode. Default value is False.\n        \"\"\"\n        super().__init__(temperature, gather_distributed)\n\n    def forward(self, out):\n        \"\"\"\n        Symmetric loss function for NNCLR.\n        \"\"\"\n        (z0, p0), (z1, p1) = out\n        loss0 = super().forward(z0, p0)\n        loss1 = super().forward(z1, p1)\n        return (loss0 + loss1) / 2\n</code></pre>"},{"location":"reference/ssl/losses/nnclr_loss/#fmcib.ssl.losses.nnclr_loss.NNCLRLoss.__init__","title":"<code>__init__(temperature=0.1, gather_distributed=False)</code>","text":"<p>Initialize a new instance of the class.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature to use for initialization. Default value is 0.1.</p> <code>0.1</code> <code>gather_distributed</code> <code>bool</code> <p>Whether to use gather distributed mode. Default value is False.</p> <code>False</code> Source code in <code>fmcib/ssl/losses/nnclr_loss.py</code> <pre><code>def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n    \"\"\"\n    Initialize a new instance of the class.\n\n    Args:\n        temperature (float): The temperature to use for initialization. Default value is 0.1.\n        gather_distributed (bool): Whether to use gather distributed mode. Default value is False.\n    \"\"\"\n    super().__init__(temperature, gather_distributed)\n</code></pre>"},{"location":"reference/ssl/losses/nnclr_loss/#fmcib.ssl.losses.nnclr_loss.NNCLRLoss.forward","title":"<code>forward(out)</code>","text":"<p>Symmetric loss function for NNCLR.</p> Source code in <code>fmcib/ssl/losses/nnclr_loss.py</code> <pre><code>def forward(self, out):\n    \"\"\"\n    Symmetric loss function for NNCLR.\n    \"\"\"\n    (z0, p0), (z1, p1) = out\n    loss0 = super().forward(z0, p0)\n    loss1 = super().forward(z1, p1)\n    return (loss0 + loss1) / 2\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_loss/","title":"ntxent_loss","text":""},{"location":"reference/ssl/losses/ntxent_loss/#fmcib.ssl.losses.ntxent_loss.NTXentLoss","title":"<code>NTXentLoss</code>","text":"<p>               Bases: <code>NTXentLoss</code></p> <p>NTXentNegativeMinedLoss: NTXentLoss with explicitly mined negatives</p> Source code in <code>fmcib/ssl/losses/ntxent_loss.py</code> <pre><code>class NTXentLoss(lightly_NTXentLoss):\n    \"\"\"\n    NTXentNegativeMinedLoss:\n    NTXentLoss with explicitly mined negatives\n    \"\"\"\n\n    def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n        \"\"\"\n        Initialize an instance of the class.\n\n        Args:\n            temperature (float, optional): The temperature parameter for the instance. Defaults to 0.1.\n            gather_distributed (bool, optional): Whether to gather distributed data. Defaults to False.\n        \"\"\"\n        super().__init__(temperature, gather_distributed)\n\n    def forward(self, out: List):\n        \"\"\"\n        Forward pass through Negative mining contrastive Cross-Entropy Loss.\n\n        Args:\n            out (List[torch.Tensor]): List of tensors\n\n        Returns:\n            float: Contrastive Cross Entropy Loss value.\n        \"\"\"\n        return super().forward(*out)\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_loss/#fmcib.ssl.losses.ntxent_loss.NTXentLoss.__init__","title":"<code>__init__(temperature=0.1, gather_distributed=False)</code>","text":"<p>Initialize an instance of the class.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature parameter for the instance. Defaults to 0.1.</p> <code>0.1</code> <code>gather_distributed</code> <code>bool</code> <p>Whether to gather distributed data. Defaults to False.</p> <code>False</code> Source code in <code>fmcib/ssl/losses/ntxent_loss.py</code> <pre><code>def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n    \"\"\"\n    Initialize an instance of the class.\n\n    Args:\n        temperature (float, optional): The temperature parameter for the instance. Defaults to 0.1.\n        gather_distributed (bool, optional): Whether to gather distributed data. Defaults to False.\n    \"\"\"\n    super().__init__(temperature, gather_distributed)\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_loss/#fmcib.ssl.losses.ntxent_loss.NTXentLoss.forward","title":"<code>forward(out)</code>","text":"<p>Forward pass through Negative mining contrastive Cross-Entropy Loss.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>List[Tensor]</code> <p>List of tensors</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Contrastive Cross Entropy Loss value.</p> Source code in <code>fmcib/ssl/losses/ntxent_loss.py</code> <pre><code>def forward(self, out: List):\n    \"\"\"\n    Forward pass through Negative mining contrastive Cross-Entropy Loss.\n\n    Args:\n        out (List[torch.Tensor]): List of tensors\n\n    Returns:\n        float: Contrastive Cross Entropy Loss value.\n    \"\"\"\n    return super().forward(*out)\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_mined_loss/","title":"ntxent_mined_loss","text":"<p>Contrastive Loss Functions</p>"},{"location":"reference/ssl/losses/ntxent_mined_loss/#fmcib.ssl.losses.ntxent_mined_loss.NTXentNegativeMinedLoss","title":"<code>NTXentNegativeMinedLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>NTXentNegativeMinedLoss: NTXentLoss with explicitly mined negatives</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature parameter for the loss calculation. Default is 0.1.</p> <code>0.1</code> <code>gather_distributed</code> <code>bool</code> <p>Whether to gather hidden representations from other processes in a distributed setting. Default is False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the absolute value of temperature is less than 1e-8.</p> Source code in <code>fmcib/ssl/losses/ntxent_mined_loss.py</code> <pre><code>class NTXentNegativeMinedLoss(torch.nn.Module):\n    \"\"\"\n    NTXentNegativeMinedLoss:\n    NTXentLoss with explicitly mined negatives\n\n    Args:\n        temperature (float): The temperature parameter for the loss calculation. Default is 0.1.\n        gather_distributed (bool): Whether to gather hidden representations from other processes in a distributed setting. Default is False.\n\n    Raises:\n        ValueError: If the absolute value of temperature is less than 1e-8.\n    \"\"\"\n\n    def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n        \"\"\"\n        Initialize the NTXentNegativeMinedLoss object.\n\n        Args:\n            temperature (float, optional): The temperature parameter for the loss function. Defaults to 0.1.\n            gather_distributed (bool, optional): Whether to use distributed gathering or not. Defaults to False.\n\n        Raises:\n            ValueError: If the absolute value of the temperature is too small.\n\n        Attributes:\n            temperature (float): The temperature parameter for the loss function.\n            gather_distributed (bool): Whether to use distributed gathering or not.\n            cross_entropy (torch.nn.CrossEntropyLoss): The cross entropy loss function.\n            eps (float): A small value to avoid division by zero.\n        \"\"\"\n        super(NTXentNegativeMinedLoss, self).__init__()\n        self.temperature = temperature\n        self.gather_distributed = gather_distributed\n        self.cross_entropy = nn.CrossEntropyLoss(reduction=\"mean\")\n        self.eps = 1e-8\n\n        if abs(self.temperature) &lt; self.eps:\n            raise ValueError(\"Illegal temperature: abs({}) &lt; 1e-8\".format(self.temperature))\n\n    def forward(self, out: Dict):\n        \"\"\"\n        Forward pass through Negative mining contrastive Cross-Entropy Loss.\n\n        Args:\n            out (Dict): Dictionary with `positive` and `negative` keys to represent positive selected and negative selected samples.\n\n        Returns:\n            torch.Tensor: Contrastive Cross Entropy Loss value.\n\n        Raises:\n            AssertionError: If `positive` or `negative` keys are not specified in the input dictionary.\n        \"\"\"\n\n        assert \"positive\" in out, \"`positive` key needs to be specified\"\n        assert \"negative\" in out, \"`negative` key needs to be specified\"\n\n        pos0, pos1 = out[\"positive\"]\n        neg0, neg1 = out[\"negative\"]\n\n        device = pos0.device\n        batch_size, _ = pos0.shape\n\n        # normalize the output to length 1\n        pos0 = nn.functional.normalize(pos0, dim=1)\n        pos1 = nn.functional.normalize(pos1, dim=1)\n        neg0 = nn.functional.normalize(neg0, dim=1)\n        neg1 = nn.functional.normalize(neg1, dim=1)\n\n        if self.gather_distributed and dist.world_size() &gt; 1:\n            # gather hidden representations from other processes\n            pos0_large = torch.cat(dist.gather(pos0), 0)\n            pos1_large = torch.cat(dist.gather(pos1), 0)\n            neg0_large = torch.cat(dist.gather(neg0), 0)\n            neg1_large = torch.cat(dist.gather(neg1), 0)\n            diag_mask = dist.eye_rank(batch_size, device=pos0.device)\n\n        else:\n            # gather hidden representations from other processes\n            pos0_large = pos0\n            pos1_large = pos1\n            neg0_large = neg0\n            neg1_large = neg1\n            diag_mask = torch.eye(batch_size, device=pos0.device, dtype=torch.bool)\n\n        logits_00 = torch.einsum(\"nc,mc-&gt;nm\", pos0, neg0_large) / self.temperature\n        logits_01 = torch.einsum(\"nc,mc-&gt;nm\", pos0, pos1_large) / self.temperature\n        logits_10 = torch.einsum(\"nc,mc-&gt;nm\", pos1, pos0_large) / self.temperature\n        logits_11 = torch.einsum(\"nc,mc-&gt;nm\", pos1, neg1_large) / self.temperature\n\n        logits_01 = logits_01[diag_mask].view(batch_size, -1)\n        logits_10 = logits_10[diag_mask].view(batch_size, -1)\n\n        logits_0100 = torch.cat([logits_01, logits_00], dim=1)\n        logits_1011 = torch.cat([logits_10, logits_11], dim=1)\n        logits = torch.cat([logits_0100, logits_1011], dim=0)\n\n        labels = torch.zeros(logits.shape[0], device=device, dtype=torch.long)\n        loss = self.cross_entropy(logits, labels)\n\n        return loss\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_mined_loss/#fmcib.ssl.losses.ntxent_mined_loss.NTXentNegativeMinedLoss.__init__","title":"<code>__init__(temperature=0.1, gather_distributed=False)</code>","text":"<p>Initialize the NTXentNegativeMinedLoss object.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature parameter for the loss function. Defaults to 0.1.</p> <code>0.1</code> <code>gather_distributed</code> <code>bool</code> <p>Whether to use distributed gathering or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the absolute value of the temperature is too small.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>The temperature parameter for the loss function.</p> <code>gather_distributed</code> <code>bool</code> <p>Whether to use distributed gathering or not.</p> <code>cross_entropy</code> <code>CrossEntropyLoss</code> <p>The cross entropy loss function.</p> <code>eps</code> <code>float</code> <p>A small value to avoid division by zero.</p> Source code in <code>fmcib/ssl/losses/ntxent_mined_loss.py</code> <pre><code>def __init__(self, temperature: float = 0.1, gather_distributed: bool = False):\n    \"\"\"\n    Initialize the NTXentNegativeMinedLoss object.\n\n    Args:\n        temperature (float, optional): The temperature parameter for the loss function. Defaults to 0.1.\n        gather_distributed (bool, optional): Whether to use distributed gathering or not. Defaults to False.\n\n    Raises:\n        ValueError: If the absolute value of the temperature is too small.\n\n    Attributes:\n        temperature (float): The temperature parameter for the loss function.\n        gather_distributed (bool): Whether to use distributed gathering or not.\n        cross_entropy (torch.nn.CrossEntropyLoss): The cross entropy loss function.\n        eps (float): A small value to avoid division by zero.\n    \"\"\"\n    super(NTXentNegativeMinedLoss, self).__init__()\n    self.temperature = temperature\n    self.gather_distributed = gather_distributed\n    self.cross_entropy = nn.CrossEntropyLoss(reduction=\"mean\")\n    self.eps = 1e-8\n\n    if abs(self.temperature) &lt; self.eps:\n        raise ValueError(\"Illegal temperature: abs({}) &lt; 1e-8\".format(self.temperature))\n</code></pre>"},{"location":"reference/ssl/losses/ntxent_mined_loss/#fmcib.ssl.losses.ntxent_mined_loss.NTXentNegativeMinedLoss.forward","title":"<code>forward(out)</code>","text":"<p>Forward pass through Negative mining contrastive Cross-Entropy Loss.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>Dict</code> <p>Dictionary with <code>positive</code> and <code>negative</code> keys to represent positive selected and negative selected samples.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Contrastive Cross Entropy Loss value.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>positive</code> or <code>negative</code> keys are not specified in the input dictionary.</p> Source code in <code>fmcib/ssl/losses/ntxent_mined_loss.py</code> <pre><code>def forward(self, out: Dict):\n    \"\"\"\n    Forward pass through Negative mining contrastive Cross-Entropy Loss.\n\n    Args:\n        out (Dict): Dictionary with `positive` and `negative` keys to represent positive selected and negative selected samples.\n\n    Returns:\n        torch.Tensor: Contrastive Cross Entropy Loss value.\n\n    Raises:\n        AssertionError: If `positive` or `negative` keys are not specified in the input dictionary.\n    \"\"\"\n\n    assert \"positive\" in out, \"`positive` key needs to be specified\"\n    assert \"negative\" in out, \"`negative` key needs to be specified\"\n\n    pos0, pos1 = out[\"positive\"]\n    neg0, neg1 = out[\"negative\"]\n\n    device = pos0.device\n    batch_size, _ = pos0.shape\n\n    # normalize the output to length 1\n    pos0 = nn.functional.normalize(pos0, dim=1)\n    pos1 = nn.functional.normalize(pos1, dim=1)\n    neg0 = nn.functional.normalize(neg0, dim=1)\n    neg1 = nn.functional.normalize(neg1, dim=1)\n\n    if self.gather_distributed and dist.world_size() &gt; 1:\n        # gather hidden representations from other processes\n        pos0_large = torch.cat(dist.gather(pos0), 0)\n        pos1_large = torch.cat(dist.gather(pos1), 0)\n        neg0_large = torch.cat(dist.gather(neg0), 0)\n        neg1_large = torch.cat(dist.gather(neg1), 0)\n        diag_mask = dist.eye_rank(batch_size, device=pos0.device)\n\n    else:\n        # gather hidden representations from other processes\n        pos0_large = pos0\n        pos1_large = pos1\n        neg0_large = neg0\n        neg1_large = neg1\n        diag_mask = torch.eye(batch_size, device=pos0.device, dtype=torch.bool)\n\n    logits_00 = torch.einsum(\"nc,mc-&gt;nm\", pos0, neg0_large) / self.temperature\n    logits_01 = torch.einsum(\"nc,mc-&gt;nm\", pos0, pos1_large) / self.temperature\n    logits_10 = torch.einsum(\"nc,mc-&gt;nm\", pos1, pos0_large) / self.temperature\n    logits_11 = torch.einsum(\"nc,mc-&gt;nm\", pos1, neg1_large) / self.temperature\n\n    logits_01 = logits_01[diag_mask].view(batch_size, -1)\n    logits_10 = logits_10[diag_mask].view(batch_size, -1)\n\n    logits_0100 = torch.cat([logits_01, logits_00], dim=1)\n    logits_1011 = torch.cat([logits_10, logits_11], dim=1)\n    logits = torch.cat([logits_0100, logits_1011], dim=0)\n\n    labels = torch.zeros(logits.shape[0], device=device, dtype=torch.long)\n    loss = self.cross_entropy(logits, labels)\n\n    return loss\n</code></pre>"},{"location":"reference/ssl/losses/swav_loss/","title":"swav_loss","text":""},{"location":"reference/ssl/losses/swav_loss/#fmcib.ssl.losses.swav_loss.SwaVLoss","title":"<code>SwaVLoss</code>","text":"<p>               Bases: <code>SwaVLoss</code></p> <p>A class representing a custom SwaV loss function.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>The temperature parameter for the loss calculation. Default is 0.1.</p> <code>sinkhorn_iterations</code> <code>int</code> <p>The number of iterations for Sinkhorn algorithm. Default is 3.</p> <code>sinkhorn_epsilon</code> <code>float</code> <p>The epsilon parameter for Sinkhorn algorithm. Default is 0.05.</p> <code>sinkhorn_gather_distributed</code> <code>bool</code> <p>Whether to gather distributed results for Sinkhorn algorithm. Default is False.</p> Source code in <code>fmcib/ssl/losses/swav_loss.py</code> <pre><code>class SwaVLoss(lightly.loss.swav_loss.SwaVLoss):\n    \"\"\"\n    A class representing a custom SwaV loss function.\n\n    Attributes:\n        temperature (float): The temperature parameter for the loss calculation. Default is 0.1.\n        sinkhorn_iterations (int): The number of iterations for Sinkhorn algorithm. Default is 3.\n        sinkhorn_epsilon (float): The epsilon parameter for Sinkhorn algorithm. Default is 0.05.\n        sinkhorn_gather_distributed (bool): Whether to gather distributed results for Sinkhorn algorithm. Default is False.\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature: float = 0.1,\n        sinkhorn_iterations: int = 3,\n        sinkhorn_epsilon: float = 0.05,\n        sinkhorn_gather_distributed: bool = False,\n    ):\n        \"\"\"\n        Initialize the object with specified parameters.\n\n        Args:\n            temperature (float, optional): The temperature parameter. Default is 0.1.\n            sinkhorn_iterations (int, optional): The number of Sinkhorn iterations. Default is 3.\n            sinkhorn_epsilon (float, optional): The epsilon parameter for Sinkhorn algorithm. Default is 0.05.\n            sinkhorn_gather_distributed (bool, optional): Whether to use distributed computation for Sinkhorn algorithm. Default is False.\n        \"\"\"\n        super().__init__(temperature, sinkhorn_iterations, sinkhorn_epsilon, sinkhorn_gather_distributed)\n\n    def forward(self, pred):\n        \"\"\"\n        Perform a forward pass of the model.\n\n        Args:\n            pred (tuple): A tuple containing the predicted outputs for high resolution, low resolution, and queue.\n\n        Returns:\n            The output of the forward pass.\n        \"\"\"\n        high_resolution_outputs, low_resolution_outputs, queue_outputs = pred\n        return super().forward(high_resolution_outputs, low_resolution_outputs, queue_outputs)\n</code></pre>"},{"location":"reference/ssl/losses/swav_loss/#fmcib.ssl.losses.swav_loss.SwaVLoss.__init__","title":"<code>__init__(temperature=0.1, sinkhorn_iterations=3, sinkhorn_epsilon=0.05, sinkhorn_gather_distributed=False)</code>","text":"<p>Initialize the object with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The temperature parameter. Default is 0.1.</p> <code>0.1</code> <code>sinkhorn_iterations</code> <code>int</code> <p>The number of Sinkhorn iterations. Default is 3.</p> <code>3</code> <code>sinkhorn_epsilon</code> <code>float</code> <p>The epsilon parameter for Sinkhorn algorithm. Default is 0.05.</p> <code>0.05</code> <code>sinkhorn_gather_distributed</code> <code>bool</code> <p>Whether to use distributed computation for Sinkhorn algorithm. Default is False.</p> <code>False</code> Source code in <code>fmcib/ssl/losses/swav_loss.py</code> <pre><code>def __init__(\n    self,\n    temperature: float = 0.1,\n    sinkhorn_iterations: int = 3,\n    sinkhorn_epsilon: float = 0.05,\n    sinkhorn_gather_distributed: bool = False,\n):\n    \"\"\"\n    Initialize the object with specified parameters.\n\n    Args:\n        temperature (float, optional): The temperature parameter. Default is 0.1.\n        sinkhorn_iterations (int, optional): The number of Sinkhorn iterations. Default is 3.\n        sinkhorn_epsilon (float, optional): The epsilon parameter for Sinkhorn algorithm. Default is 0.05.\n        sinkhorn_gather_distributed (bool, optional): Whether to use distributed computation for Sinkhorn algorithm. Default is False.\n    \"\"\"\n    super().__init__(temperature, sinkhorn_iterations, sinkhorn_epsilon, sinkhorn_gather_distributed)\n</code></pre>"},{"location":"reference/ssl/losses/swav_loss/#fmcib.ssl.losses.swav_loss.SwaVLoss.forward","title":"<code>forward(pred)</code>","text":"<p>Perform a forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>tuple</code> <p>A tuple containing the predicted outputs for high resolution, low resolution, and queue.</p> required <p>Returns:</p> Type Description <p>The output of the forward pass.</p> Source code in <code>fmcib/ssl/losses/swav_loss.py</code> <pre><code>def forward(self, pred):\n    \"\"\"\n    Perform a forward pass of the model.\n\n    Args:\n        pred (tuple): A tuple containing the predicted outputs for high resolution, low resolution, and queue.\n\n    Returns:\n        The output of the forward pass.\n    \"\"\"\n    high_resolution_outputs, low_resolution_outputs, queue_outputs = pred\n    return super().forward(high_resolution_outputs, low_resolution_outputs, queue_outputs)\n</code></pre>"},{"location":"reference/ssl/modules/","title":"modules","text":""},{"location":"reference/ssl/modules/exneg_simclr/","title":"exneg_simclr","text":""},{"location":"reference/ssl/modules/exneg_simclr/#fmcib.ssl.modules.exneg_simclr.ExNegSimCLR","title":"<code>ExNegSimCLR</code>","text":"<p>               Bases: <code>SimCLR</code></p> <p>Extended Negative Sampling SimCLR model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>The backbone model.</p> required <code>num_ftrs</code> <code>int</code> <p>Number of features in the bottleneck layer. Default is 32.</p> <code>32</code> <code>out_dim</code> <code>int</code> <p>Dimension of the output feature embeddings. Default is 128.</p> <code>128</code> Source code in <code>fmcib/ssl/modules/exneg_simclr.py</code> <pre><code>class ExNegSimCLR(lightly_SimCLR):\n    \"\"\"\n    Extended Negative Sampling SimCLR model.\n\n    Args:\n        backbone (nn.Module): The backbone model.\n        num_ftrs (int): Number of features in the bottleneck layer. Default is 32.\n        out_dim (int): Dimension of the output feature embeddings. Default is 128.\n    \"\"\"\n\n    def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128) -&gt; None:\n        \"\"\"\n        Initialize the object.\n\n        Args:\n            backbone (nn.Module): The backbone neural network.\n            num_ftrs (int, optional): The number of input features for the projection head. Default is 32.\n            out_dim (int, optional): The output dimension of the projection head. Default is 128.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super().__init__(backbone, num_ftrs, out_dim)\n        # replace the projection head with a new one\n        self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs // 2, out_dim, batch_norm=False)\n\n    def forward(self, x: Union[Dict, torch.Tensor], return_features: bool = False):\n        \"\"\"\n        Forward pass of the ExNegSimCLR model.\n\n        Args:\n            x (Union[Dict, torch.Tensor]): Input data. If a dictionary, it should contain multiple views of the same image.\n            return_features (bool): Whether to return the intermediate feature embeddings. Default is False.\n\n        Returns:\n            Dict: Output dictionary containing the forward pass results for each input view.\n        \"\"\"\n        assert isinstance(x, dict), \"Input to forward must be a `dict` for ExNegSimCLR\"\n        out = {}\n        for key, value in x.items():\n            if isinstance(value, list):\n                out[key] = super().forward(*value, return_features)\n\n        return out\n</code></pre>"},{"location":"reference/ssl/modules/exneg_simclr/#fmcib.ssl.modules.exneg_simclr.ExNegSimCLR.__init__","title":"<code>__init__(backbone, num_ftrs=32, out_dim=128)</code>","text":"<p>Initialize the object.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>The backbone neural network.</p> required <code>num_ftrs</code> <code>int</code> <p>The number of input features for the projection head. Default is 32.</p> <code>32</code> <code>out_dim</code> <code>int</code> <p>The output dimension of the projection head. Default is 128.</p> <code>128</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>fmcib/ssl/modules/exneg_simclr.py</code> <pre><code>def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128) -&gt; None:\n    \"\"\"\n    Initialize the object.\n\n    Args:\n        backbone (nn.Module): The backbone neural network.\n        num_ftrs (int, optional): The number of input features for the projection head. Default is 32.\n        out_dim (int, optional): The output dimension of the projection head. Default is 128.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super().__init__(backbone, num_ftrs, out_dim)\n    # replace the projection head with a new one\n    self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs // 2, out_dim, batch_norm=False)\n</code></pre>"},{"location":"reference/ssl/modules/exneg_simclr/#fmcib.ssl.modules.exneg_simclr.ExNegSimCLR.forward","title":"<code>forward(x, return_features=False)</code>","text":"<p>Forward pass of the ExNegSimCLR model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Dict, Tensor]</code> <p>Input data. If a dictionary, it should contain multiple views of the same image.</p> required <code>return_features</code> <code>bool</code> <p>Whether to return the intermediate feature embeddings. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Dict</code> <p>Output dictionary containing the forward pass results for each input view.</p> Source code in <code>fmcib/ssl/modules/exneg_simclr.py</code> <pre><code>def forward(self, x: Union[Dict, torch.Tensor], return_features: bool = False):\n    \"\"\"\n    Forward pass of the ExNegSimCLR model.\n\n    Args:\n        x (Union[Dict, torch.Tensor]): Input data. If a dictionary, it should contain multiple views of the same image.\n        return_features (bool): Whether to return the intermediate feature embeddings. Default is False.\n\n    Returns:\n        Dict: Output dictionary containing the forward pass results for each input view.\n    \"\"\"\n    assert isinstance(x, dict), \"Input to forward must be a `dict` for ExNegSimCLR\"\n    out = {}\n    for key, value in x.items():\n        if isinstance(value, list):\n            out[key] = super().forward(*value, return_features)\n\n    return out\n</code></pre>"},{"location":"reference/ssl/modules/nnclr/","title":"nnclr","text":""},{"location":"reference/ssl/modules/nnclr/#fmcib.ssl.modules.nnclr.NNCLR","title":"<code>NNCLR</code>","text":"<p>               Bases: <code>Module</code></p> <p>Taken largely from https://github.com/lightly-ai/lightly/blob/master/lightly/models/nnclr.py</p> Source code in <code>fmcib/ssl/modules/nnclr.py</code> <pre><code>class NNCLR(nn.Module):\n    \"\"\"\n    Taken largely from https://github.com/lightly-ai/lightly/blob/master/lightly/models/nnclr.py\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        num_ftrs: int = 4096,\n        proj_hidden_dim: int = 4096,\n        pred_hidden_dim: int = 4096,\n        out_dim: int = 256,\n        memory_bank_size: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the NNCLR model.\n\n        Args:\n            backbone (nn.Module): The backbone neural network model.\n            num_ftrs (int, optional): The number of features in the backbone output. Default is 4096.\n            proj_hidden_dim (int, optional): The hidden dimension of the projection head. Default is 4096.\n            pred_hidden_dim (int, optional): The hidden dimension of the prediction head. Default is 4096.\n            out_dim (int, optional): The output dimension of the model. Default is 256.\n            memory_bank_size (int, optional): The size of the memory bank module. Default is 4096.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super().__init__()\n        self.backbone = backbone\n        self.projection_head = NNCLRProjectionHead(num_ftrs, proj_hidden_dim, out_dim)\n        self.prediction_head = NNCLRPredictionHead(out_dim, pred_hidden_dim, out_dim)\n        self.memory_bank = NNMemoryBankModule(memory_bank_size)\n\n    def forward(\n        self,\n        x: List[torch.Tensor],\n        get_nearest_neighbor: bool = True,\n    ):\n        # forward pass of first input x0\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (List[torch.Tensor]): A list containing two input tensors.\n            get_nearest_neighbor (bool, optional): Whether to compute and update the nearest neighbor vectors.\n                Defaults to True.\n\n        Returns:\n            Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n                A tuple containing two tuples. The inner tuples contain the projection and prediction vectors\n                for each input tensor.\n        \"\"\"\n        x0, x1 = x\n        f0 = self.backbone(x0).flatten(start_dim=1)\n        z0 = self.projection_head(f0)\n        p0 = self.prediction_head(z0)\n\n        if get_nearest_neighbor:\n            z0 = self.memory_bank(z0, update=False)\n\n        # forward pass of second input x1\n        f1 = self.backbone(x1).flatten(start_dim=1)\n        z1 = self.projection_head(f1)\n        p1 = self.prediction_head(z1)\n\n        if get_nearest_neighbor:\n            z1 = self.memory_bank(z1, update=True)\n\n        return (z0, p0), (z1, p1)\n</code></pre>"},{"location":"reference/ssl/modules/nnclr/#fmcib.ssl.modules.nnclr.NNCLR.__init__","title":"<code>__init__(backbone, num_ftrs=4096, proj_hidden_dim=4096, pred_hidden_dim=4096, out_dim=256, memory_bank_size=4096)</code>","text":"<p>Initialize the NNCLR model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>The backbone neural network model.</p> required <code>num_ftrs</code> <code>int</code> <p>The number of features in the backbone output. Default is 4096.</p> <code>4096</code> <code>proj_hidden_dim</code> <code>int</code> <p>The hidden dimension of the projection head. Default is 4096.</p> <code>4096</code> <code>pred_hidden_dim</code> <code>int</code> <p>The hidden dimension of the prediction head. Default is 4096.</p> <code>4096</code> <code>out_dim</code> <code>int</code> <p>The output dimension of the model. Default is 256.</p> <code>256</code> <code>memory_bank_size</code> <code>int</code> <p>The size of the memory bank module. Default is 4096.</p> <code>4096</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>fmcib/ssl/modules/nnclr.py</code> <pre><code>def __init__(\n    self,\n    backbone: nn.Module,\n    num_ftrs: int = 4096,\n    proj_hidden_dim: int = 4096,\n    pred_hidden_dim: int = 4096,\n    out_dim: int = 256,\n    memory_bank_size: int = 4096,\n) -&gt; None:\n    \"\"\"\n    Initialize the NNCLR model.\n\n    Args:\n        backbone (nn.Module): The backbone neural network model.\n        num_ftrs (int, optional): The number of features in the backbone output. Default is 4096.\n        proj_hidden_dim (int, optional): The hidden dimension of the projection head. Default is 4096.\n        pred_hidden_dim (int, optional): The hidden dimension of the prediction head. Default is 4096.\n        out_dim (int, optional): The output dimension of the model. Default is 256.\n        memory_bank_size (int, optional): The size of the memory bank module. Default is 4096.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super().__init__()\n    self.backbone = backbone\n    self.projection_head = NNCLRProjectionHead(num_ftrs, proj_hidden_dim, out_dim)\n    self.prediction_head = NNCLRPredictionHead(out_dim, pred_hidden_dim, out_dim)\n    self.memory_bank = NNMemoryBankModule(memory_bank_size)\n</code></pre>"},{"location":"reference/ssl/modules/nnclr/#fmcib.ssl.modules.nnclr.NNCLR.forward","title":"<code>forward(x, get_nearest_neighbor=True)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>List[Tensor]</code> <p>A list containing two input tensors.</p> required <code>get_nearest_neighbor</code> <code>bool</code> <p>Whether to compute and update the nearest neighbor vectors. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]: A tuple containing two tuples. The inner tuples contain the projection and prediction vectors for each input tensor.</p> Source code in <code>fmcib/ssl/modules/nnclr.py</code> <pre><code>def forward(\n    self,\n    x: List[torch.Tensor],\n    get_nearest_neighbor: bool = True,\n):\n    # forward pass of first input x0\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        x (List[torch.Tensor]): A list containing two input tensors.\n        get_nearest_neighbor (bool, optional): Whether to compute and update the nearest neighbor vectors.\n            Defaults to True.\n\n    Returns:\n        Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n            A tuple containing two tuples. The inner tuples contain the projection and prediction vectors\n            for each input tensor.\n    \"\"\"\n    x0, x1 = x\n    f0 = self.backbone(x0).flatten(start_dim=1)\n    z0 = self.projection_head(f0)\n    p0 = self.prediction_head(z0)\n\n    if get_nearest_neighbor:\n        z0 = self.memory_bank(z0, update=False)\n\n    # forward pass of second input x1\n    f1 = self.backbone(x1).flatten(start_dim=1)\n    z1 = self.projection_head(f1)\n    p1 = self.prediction_head(z1)\n\n    if get_nearest_neighbor:\n        z1 = self.memory_bank(z1, update=True)\n\n    return (z0, p0), (z1, p1)\n</code></pre>"},{"location":"reference/ssl/modules/simclr/","title":"simclr","text":""},{"location":"reference/ssl/modules/simclr/#fmcib.ssl.modules.simclr.SimCLR","title":"<code>SimCLR</code>","text":"<p>               Bases: <code>SimCLR</code></p> <p>A class representing a SimCLR model.</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>Module</code> <p>The backbone model used in the SimCLR model.</p> <code>num_ftrs</code> <code>int</code> <p>The number of output features from the backbone model.</p> <code>out_dim</code> <code>int</code> <p>The dimension of the output representations.</p> <code>projection_head</code> <code>SimCLRProjectionHead</code> <p>The projection head used for projection head training.</p> Source code in <code>fmcib/ssl/modules/simclr.py</code> <pre><code>class SimCLR(lightly_SimCLR):\n    \"\"\"\n    A class representing a SimCLR model.\n\n    Attributes:\n        backbone (nn.Module): The backbone model used in the SimCLR model.\n        num_ftrs (int): The number of output features from the backbone model.\n        out_dim (int): The dimension of the output representations.\n        projection_head (SimCLRProjectionHead): The projection head used for projection head training.\n    \"\"\"\n\n    def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128):\n        \"\"\"\n        Initialize the object with a backbone network, number of features, and output dimension.\n\n        Args:\n            backbone (nn.Module): The backbone network.\n            num_ftrs (int): The number of features. Default is 32.\n            out_dim (int): The output dimension. Default is 128.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        super().__init__(backbone, num_ftrs, out_dim)\n        self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs // 2, out_dim, batch_norm=False)\n\n    def forward(self, x, return_features=False):\n        \"\"\"\n        Perform a forward pass of the neural network.\n\n        Args:\n            x (tuple): A tuple of input data. Each element of the tuple represents a different input.\n            return_features (bool, optional): Whether to return the intermediate features. Default is False.\n\n        Returns:\n            torch.Tensor or tuple: The output of the forward pass. If return_features is False, a single tensor is returned.\n                If return_features is True, a tuple is returned consisting of the output tensor and the intermediate features.\n\n        Raises:\n            None.\n        \"\"\"\n        return super().forward(*x, return_features)\n</code></pre>"},{"location":"reference/ssl/modules/simclr/#fmcib.ssl.modules.simclr.SimCLR.__init__","title":"<code>__init__(backbone, num_ftrs=32, out_dim=128)</code>","text":"<p>Initialize the object with a backbone network, number of features, and output dimension.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>The backbone network.</p> required <code>num_ftrs</code> <code>int</code> <p>The number of features. Default is 32.</p> <code>32</code> <code>out_dim</code> <code>int</code> <p>The output dimension. Default is 128.</p> <code>128</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/ssl/modules/simclr.py</code> <pre><code>def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128):\n    \"\"\"\n    Initialize the object with a backbone network, number of features, and output dimension.\n\n    Args:\n        backbone (nn.Module): The backbone network.\n        num_ftrs (int): The number of features. Default is 32.\n        out_dim (int): The output dimension. Default is 128.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    super().__init__(backbone, num_ftrs, out_dim)\n    self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs // 2, out_dim, batch_norm=False)\n</code></pre>"},{"location":"reference/ssl/modules/simclr/#fmcib.ssl.modules.simclr.SimCLR.forward","title":"<code>forward(x, return_features=False)</code>","text":"<p>Perform a forward pass of the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tuple</code> <p>A tuple of input data. Each element of the tuple represents a different input.</p> required <code>return_features</code> <code>bool</code> <p>Whether to return the intermediate features. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <p>torch.Tensor or tuple: The output of the forward pass. If return_features is False, a single tensor is returned. If return_features is True, a tuple is returned consisting of the output tensor and the intermediate features.</p> Source code in <code>fmcib/ssl/modules/simclr.py</code> <pre><code>def forward(self, x, return_features=False):\n    \"\"\"\n    Perform a forward pass of the neural network.\n\n    Args:\n        x (tuple): A tuple of input data. Each element of the tuple represents a different input.\n        return_features (bool, optional): Whether to return the intermediate features. Default is False.\n\n    Returns:\n        torch.Tensor or tuple: The output of the forward pass. If return_features is False, a single tensor is returned.\n            If return_features is True, a tuple is returned consisting of the output tensor and the intermediate features.\n\n    Raises:\n        None.\n    \"\"\"\n    return super().forward(*x, return_features)\n</code></pre>"},{"location":"reference/ssl/modules/swav/","title":"swav","text":""},{"location":"reference/ssl/modules/swav/#fmcib.ssl.modules.swav.SwaV","title":"<code>SwaV</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements the SwAV (Swapping Assignments between multiple Views of the same image) model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>CNN backbone for feature extraction.</p> required <code>num_ftrs</code> <code>int</code> <p>Number of input features for the projection head.</p> required <code>out_dim</code> <code>int</code> <p>Output dimension for the projection head.</p> required <code>n_prototypes</code> <code>int</code> <p>Number of prototypes to compute.</p> required <code>n_queues</code> <code>int</code> <p>Number of memory banks (queues). Should be equal to the number of high-resolution inputs.</p> required <code>queue_length</code> <code>int</code> <p>Length of the memory bank. Defaults to 0.</p> <code>0</code> <code>start_queue_at_epoch</code> <code>int</code> <p>Number of the epoch at which SwaV starts using the queued features. Defaults to 0.</p> <code>0</code> <code>n_steps_frozen_prototypes</code> <code>int</code> <p>Number of steps during which we keep the prototypes fixed. Defaults to 0.</p> <code>0</code> Source code in <code>fmcib/ssl/modules/swav.py</code> <pre><code>class SwaV(nn.Module):\n    \"\"\"\n    Implements the SwAV (Swapping Assignments between multiple Views of the same image) model.\n\n    Args:\n        backbone (nn.Module): CNN backbone for feature extraction.\n        num_ftrs (int): Number of input features for the projection head.\n        out_dim (int): Output dimension for the projection head.\n        n_prototypes (int): Number of prototypes to compute.\n        n_queues (int): Number of memory banks (queues). Should be equal to the number of high-resolution inputs.\n        queue_length (int, optional): Length of the memory bank. Defaults to 0.\n        start_queue_at_epoch (int, optional): Number of the epoch at which SwaV starts using the queued features. Defaults to 0.\n        n_steps_frozen_prototypes (int, optional): Number of steps during which we keep the prototypes fixed. Defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        num_ftrs: int,\n        out_dim: int,\n        n_prototypes: int,\n        n_queues: int,\n        queue_length: int = 0,\n        start_queue_at_epoch: int = 0,\n        n_steps_frozen_prototypes: int = 0,\n    ):\n        \"\"\"\n        Initialize a SwaV model.\n\n        Args:\n            backbone (nn.Module): The backbone model.\n            num_ftrs (int): The number of input features.\n            out_dim (int): The dimension of the output.\n            n_prototypes (int): The number of prototypes.\n            n_queues (int): The number of queues.\n            queue_length (int, optional): The length of the queue. Default is 0.\n            start_queue_at_epoch (int, optional): The epoch at which to start using the queue. Default is 0.\n            n_steps_frozen_prototypes (int, optional): The number of steps to freeze prototypes. Default is 0.\n\n        Returns:\n            None\n\n        Attributes:\n            backbone (nn.Module): The backbone model.\n            projection_head (SwaVProjectionHead): The projection head.\n            prototypes (SwaVPrototypes): The prototypes.\n            queues (nn.ModuleList, optional): The queues. If n_queues &gt; 0, this will be initialized with MemoryBankModules.\n            queue_length (int, optional): The length of the queue.\n            num_features_queued (int): The number of features queued.\n            start_queue_at_epoch (int): The epoch at which to start using the queue.\n        \"\"\"\n        super().__init__()\n        # Backbone for feature extraction\n        self.backbone = backbone\n        # Projection head to project features to a lower-dimensional space\n        self.projection_head = SwaVProjectionHead(num_ftrs, num_ftrs // 2, out_dim)\n        # SwAV Prototypes module for prototype computation\n        self.prototypes = SwaVPrototypes(out_dim, n_prototypes, n_steps_frozen_prototypes)\n\n        self.queues = None\n        if n_queues &gt; 0:\n            # Initialize the memory banks (queues)\n            self.queues = nn.ModuleList([MemoryBankModule(size=queue_length) for _ in range(n_queues)])\n            self.queue_length = queue_length\n            self.num_features_queued = 0\n            self.start_queue_at_epoch = start_queue_at_epoch\n\n    def forward(self, input, epoch=None, step=None):\n        \"\"\"\n        Performs the forward pass for the SwAV model.\n\n        Args:\n            input (Tuple[List[Tensor], List[Tensor]]): A tuple consisting of a list of high-resolution input images\n                and a list of low-resolution input images.\n            epoch (int, optional): Current training epoch. Required if `start_queue_at_epoch` &gt; 0. Defaults to None.\n            step (int, optional): Current training step. Required if `n_steps_frozen_prototypes` &gt; 0. Defaults to None.\n\n        Returns:\n            Tuple[List[Tensor], List[Tensor], List[Tensor]]: A tuple containing lists of high-resolution prototypes,\n                low-resolution prototypes, and queue prototypes.\n        \"\"\"\n        high_resolution, low_resolution = input\n\n        # Normalize prototypes\n        self.prototypes.normalize()\n\n        # Compute high and low resolution features\n        high_resolution_features = [self._subforward(x) for x in high_resolution]\n        low_resolution_features = [self._subforward(x) for x in low_resolution]\n\n        # Compute prototypes for high and low resolution features\n        high_resolution_prototypes = [self.prototypes(x, epoch) for x in high_resolution_features]\n        low_resolution_prototypes = [self.prototypes(x, epoch) for x in low_resolution_features]\n        # Compute prototypes for queued features\n        queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n\n        return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n\n    def _subforward(self, input):\n        \"\"\"\n        Subforward pass to compute features for the input image.\n\n        Args:\n            input (Tensor): Input image tensor.\n\n        Returns:\n            Tensor: L2-normalized feature tensor.\n        \"\"\"\n        # Extract features using the backbone\n        features = self.backbone(input).flatten(start_dim=1)\n        # Project features using the projection head\n        features = self.projection_head(features)\n        # L2-normalize features\n        features = nn.functional.normalize(features, dim=1, p=2)\n        return features\n\n    @torch.no_grad()\n    def _get_queue_prototypes(self, high_resolution_features, epoch=None):\n        \"\"\"\n        Compute the queue prototypes for the given high-resolution features.\n\n        Args:\n            high_resolution_features (List[Tensor]): List of high-resolution feature tensors.\n            epoch (int, optional): Current epoch number. Required if `start_queue_at_epoch` &gt; 0. Defaults to None.\n\n        Returns:\n            List[Tensor] or None: List of queue prototype tensors if conditions are met, otherwise None.\n        \"\"\"\n        if self.queues is None:\n            return None\n\n        if len(high_resolution_features) != len(self.queues):\n            raise ValueError(\n                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n            )\n\n        # Get the queue features\n        queue_features = []\n        for i in range(len(self.queues)):\n            _, features = self.queues[i](high_resolution_features[i], update=True)\n            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n            features = torch.permute(features, (1, 0))\n            queue_features.append(features)\n\n        # Do not return queue prototypes if not enough features have been queued\n        self.num_features_queued += high_resolution_features[0].shape[0]\n        if self.num_features_queued &lt; self.queue_length:\n            return None\n\n        # If loss calculation with queue prototypes starts at a later epoch,\n        # just queue the features and return None instead of queue prototypes.\n        if self.start_queue_at_epoch &gt; 0:\n            if epoch is None:\n                raise ValueError(\n                    \"The epoch number must be passed to the `forward()` \" \"method if `start_queue_at_epoch` is greater than 0.\"\n                )\n            if epoch &lt; self.start_queue_at_epoch:\n                return None\n\n        # Assign prototypes\n        queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n        # Do not return queue prototypes if not enough features have been queued\n        return queue_prototypes\n</code></pre>"},{"location":"reference/ssl/modules/swav/#fmcib.ssl.modules.swav.SwaV.__init__","title":"<code>__init__(backbone, num_ftrs, out_dim, n_prototypes, n_queues, queue_length=0, start_queue_at_epoch=0, n_steps_frozen_prototypes=0)</code>","text":"<p>Initialize a SwaV model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>The backbone model.</p> required <code>num_ftrs</code> <code>int</code> <p>The number of input features.</p> required <code>out_dim</code> <code>int</code> <p>The dimension of the output.</p> required <code>n_prototypes</code> <code>int</code> <p>The number of prototypes.</p> required <code>n_queues</code> <code>int</code> <p>The number of queues.</p> required <code>queue_length</code> <code>int</code> <p>The length of the queue. Default is 0.</p> <code>0</code> <code>start_queue_at_epoch</code> <code>int</code> <p>The epoch at which to start using the queue. Default is 0.</p> <code>0</code> <code>n_steps_frozen_prototypes</code> <code>int</code> <p>The number of steps to freeze prototypes. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <p>None</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>Module</code> <p>The backbone model.</p> <code>projection_head</code> <code>SwaVProjectionHead</code> <p>The projection head.</p> <code>prototypes</code> <code>SwaVPrototypes</code> <p>The prototypes.</p> <code>queues</code> <code>ModuleList</code> <p>The queues. If n_queues &gt; 0, this will be initialized with MemoryBankModules.</p> <code>queue_length</code> <code>int</code> <p>The length of the queue.</p> <code>num_features_queued</code> <code>int</code> <p>The number of features queued.</p> <code>start_queue_at_epoch</code> <code>int</code> <p>The epoch at which to start using the queue.</p> Source code in <code>fmcib/ssl/modules/swav.py</code> <pre><code>def __init__(\n    self,\n    backbone: nn.Module,\n    num_ftrs: int,\n    out_dim: int,\n    n_prototypes: int,\n    n_queues: int,\n    queue_length: int = 0,\n    start_queue_at_epoch: int = 0,\n    n_steps_frozen_prototypes: int = 0,\n):\n    \"\"\"\n    Initialize a SwaV model.\n\n    Args:\n        backbone (nn.Module): The backbone model.\n        num_ftrs (int): The number of input features.\n        out_dim (int): The dimension of the output.\n        n_prototypes (int): The number of prototypes.\n        n_queues (int): The number of queues.\n        queue_length (int, optional): The length of the queue. Default is 0.\n        start_queue_at_epoch (int, optional): The epoch at which to start using the queue. Default is 0.\n        n_steps_frozen_prototypes (int, optional): The number of steps to freeze prototypes. Default is 0.\n\n    Returns:\n        None\n\n    Attributes:\n        backbone (nn.Module): The backbone model.\n        projection_head (SwaVProjectionHead): The projection head.\n        prototypes (SwaVPrototypes): The prototypes.\n        queues (nn.ModuleList, optional): The queues. If n_queues &gt; 0, this will be initialized with MemoryBankModules.\n        queue_length (int, optional): The length of the queue.\n        num_features_queued (int): The number of features queued.\n        start_queue_at_epoch (int): The epoch at which to start using the queue.\n    \"\"\"\n    super().__init__()\n    # Backbone for feature extraction\n    self.backbone = backbone\n    # Projection head to project features to a lower-dimensional space\n    self.projection_head = SwaVProjectionHead(num_ftrs, num_ftrs // 2, out_dim)\n    # SwAV Prototypes module for prototype computation\n    self.prototypes = SwaVPrototypes(out_dim, n_prototypes, n_steps_frozen_prototypes)\n\n    self.queues = None\n    if n_queues &gt; 0:\n        # Initialize the memory banks (queues)\n        self.queues = nn.ModuleList([MemoryBankModule(size=queue_length) for _ in range(n_queues)])\n        self.queue_length = queue_length\n        self.num_features_queued = 0\n        self.start_queue_at_epoch = start_queue_at_epoch\n</code></pre>"},{"location":"reference/ssl/modules/swav/#fmcib.ssl.modules.swav.SwaV._get_queue_prototypes","title":"<code>_get_queue_prototypes(high_resolution_features, epoch=None)</code>","text":"<p>Compute the queue prototypes for the given high-resolution features.</p> <p>Parameters:</p> Name Type Description Default <code>high_resolution_features</code> <code>List[Tensor]</code> <p>List of high-resolution feature tensors.</p> required <code>epoch</code> <code>int</code> <p>Current epoch number. Required if <code>start_queue_at_epoch</code> &gt; 0. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Tensor] or None: List of queue prototype tensors if conditions are met, otherwise None.</p> Source code in <code>fmcib/ssl/modules/swav.py</code> <pre><code>@torch.no_grad()\ndef _get_queue_prototypes(self, high_resolution_features, epoch=None):\n    \"\"\"\n    Compute the queue prototypes for the given high-resolution features.\n\n    Args:\n        high_resolution_features (List[Tensor]): List of high-resolution feature tensors.\n        epoch (int, optional): Current epoch number. Required if `start_queue_at_epoch` &gt; 0. Defaults to None.\n\n    Returns:\n        List[Tensor] or None: List of queue prototype tensors if conditions are met, otherwise None.\n    \"\"\"\n    if self.queues is None:\n        return None\n\n    if len(high_resolution_features) != len(self.queues):\n        raise ValueError(\n            f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n            f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n        )\n\n    # Get the queue features\n    queue_features = []\n    for i in range(len(self.queues)):\n        _, features = self.queues[i](high_resolution_features[i], update=True)\n        # Queue features are in (num_ftrs X queue_length) shape, while the high res\n        # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n        features = torch.permute(features, (1, 0))\n        queue_features.append(features)\n\n    # Do not return queue prototypes if not enough features have been queued\n    self.num_features_queued += high_resolution_features[0].shape[0]\n    if self.num_features_queued &lt; self.queue_length:\n        return None\n\n    # If loss calculation with queue prototypes starts at a later epoch,\n    # just queue the features and return None instead of queue prototypes.\n    if self.start_queue_at_epoch &gt; 0:\n        if epoch is None:\n            raise ValueError(\n                \"The epoch number must be passed to the `forward()` \" \"method if `start_queue_at_epoch` is greater than 0.\"\n            )\n        if epoch &lt; self.start_queue_at_epoch:\n            return None\n\n    # Assign prototypes\n    queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n    # Do not return queue prototypes if not enough features have been queued\n    return queue_prototypes\n</code></pre>"},{"location":"reference/ssl/modules/swav/#fmcib.ssl.modules.swav.SwaV._subforward","title":"<code>_subforward(input)</code>","text":"<p>Subforward pass to compute features for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input image tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>L2-normalized feature tensor.</p> Source code in <code>fmcib/ssl/modules/swav.py</code> <pre><code>def _subforward(self, input):\n    \"\"\"\n    Subforward pass to compute features for the input image.\n\n    Args:\n        input (Tensor): Input image tensor.\n\n    Returns:\n        Tensor: L2-normalized feature tensor.\n    \"\"\"\n    # Extract features using the backbone\n    features = self.backbone(input).flatten(start_dim=1)\n    # Project features using the projection head\n    features = self.projection_head(features)\n    # L2-normalize features\n    features = nn.functional.normalize(features, dim=1, p=2)\n    return features\n</code></pre>"},{"location":"reference/ssl/modules/swav/#fmcib.ssl.modules.swav.SwaV.forward","title":"<code>forward(input, epoch=None, step=None)</code>","text":"<p>Performs the forward pass for the SwAV model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tuple[List[Tensor], List[Tensor]]</code> <p>A tuple consisting of a list of high-resolution input images and a list of low-resolution input images.</p> required <code>epoch</code> <code>int</code> <p>Current training epoch. Required if <code>start_queue_at_epoch</code> &gt; 0. Defaults to None.</p> <code>None</code> <code>step</code> <code>int</code> <p>Current training step. Required if <code>n_steps_frozen_prototypes</code> &gt; 0. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple[List[Tensor], List[Tensor], List[Tensor]]: A tuple containing lists of high-resolution prototypes, low-resolution prototypes, and queue prototypes.</p> Source code in <code>fmcib/ssl/modules/swav.py</code> <pre><code>def forward(self, input, epoch=None, step=None):\n    \"\"\"\n    Performs the forward pass for the SwAV model.\n\n    Args:\n        input (Tuple[List[Tensor], List[Tensor]]): A tuple consisting of a list of high-resolution input images\n            and a list of low-resolution input images.\n        epoch (int, optional): Current training epoch. Required if `start_queue_at_epoch` &gt; 0. Defaults to None.\n        step (int, optional): Current training step. Required if `n_steps_frozen_prototypes` &gt; 0. Defaults to None.\n\n    Returns:\n        Tuple[List[Tensor], List[Tensor], List[Tensor]]: A tuple containing lists of high-resolution prototypes,\n            low-resolution prototypes, and queue prototypes.\n    \"\"\"\n    high_resolution, low_resolution = input\n\n    # Normalize prototypes\n    self.prototypes.normalize()\n\n    # Compute high and low resolution features\n    high_resolution_features = [self._subforward(x) for x in high_resolution]\n    low_resolution_features = [self._subforward(x) for x in low_resolution]\n\n    # Compute prototypes for high and low resolution features\n    high_resolution_prototypes = [self.prototypes(x, epoch) for x in high_resolution_features]\n    low_resolution_prototypes = [self.prototypes(x, epoch) for x in low_resolution_features]\n    # Compute prototypes for queued features\n    queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n\n    return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n</code></pre>"},{"location":"reference/transforms/","title":"transforms","text":""},{"location":"reference/transforms/duplicate/","title":"duplicate","text":""},{"location":"reference/transforms/duplicate/#fmcib.transforms.duplicate.Duplicate","title":"<code>Duplicate</code>","text":"<p>Duplicate an input and apply two different transforms. Used for SimCLR primarily.</p> Source code in <code>fmcib/transforms/duplicate.py</code> <pre><code>class Duplicate:\n    \"\"\"\n    Duplicate an input and apply two different transforms. Used for SimCLR primarily.\n    \"\"\"\n\n    def __init__(self, transforms1: Optional[Callable] = None, transforms2: Optional[Callable] = None):\n        \"\"\"\n        Duplicates an input and applies the given transformations to each copy separately.\n\n        Args:\n            transforms1 (Optional[Callable]): _description_. Default is None.\n            transforms2 (Optional[Callable]): _description_. Default is None.\n        \"\"\"\n        # Wrapped into a list if it isn't one already to allow both a\n        # list of transforms as well as `torchvision.transform.Compose` transforms.\n        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n\n    def __call__(self, input: Any) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            input (torch.Tensor or any other type supported by the given transforms): Input.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: A tuple of two tensors.\n        \"\"\"\n        out1, out2 = input, deepcopy(input)\n        if self.transforms1 is not None:\n            out1 = self.transforms1(out1)\n        if self.transforms2 is not None:\n            out2 = self.transforms2(out2)\n        return (out1, out2)\n</code></pre>"},{"location":"reference/transforms/duplicate/#fmcib.transforms.duplicate.Duplicate.__call__","title":"<code>__call__(input)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input</code> <code>torch.Tensor or any other type supported by the given transforms</code> <p>Input.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: A tuple of two tensors.</p> Source code in <code>fmcib/transforms/duplicate.py</code> <pre><code>def __call__(self, input: Any) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        input (torch.Tensor or any other type supported by the given transforms): Input.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple of two tensors.\n    \"\"\"\n    out1, out2 = input, deepcopy(input)\n    if self.transforms1 is not None:\n        out1 = self.transforms1(out1)\n    if self.transforms2 is not None:\n        out2 = self.transforms2(out2)\n    return (out1, out2)\n</code></pre>"},{"location":"reference/transforms/duplicate/#fmcib.transforms.duplicate.Duplicate.__init__","title":"<code>__init__(transforms1=None, transforms2=None)</code>","text":"<p>Duplicates an input and applies the given transformations to each copy separately.</p> <p>Parameters:</p> Name Type Description Default <code>transforms1</code> <code>Optional[Callable]</code> <p>description. Default is None.</p> <code>None</code> <code>transforms2</code> <code>Optional[Callable]</code> <p>description. Default is None.</p> <code>None</code> Source code in <code>fmcib/transforms/duplicate.py</code> <pre><code>def __init__(self, transforms1: Optional[Callable] = None, transforms2: Optional[Callable] = None):\n    \"\"\"\n    Duplicates an input and applies the given transformations to each copy separately.\n\n    Args:\n        transforms1 (Optional[Callable]): _description_. Default is None.\n        transforms2 (Optional[Callable]): _description_. Default is None.\n    \"\"\"\n    # Wrapped into a list if it isn't one already to allow both a\n    # list of transforms as well as `torchvision.transform.Compose` transforms.\n    self.transforms1 = transforms1\n    self.transforms2 = transforms2\n</code></pre>"},{"location":"reference/transforms/med3d/","title":"med3d","text":""},{"location":"reference/transforms/med3d/#fmcib.transforms.med3d.IntensityNormalizeOneVolume","title":"<code>IntensityNormalizeOneVolume</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A class representing an intensity normalized volume.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Normalize the intensity of an n-dimensional volume based on the mean and standard deviation of the non-zero region.</p> <code>Args</code> <p>volume (numpy.ndarray): The input n-dimensional volume.</p> <code>Returns</code> <p>out (numpy.ndarray): The normalized n-dimensional volume.</p> Source code in <code>fmcib/transforms/med3d.py</code> <pre><code>class IntensityNormalizeOneVolume(Transform):\n    \"\"\"\n    A class representing an intensity normalized volume.\n\n    Attributes:\n        None\n\n    Methods:\n        __call__(self, volume): Normalize the intensity of an n-dimensional volume based on the mean and standard deviation of the non-zero region.\n\n        Args:\n            volume (numpy.ndarray): The input n-dimensional volume.\n\n        Returns:\n            out (numpy.ndarray): The normalized n-dimensional volume.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the object.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n\n    def __call__(self, volume):\n        \"\"\"\n        Normalize the intensity of an nd volume based on the mean and std of the non-zero region.\n\n        Args:\n            volume: The input nd volume.\n\n        Returns:\n            out: The normalized nd volume.\n        \"\"\"\n        volume = volume.astype(np.float32)\n        low, high = np.percentile(volume, [0.5, 99.5])\n        if high &gt; 0:\n            volume = np.clip(volume, low, high)\n\n        pixels = volume[volume &gt; 0]\n        mean = pixels.mean()\n        std = pixels.std()\n        out = (volume - mean) / std\n        out_random = np.random.normal(0, 1, size=volume.shape)\n        out[volume == 0] = out_random[volume == 0]\n        return out\n</code></pre>"},{"location":"reference/transforms/med3d/#fmcib.transforms.med3d.IntensityNormalizeOneVolume.__call__","title":"<code>__call__(volume)</code>","text":"<p>Normalize the intensity of an nd volume based on the mean and std of the non-zero region.</p> <p>Parameters:</p> Name Type Description Default <code>volume</code> <p>The input nd volume.</p> required <p>Returns:</p> Name Type Description <code>out</code> <p>The normalized nd volume.</p> Source code in <code>fmcib/transforms/med3d.py</code> <pre><code>def __call__(self, volume):\n    \"\"\"\n    Normalize the intensity of an nd volume based on the mean and std of the non-zero region.\n\n    Args:\n        volume: The input nd volume.\n\n    Returns:\n        out: The normalized nd volume.\n    \"\"\"\n    volume = volume.astype(np.float32)\n    low, high = np.percentile(volume, [0.5, 99.5])\n    if high &gt; 0:\n        volume = np.clip(volume, low, high)\n\n    pixels = volume[volume &gt; 0]\n    mean = pixels.mean()\n    std = pixels.std()\n    out = (volume - mean) / std\n    out_random = np.random.normal(0, 1, size=volume.shape)\n    out[volume == 0] = out_random[volume == 0]\n    return out\n</code></pre>"},{"location":"reference/transforms/med3d/#fmcib.transforms.med3d.IntensityNormalizeOneVolume.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/transforms/med3d.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the object.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/transforms/multicrop/","title":"multicrop","text":""},{"location":"reference/transforms/multicrop/#fmcib.transforms.multicrop.MultiCrop","title":"<code>MultiCrop</code>","text":"<p>Multi-Crop augmentation.</p> Source code in <code>fmcib/transforms/multicrop.py</code> <pre><code>class MultiCrop:\n    \"\"\"\n    Multi-Crop augmentation.\n    \"\"\"\n\n    def __init__(self, high_resolution_transforms: List[Callable], low_resolution_transforms: Optional[List[Callable]]):\n        \"\"\"\n        Initialize an instance of a class with transformations for high-resolution and low-resolution images.\n\n        Args:\n            high_resolution_transforms (list): A list of Callable objects representing the transformations to be applied to high-resolution images.\n            low_resolution_transforms (list, optional): A list of Callable objects representing the transformations to be applied to low-resolution images. Default is None.\n        \"\"\"\n        self.high_resolution_transforms = ensure_list(high_resolution_transforms)\n        self.low_resolution_transforms = ensure_list(low_resolution_transforms)\n\n    def __call__(self, input):\n        \"\"\"\n        This function applies a set of transformations to an input image and returns high and low-resolution crops.\n\n        Args:\n            input (image): The input image to be transformed.\n\n        Returns:\n            tuple: A tuple containing two lists:\n                - high_resolution_crops (list): A list of high-resolution cropped images.\n                - low_resolution_crops (list): A list of low-resolution cropped images.\n        \"\"\"\n        high_resolution_crops = [transform(input) for transform in self.high_resolution_transforms]\n        low_resolution_crops = [transform(input) for transform in self.low_resolution_transforms]\n        return high_resolution_crops, low_resolution_crops\n</code></pre>"},{"location":"reference/transforms/multicrop/#fmcib.transforms.multicrop.MultiCrop.__call__","title":"<code>__call__(input)</code>","text":"<p>This function applies a set of transformations to an input image and returns high and low-resolution crops.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>image</code> <p>The input image to be transformed.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing two lists: - high_resolution_crops (list): A list of high-resolution cropped images. - low_resolution_crops (list): A list of low-resolution cropped images.</p> Source code in <code>fmcib/transforms/multicrop.py</code> <pre><code>def __call__(self, input):\n    \"\"\"\n    This function applies a set of transformations to an input image and returns high and low-resolution crops.\n\n    Args:\n        input (image): The input image to be transformed.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - high_resolution_crops (list): A list of high-resolution cropped images.\n            - low_resolution_crops (list): A list of low-resolution cropped images.\n    \"\"\"\n    high_resolution_crops = [transform(input) for transform in self.high_resolution_transforms]\n    low_resolution_crops = [transform(input) for transform in self.low_resolution_transforms]\n    return high_resolution_crops, low_resolution_crops\n</code></pre>"},{"location":"reference/transforms/multicrop/#fmcib.transforms.multicrop.MultiCrop.__init__","title":"<code>__init__(high_resolution_transforms, low_resolution_transforms)</code>","text":"<p>Initialize an instance of a class with transformations for high-resolution and low-resolution images.</p> <p>Parameters:</p> Name Type Description Default <code>high_resolution_transforms</code> <code>list</code> <p>A list of Callable objects representing the transformations to be applied to high-resolution images.</p> required <code>low_resolution_transforms</code> <code>list</code> <p>A list of Callable objects representing the transformations to be applied to low-resolution images. Default is None.</p> required Source code in <code>fmcib/transforms/multicrop.py</code> <pre><code>def __init__(self, high_resolution_transforms: List[Callable], low_resolution_transforms: Optional[List[Callable]]):\n    \"\"\"\n    Initialize an instance of a class with transformations for high-resolution and low-resolution images.\n\n    Args:\n        high_resolution_transforms (list): A list of Callable objects representing the transformations to be applied to high-resolution images.\n        low_resolution_transforms (list, optional): A list of Callable objects representing the transformations to be applied to low-resolution images. Default is None.\n    \"\"\"\n    self.high_resolution_transforms = ensure_list(high_resolution_transforms)\n    self.low_resolution_transforms = ensure_list(low_resolution_transforms)\n</code></pre>"},{"location":"reference/transforms/random_resized_crop/","title":"random_resized_crop","text":""},{"location":"reference/transforms/random_resized_crop/#fmcib.transforms.random_resized_crop.RandomResizedCrop3D","title":"<code>RandomResizedCrop3D</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Combines monai's random spatial crop followed by resize to the desired size.</p> <p>Modifications: 1. The spatial crop is done with the same dimensions for all the axes. 2. Handles cases where the image_size is less than the crop_size by choosing the smallest dimension as the random scale.</p> Source code in <code>fmcib/transforms/random_resized_crop.py</code> <pre><code>class RandomResizedCrop3D(Transform):\n    \"\"\"\n    Combines monai's random spatial crop followed by resize to the desired size.\n\n    Modifications:\n    1. The spatial crop is done with the same dimensions for all the axes.\n    2. Handles cases where the image_size is less than the crop_size by choosing the smallest dimension as the random scale.\n    \"\"\"\n\n    def __init__(self, prob: float = 1, size: int = 50, scale: List[float] = [0.5, 1.0]):\n        \"\"\"\n        Args:\n            scale (List[int]): Specifies the lower and upper bounds for the random area of the crop,\n             before resizing. The scale is defined with respect to the area of the original image.\n        \"\"\"\n        super().__init__()\n        self.prob = prob\n        self.scale = scale\n        self.size = [size] * 3\n\n    def __call__(self, image):\n        \"\"\"\n        Call method to apply random scale cropping and resizing to an image.\n\n        Args:\n            image (torch.Tensor): The input image.\n\n        Returns:\n            torch.Tensor: The transformed image.\n        \"\"\"\n        if torch.rand(1) &lt; self.prob:\n            random_scale = torch.empty(1).uniform_(*self.scale).item()\n            rand_cropper = RandScaleCrop(random_scale, random_size=False)\n            resizer = Resize(self.size, mode=\"trilinear\")\n\n            for transform in [rand_cropper, resizer]:\n                image = transform(image)\n\n        return image\n</code></pre>"},{"location":"reference/transforms/random_resized_crop/#fmcib.transforms.random_resized_crop.RandomResizedCrop3D.__call__","title":"<code>__call__(image)</code>","text":"<p>Call method to apply random scale cropping and resizing to an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>The input image.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The transformed image.</p> Source code in <code>fmcib/transforms/random_resized_crop.py</code> <pre><code>def __call__(self, image):\n    \"\"\"\n    Call method to apply random scale cropping and resizing to an image.\n\n    Args:\n        image (torch.Tensor): The input image.\n\n    Returns:\n        torch.Tensor: The transformed image.\n    \"\"\"\n    if torch.rand(1) &lt; self.prob:\n        random_scale = torch.empty(1).uniform_(*self.scale).item()\n        rand_cropper = RandScaleCrop(random_scale, random_size=False)\n        resizer = Resize(self.size, mode=\"trilinear\")\n\n        for transform in [rand_cropper, resizer]:\n            image = transform(image)\n\n    return image\n</code></pre>"},{"location":"reference/transforms/random_resized_crop/#fmcib.transforms.random_resized_crop.RandomResizedCrop3D.__init__","title":"<code>__init__(prob=1, size=50, scale=[0.5, 1.0])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>scale</code> <code>List[int]</code> <p>Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image.</p> <code>[0.5, 1.0]</code> Source code in <code>fmcib/transforms/random_resized_crop.py</code> <pre><code>def __init__(self, prob: float = 1, size: int = 50, scale: List[float] = [0.5, 1.0]):\n    \"\"\"\n    Args:\n        scale (List[int]): Specifies the lower and upper bounds for the random area of the crop,\n         before resizing. The scale is defined with respect to the area of the original image.\n    \"\"\"\n    super().__init__()\n    self.prob = prob\n    self.scale = scale\n    self.size = [size] * 3\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#fmcib.utils.SuppressPrint","title":"<code>SuppressPrint</code>","text":"<p>A class that temporarily suppresses print statements.</p> <p>Methods:</p> Name Description <code>__enter__</code> <p>Sets sys.stdout to a dummy file object, suppressing print output.</p> <code>__exit__</code> <p>Restores sys.stdout to its original value.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>class SuppressPrint:\n    \"\"\"\n    A class that temporarily suppresses print statements.\n\n    Methods:\n        __enter__(): Sets sys.stdout to a dummy file object, suppressing print output.\n        __exit__(exc_type, exc_val, exc_tb): Restores sys.stdout to its original value.\n    \"\"\"\n\n    def __enter__(self):\n        \"\"\"\n        Enter the context manager and redirect the standard output to nothing.\n\n        Returns:\n            object: The context manager object.\n\n        Notes:\n            This context manager is used to redirect the standard output to nothing using the `open` function.\n            It saves the original standard output and assigns a new output destination as `/dev/null` on Unix-like systems.\n        \"\"\"\n        self._original_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Restores the original stdout and closes the modified stdout.\n\n        Args:\n            exc_type (type): The exception type, if an exception occurred. Otherwise, None.\n            exc_val (Exception): The exception instance, if an exception occurred. Otherwise, None.\n            exc_tb (traceback): The traceback object, if an exception occurred. Otherwise, None.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.SuppressPrint.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context manager and redirect the standard output to nothing.</p> <p>Returns:</p> Name Type Description <code>object</code> <p>The context manager object.</p> Notes <p>This context manager is used to redirect the standard output to nothing using the <code>open</code> function. It saves the original standard output and assigns a new output destination as <code>/dev/null</code> on Unix-like systems.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    Enter the context manager and redirect the standard output to nothing.\n\n    Returns:\n        object: The context manager object.\n\n    Notes:\n        This context manager is used to redirect the standard output to nothing using the `open` function.\n        It saves the original standard output and assigns a new output destination as `/dev/null` on Unix-like systems.\n    \"\"\"\n    self._original_stdout = sys.stdout\n    sys.stdout = open(os.devnull, \"w\")\n    return self\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.SuppressPrint.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Restores the original stdout and closes the modified stdout.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>type</code> <p>The exception type, if an exception occurred. Otherwise, None.</p> required <code>exc_val</code> <code>Exception</code> <p>The exception instance, if an exception occurred. Otherwise, None.</p> required <code>exc_tb</code> <code>traceback</code> <p>The traceback object, if an exception occurred. Otherwise, None.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Restores the original stdout and closes the modified stdout.\n\n    Args:\n        exc_type (type): The exception type, if an exception occurred. Otherwise, None.\n        exc_val (Exception): The exception instance, if an exception occurred. Otherwise, None.\n        exc_tb (traceback): The traceback object, if an exception occurred. Otherwise, None.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    sys.stdout.close()\n    sys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.build_image_seed_dict","title":"<code>build_image_seed_dict(path, samples=None)</code>","text":"<p>Build a dictionary of image seeds from DICOM files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory containing DICOM files.</p> required <code>samples</code> <code>int</code> <p>The number of samples to process. If None, all samples will be processed.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the image seeds.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def build_image_seed_dict(path, samples=None):\n    \"\"\"\n    Build a dictionary of image seeds from DICOM files.\n\n    Args:\n        path (str): The path to the directory containing DICOM files.\n        samples (int, optional): The number of samples to process. If None, all samples will be processed.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the image seeds.\n    \"\"\"\n    sorted_dir = Path(path).resolve()\n    series_dirs = [x.parent for x in sorted_dir.rglob(\"*.dcm\")]\n    series_dirs = sorted(list(set(series_dirs)))\n\n    logger.info(\"Converting DICOM files to NIFTI ...\")\n\n    if samples is None:\n        samples = len(series_dirs)\n\n    rows = []\n\n    num_workers = os.cpu_count()  # Adjust this value based on the number of available CPU cores\n    with concurrent.futures.ProcessPoolExecutor(num_workers) as executor:\n        processed_rows = list(tqdm(executor.map(process_series_dir, series_dirs[:samples]), total=samples))\n\n    rows = [row for row in processed_rows if row]\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.dcmseg2nii","title":"<code>dcmseg2nii(dcmseg_path, output_dir, tag='')</code>","text":"<p>Convert a DICOM Segmentation object to NIfTI format and save the resulting segment images.</p> <p>Parameters:</p> Name Type Description Default <code>dcmseg_path</code> <code>str</code> <p>The file path of the DICOM Segmentation object.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the NIfTI files will be saved.</p> required <code>tag</code> <code>str</code> <p>An optional tag to prepend to the output file names. Defaults to \"\".</p> <code>''</code> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def dcmseg2nii(dcmseg_path, output_dir, tag=\"\"):\n    \"\"\"\n    Convert a DICOM Segmentation object to NIfTI format and save the resulting segment images.\n\n    Args:\n        dcmseg_path (str): The file path of the DICOM Segmentation object.\n        output_dir (str): The directory where the NIfTI files will be saved.\n        tag (str, optional): An optional tag to prepend to the output file names. Defaults to \"\".\n    \"\"\"\n    dcm = pydicom.dcmread(dcmseg_path)\n    reader = pydicom_seg.SegmentReader()\n    result = reader.read(dcm)\n\n    for segment_number in result.available_segments:\n        image = result.segment_image(segment_number)  # lazy construction\n        sitk.WriteImage(image, output_dir + f\"/{tag}{segment_number}.nii.gz\", True)\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.download_LUNG1","title":"<code>download_LUNG1(path, samples=None)</code>","text":"<p>Downloads the LUNG1 data manifest from Dropbox and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the LUNG1 data manifest will be saved.</p> required <code>samples</code> <code>list</code> <p>A list of specific samples to download. If None, all samples will be downloaded.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_LUNG1(path, samples=None):\n    \"\"\"\n    Downloads the LUNG1 data manifest from Dropbox and saves it to the specified path.\n\n    Parameters:\n        path (str): The directory path where the LUNG1 data manifest will be saved.\n        samples (list, optional): A list of specific samples to download. If None, all samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    save_dir = Path(path).resolve()\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(\"Downloading LUNG1 manifest from Dropbox ...\")\n    # Download LUNG1 data manifest, this is precomputed but any set of GCS dicom files can be used here\n    wget.download(\n        \"https://www.dropbox.com/s/lkvv33nmepecyu5/nsclc_radiomics.csv?dl=1\",\n        out=f\"{save_dir}/nsclc_radiomics.csv\",\n    )\n\n    df = pd.read_csv(f\"{save_dir}/nsclc_radiomics.csv\")\n\n    download_from_manifest(df, save_dir, samples)\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.download_RADIO","title":"<code>download_RADIO(path, samples=None)</code>","text":"<p>Downloads the RADIO manifest from Dropbox and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path where the manifest file will be saved.</p> required <code>samples</code> <code>list</code> <p>A list of sample names to download. If None, all samples will be downloaded.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_RADIO(path, samples=None):\n    \"\"\"\n    Downloads the RADIO manifest from Dropbox and saves it to the specified path.\n\n    Args:\n        path (str): The path where the manifest file will be saved.\n        samples (list, optional): A list of sample names to download. If None, all samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    save_dir = Path(path).resolve()\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(\"Downloading RADIO manifest from Dropbox ...\")\n    # Download RADIO data manifest, this is precomputed but any set of GCS dicom files can be used here\n    wget.download(\n        \"https://www.dropbox.com/s/nhh1tb0rclrb7mw/nsclc_radiogenomics.csv?dl=1\",\n        out=f\"{save_dir}/nsclc_radiogenomics.csv\",\n    )\n\n    df = pd.read_csv(f\"{save_dir}/nsclc_radiogenomics.csv\")\n\n    download_from_manifest(df, save_dir, samples)\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.download_from_manifest","title":"<code>download_from_manifest(df, save_dir, samples)</code>","text":"<p>Downloads DICOM data from IDC (Imaging Data Commons) based on the provided manifest.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The manifest DataFrame containing information about the DICOM files.</p> required <code>save_dir</code> <code>Path</code> <p>The directory where the downloaded DICOM files will be saved.</p> required <code>samples</code> <code>int</code> <p>The number of random samples to download. If None, all available samples will be downloaded.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_from_manifest(df, save_dir, samples):\n    \"\"\"\n    Downloads DICOM data from IDC (Imaging Data Commons) based on the provided manifest.\n\n    Parameters:\n        df (pandas.DataFrame): The manifest DataFrame containing information about the DICOM files.\n        save_dir (pathlib.Path): The directory where the downloaded DICOM files will be saved.\n        samples (int): The number of random samples to download. If None, all available samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    # Instantiates a client\n    storage_client = storage.Client.create_anonymous_client()\n    logger.info(\"Downloading DICOM data from IDC (Imaging Data Commons) ...\")\n    (save_dir / \"dicom\").mkdir(exist_ok=True, parents=True)\n\n    if samples is not None:\n        assert \"PatientID\" in df.columns\n        rows_with_annotations = df[df[\"Modality\"].isin([\"RTSTRUCT\", \"SEG\"])]\n        unique_elements = rows_with_annotations[\"PatientID\"].unique()\n        selected_elements = np.random.choice(unique_elements, min(len(unique_elements), samples), replace=False)\n        df = df[df[\"PatientID\"].isin(selected_elements)]\n\n    def download_file(row):\n        \"\"\"\n        Download a file from Google Cloud Storage.\n\n        Args:\n            row (dict): A dictionary containing the row data.\n\n        Raises:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        bucket_name, directory, file = row[\"gcs_url\"].split(\"/\")[-3:]\n        fn = f\"{directory}/{file}\"\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(fn)\n\n        current_save_dir = save_dir / \"dicom\" / row[\"PatientID\"] / row[\"StudyInstanceUID\"]\n        current_save_dir.mkdir(exist_ok=True, parents=True)\n        blob.download_to_filename(\n            str(current_save_dir / f'{row[\"Modality\"]}_{row[\"SeriesInstanceUID\"]}_{row[\"InstanceNumber\"]}.dcm')\n        )\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = []\n        for idx, row in df.iterrows():\n            futures.append(executor.submit(download_file, row))\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            pass\n</code></pre>"},{"location":"reference/utils/#fmcib.utils.process_series_dir","title":"<code>process_series_dir(series_dir)</code>","text":"<p>Process the series directory and extract relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>series_dir</code> <code>Path</code> <p>The path to the series directory.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the extracted information, including the   image path, patient ID, and centroid coordinates.</p> <code>None</code> <p>If there's no RTSTRUCT or SEG file, or any step fails.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def process_series_dir(series_dir: Path):\n    \"\"\"\n    Process the series directory and extract relevant information.\n\n    Args:\n        series_dir (Path): The path to the series directory.\n\n    Returns:\n        dict: A dictionary containing the extracted information, including the\n              image path, patient ID, and centroid coordinates.\n        None: If there's no RTSTRUCT or SEG file, or any step fails.\n\n    Raises:\n        None\n    \"\"\"\n    # Check if RTSTRUCT file exists\n    rt_struct_files = list(series_dir.glob(\"*RTSTRUCT*\"))\n    seg_files = list(series_dir.glob(\"*SEG*\"))\n\n    # Convert DICOM to NIfTI based on whether it's RTSTRUCT or SEG\n    if len(rt_struct_files) != 0:\n        dcmrtstruct2nii(str(rt_struct_files[0]), str(series_dir), str(series_dir))\n\n    elif len(seg_files) != 0:\n        dcmseg2nii(str(seg_files[0]), str(series_dir), tag=\"GTV-\")\n\n        # Build the main image NIfTI\n        try:\n            series_id = str(list(series_dir.glob(\"CT*.dcm\"))[0]).split(\"_\")[-2]\n        except IndexError:\n            logger.warning(f\"No 'CT*.dcm' file found under {series_dir}. Skipping.\")\n            return None\n\n        dicom_image = DcmInputAdapter().ingest(str(series_dir), series_id=series_id)\n        nii_output_adapter = NiiOutputAdapter()\n        nii_output_adapter.write(dicom_image, f\"{series_dir}/image\", gzip=True)\n\n    else:\n        logger.warning(f\"No RTSTRUCT or SEG file found in {series_dir}. Skipping.\")\n        return None\n\n    # Read the image (generated above)\n    image_path = series_dir / \"image.nii.gz\"\n    if not image_path.exists():\n        logger.warning(f\"No image file found at {image_path}. Skipping.\")\n        return None\n\n    try:\n        image = sitk.ReadImage(str(image_path))\n    except Exception as e:\n        logger.error(f\"Failed to read image {image_path}: {e}\")\n        return None\n\n    # Find the GTV-1 mask files\n    gtv1_masks = list(series_dir.glob(\"*GTV-1*.nii.gz\"))\n    if not gtv1_masks:\n        logger.warning(f\"No GTV-1 mask found in {series_dir}. Skipping.\")\n        return None\n\n    mask_path = gtv1_masks[0]\n    try:\n        mask = sitk.ReadImage(str(mask_path))\n    except Exception as e:\n        logger.error(f\"Failed to read mask {mask_path}: {e}\")\n        return None\n\n    # Extract centroid from the mask\n    label_shape_filter = sitk.LabelShapeStatisticsImageFilter()\n    label_shape_filter.Execute(mask)\n\n    # Some masks label is 1, others are 255; try 255 first, else 1\n    try:\n        centroid = label_shape_filter.GetCentroid(255)\n    except:\n        try:\n            centroid = label_shape_filter.GetCentroid(1)\n        except Exception as e:\n            logger.warning(f\"Could not extract centroid from mask {mask_path}: {e}\")\n            return None\n\n    x, y, z = centroid\n\n    row = {\n        \"image_path\": str(image_path),\n        \"PatientID\": series_dir.parent.name,\n        \"coordX\": x,\n        \"coordY\": y,\n        \"coordZ\": z,\n    }\n\n    return row\n</code></pre>"},{"location":"reference/utils/download_utils/","title":"download_utils","text":""},{"location":"reference/utils/download_utils/#fmcib.utils.download_utils.bar_progress","title":"<code>bar_progress(current, total, width=80)</code>","text":"<p>Display a progress bar for a download.</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>int</code> <p>The current progress value.</p> required <code>total</code> <code>int</code> <p>The total progress value.</p> required <code>width</code> <code>int</code> <p>The width of the progress bar in characters. Defaults to 80.</p> <code>80</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/download_utils.py</code> <pre><code>def bar_progress(current, total, width=80):\n    \"\"\"\n    Display a progress bar for a download.\n\n    Args:\n        current (int): The current progress value.\n        total (int): The total progress value.\n        width (int, optional): The width of the progress bar in characters. Defaults to 80.\n\n    Raises:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n    # Don't use print() as it will print in new line every time.\n    sys.stdout.write(\"\\r\" + progress_message)\n    sys.stdout.flush()\n</code></pre>"},{"location":"reference/utils/idc_helper/","title":"idc_helper","text":""},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.SuppressPrint","title":"<code>SuppressPrint</code>","text":"<p>A class that temporarily suppresses print statements.</p> <p>Methods:</p> Name Description <code>__enter__</code> <p>Sets sys.stdout to a dummy file object, suppressing print output.</p> <code>__exit__</code> <p>Restores sys.stdout to its original value.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>class SuppressPrint:\n    \"\"\"\n    A class that temporarily suppresses print statements.\n\n    Methods:\n        __enter__(): Sets sys.stdout to a dummy file object, suppressing print output.\n        __exit__(exc_type, exc_val, exc_tb): Restores sys.stdout to its original value.\n    \"\"\"\n\n    def __enter__(self):\n        \"\"\"\n        Enter the context manager and redirect the standard output to nothing.\n\n        Returns:\n            object: The context manager object.\n\n        Notes:\n            This context manager is used to redirect the standard output to nothing using the `open` function.\n            It saves the original standard output and assigns a new output destination as `/dev/null` on Unix-like systems.\n        \"\"\"\n        self._original_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Restores the original stdout and closes the modified stdout.\n\n        Args:\n            exc_type (type): The exception type, if an exception occurred. Otherwise, None.\n            exc_val (Exception): The exception instance, if an exception occurred. Otherwise, None.\n            exc_tb (traceback): The traceback object, if an exception occurred. Otherwise, None.\n\n        Returns:\n            None\n\n        Raises:\n            None\n        \"\"\"\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.SuppressPrint.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context manager and redirect the standard output to nothing.</p> <p>Returns:</p> Name Type Description <code>object</code> <p>The context manager object.</p> Notes <p>This context manager is used to redirect the standard output to nothing using the <code>open</code> function. It saves the original standard output and assigns a new output destination as <code>/dev/null</code> on Unix-like systems.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    Enter the context manager and redirect the standard output to nothing.\n\n    Returns:\n        object: The context manager object.\n\n    Notes:\n        This context manager is used to redirect the standard output to nothing using the `open` function.\n        It saves the original standard output and assigns a new output destination as `/dev/null` on Unix-like systems.\n    \"\"\"\n    self._original_stdout = sys.stdout\n    sys.stdout = open(os.devnull, \"w\")\n    return self\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.SuppressPrint.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Restores the original stdout and closes the modified stdout.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>type</code> <p>The exception type, if an exception occurred. Otherwise, None.</p> required <code>exc_val</code> <code>Exception</code> <p>The exception instance, if an exception occurred. Otherwise, None.</p> required <code>exc_tb</code> <code>traceback</code> <p>The traceback object, if an exception occurred. Otherwise, None.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Restores the original stdout and closes the modified stdout.\n\n    Args:\n        exc_type (type): The exception type, if an exception occurred. Otherwise, None.\n        exc_val (Exception): The exception instance, if an exception occurred. Otherwise, None.\n        exc_tb (traceback): The traceback object, if an exception occurred. Otherwise, None.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    sys.stdout.close()\n    sys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.build_image_seed_dict","title":"<code>build_image_seed_dict(path, samples=None)</code>","text":"<p>Build a dictionary of image seeds from DICOM files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory containing DICOM files.</p> required <code>samples</code> <code>int</code> <p>The number of samples to process. If None, all samples will be processed.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the image seeds.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def build_image_seed_dict(path, samples=None):\n    \"\"\"\n    Build a dictionary of image seeds from DICOM files.\n\n    Args:\n        path (str): The path to the directory containing DICOM files.\n        samples (int, optional): The number of samples to process. If None, all samples will be processed.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the image seeds.\n    \"\"\"\n    sorted_dir = Path(path).resolve()\n    series_dirs = [x.parent for x in sorted_dir.rglob(\"*.dcm\")]\n    series_dirs = sorted(list(set(series_dirs)))\n\n    logger.info(\"Converting DICOM files to NIFTI ...\")\n\n    if samples is None:\n        samples = len(series_dirs)\n\n    rows = []\n\n    num_workers = os.cpu_count()  # Adjust this value based on the number of available CPU cores\n    with concurrent.futures.ProcessPoolExecutor(num_workers) as executor:\n        processed_rows = list(tqdm(executor.map(process_series_dir, series_dirs[:samples]), total=samples))\n\n    rows = [row for row in processed_rows if row]\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.dcmseg2nii","title":"<code>dcmseg2nii(dcmseg_path, output_dir, tag='')</code>","text":"<p>Convert a DICOM Segmentation object to NIfTI format and save the resulting segment images.</p> <p>Parameters:</p> Name Type Description Default <code>dcmseg_path</code> <code>str</code> <p>The file path of the DICOM Segmentation object.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the NIfTI files will be saved.</p> required <code>tag</code> <code>str</code> <p>An optional tag to prepend to the output file names. Defaults to \"\".</p> <code>''</code> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def dcmseg2nii(dcmseg_path, output_dir, tag=\"\"):\n    \"\"\"\n    Convert a DICOM Segmentation object to NIfTI format and save the resulting segment images.\n\n    Args:\n        dcmseg_path (str): The file path of the DICOM Segmentation object.\n        output_dir (str): The directory where the NIfTI files will be saved.\n        tag (str, optional): An optional tag to prepend to the output file names. Defaults to \"\".\n    \"\"\"\n    dcm = pydicom.dcmread(dcmseg_path)\n    reader = pydicom_seg.SegmentReader()\n    result = reader.read(dcm)\n\n    for segment_number in result.available_segments:\n        image = result.segment_image(segment_number)  # lazy construction\n        sitk.WriteImage(image, output_dir + f\"/{tag}{segment_number}.nii.gz\", True)\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.download_LUNG1","title":"<code>download_LUNG1(path, samples=None)</code>","text":"<p>Downloads the LUNG1 data manifest from Dropbox and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the LUNG1 data manifest will be saved.</p> required <code>samples</code> <code>list</code> <p>A list of specific samples to download. If None, all samples will be downloaded.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_LUNG1(path, samples=None):\n    \"\"\"\n    Downloads the LUNG1 data manifest from Dropbox and saves it to the specified path.\n\n    Parameters:\n        path (str): The directory path where the LUNG1 data manifest will be saved.\n        samples (list, optional): A list of specific samples to download. If None, all samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    save_dir = Path(path).resolve()\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(\"Downloading LUNG1 manifest from Dropbox ...\")\n    # Download LUNG1 data manifest, this is precomputed but any set of GCS dicom files can be used here\n    wget.download(\n        \"https://www.dropbox.com/s/lkvv33nmepecyu5/nsclc_radiomics.csv?dl=1\",\n        out=f\"{save_dir}/nsclc_radiomics.csv\",\n    )\n\n    df = pd.read_csv(f\"{save_dir}/nsclc_radiomics.csv\")\n\n    download_from_manifest(df, save_dir, samples)\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.download_RADIO","title":"<code>download_RADIO(path, samples=None)</code>","text":"<p>Downloads the RADIO manifest from Dropbox and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path where the manifest file will be saved.</p> required <code>samples</code> <code>list</code> <p>A list of sample names to download. If None, all samples will be downloaded.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_RADIO(path, samples=None):\n    \"\"\"\n    Downloads the RADIO manifest from Dropbox and saves it to the specified path.\n\n    Args:\n        path (str): The path where the manifest file will be saved.\n        samples (list, optional): A list of sample names to download. If None, all samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    save_dir = Path(path).resolve()\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(\"Downloading RADIO manifest from Dropbox ...\")\n    # Download RADIO data manifest, this is precomputed but any set of GCS dicom files can be used here\n    wget.download(\n        \"https://www.dropbox.com/s/nhh1tb0rclrb7mw/nsclc_radiogenomics.csv?dl=1\",\n        out=f\"{save_dir}/nsclc_radiogenomics.csv\",\n    )\n\n    df = pd.read_csv(f\"{save_dir}/nsclc_radiogenomics.csv\")\n\n    download_from_manifest(df, save_dir, samples)\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.download_from_manifest","title":"<code>download_from_manifest(df, save_dir, samples)</code>","text":"<p>Downloads DICOM data from IDC (Imaging Data Commons) based on the provided manifest.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The manifest DataFrame containing information about the DICOM files.</p> required <code>save_dir</code> <code>Path</code> <p>The directory where the downloaded DICOM files will be saved.</p> required <code>samples</code> <code>int</code> <p>The number of random samples to download. If None, all available samples will be downloaded.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def download_from_manifest(df, save_dir, samples):\n    \"\"\"\n    Downloads DICOM data from IDC (Imaging Data Commons) based on the provided manifest.\n\n    Parameters:\n        df (pandas.DataFrame): The manifest DataFrame containing information about the DICOM files.\n        save_dir (pathlib.Path): The directory where the downloaded DICOM files will be saved.\n        samples (int): The number of random samples to download. If None, all available samples will be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    # Instantiates a client\n    storage_client = storage.Client.create_anonymous_client()\n    logger.info(\"Downloading DICOM data from IDC (Imaging Data Commons) ...\")\n    (save_dir / \"dicom\").mkdir(exist_ok=True, parents=True)\n\n    if samples is not None:\n        assert \"PatientID\" in df.columns\n        rows_with_annotations = df[df[\"Modality\"].isin([\"RTSTRUCT\", \"SEG\"])]\n        unique_elements = rows_with_annotations[\"PatientID\"].unique()\n        selected_elements = np.random.choice(unique_elements, min(len(unique_elements), samples), replace=False)\n        df = df[df[\"PatientID\"].isin(selected_elements)]\n\n    def download_file(row):\n        \"\"\"\n        Download a file from Google Cloud Storage.\n\n        Args:\n            row (dict): A dictionary containing the row data.\n\n        Raises:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        bucket_name, directory, file = row[\"gcs_url\"].split(\"/\")[-3:]\n        fn = f\"{directory}/{file}\"\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(fn)\n\n        current_save_dir = save_dir / \"dicom\" / row[\"PatientID\"] / row[\"StudyInstanceUID\"]\n        current_save_dir.mkdir(exist_ok=True, parents=True)\n        blob.download_to_filename(\n            str(current_save_dir / f'{row[\"Modality\"]}_{row[\"SeriesInstanceUID\"]}_{row[\"InstanceNumber\"]}.dcm')\n        )\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = []\n        for idx, row in df.iterrows():\n            futures.append(executor.submit(download_file, row))\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            pass\n</code></pre>"},{"location":"reference/utils/idc_helper/#fmcib.utils.idc_helper.process_series_dir","title":"<code>process_series_dir(series_dir)</code>","text":"<p>Process the series directory and extract relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>series_dir</code> <code>Path</code> <p>The path to the series directory.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the extracted information, including the   image path, patient ID, and centroid coordinates.</p> <code>None</code> <p>If there's no RTSTRUCT or SEG file, or any step fails.</p> Source code in <code>fmcib/utils/idc_helper.py</code> <pre><code>def process_series_dir(series_dir: Path):\n    \"\"\"\n    Process the series directory and extract relevant information.\n\n    Args:\n        series_dir (Path): The path to the series directory.\n\n    Returns:\n        dict: A dictionary containing the extracted information, including the\n              image path, patient ID, and centroid coordinates.\n        None: If there's no RTSTRUCT or SEG file, or any step fails.\n\n    Raises:\n        None\n    \"\"\"\n    # Check if RTSTRUCT file exists\n    rt_struct_files = list(series_dir.glob(\"*RTSTRUCT*\"))\n    seg_files = list(series_dir.glob(\"*SEG*\"))\n\n    # Convert DICOM to NIfTI based on whether it's RTSTRUCT or SEG\n    if len(rt_struct_files) != 0:\n        dcmrtstruct2nii(str(rt_struct_files[0]), str(series_dir), str(series_dir))\n\n    elif len(seg_files) != 0:\n        dcmseg2nii(str(seg_files[0]), str(series_dir), tag=\"GTV-\")\n\n        # Build the main image NIfTI\n        try:\n            series_id = str(list(series_dir.glob(\"CT*.dcm\"))[0]).split(\"_\")[-2]\n        except IndexError:\n            logger.warning(f\"No 'CT*.dcm' file found under {series_dir}. Skipping.\")\n            return None\n\n        dicom_image = DcmInputAdapter().ingest(str(series_dir), series_id=series_id)\n        nii_output_adapter = NiiOutputAdapter()\n        nii_output_adapter.write(dicom_image, f\"{series_dir}/image\", gzip=True)\n\n    else:\n        logger.warning(f\"No RTSTRUCT or SEG file found in {series_dir}. Skipping.\")\n        return None\n\n    # Read the image (generated above)\n    image_path = series_dir / \"image.nii.gz\"\n    if not image_path.exists():\n        logger.warning(f\"No image file found at {image_path}. Skipping.\")\n        return None\n\n    try:\n        image = sitk.ReadImage(str(image_path))\n    except Exception as e:\n        logger.error(f\"Failed to read image {image_path}: {e}\")\n        return None\n\n    # Find the GTV-1 mask files\n    gtv1_masks = list(series_dir.glob(\"*GTV-1*.nii.gz\"))\n    if not gtv1_masks:\n        logger.warning(f\"No GTV-1 mask found in {series_dir}. Skipping.\")\n        return None\n\n    mask_path = gtv1_masks[0]\n    try:\n        mask = sitk.ReadImage(str(mask_path))\n    except Exception as e:\n        logger.error(f\"Failed to read mask {mask_path}: {e}\")\n        return None\n\n    # Extract centroid from the mask\n    label_shape_filter = sitk.LabelShapeStatisticsImageFilter()\n    label_shape_filter.Execute(mask)\n\n    # Some masks label is 1, others are 255; try 255 first, else 1\n    try:\n        centroid = label_shape_filter.GetCentroid(255)\n    except:\n        try:\n            centroid = label_shape_filter.GetCentroid(1)\n        except Exception as e:\n            logger.warning(f\"Could not extract centroid from mask {mask_path}: {e}\")\n            return None\n\n    x, y, z = centroid\n\n    row = {\n        \"image_path\": str(image_path),\n        \"PatientID\": series_dir.parent.name,\n        \"coordX\": x,\n        \"coordY\": y,\n        \"coordZ\": z,\n    }\n\n    return row\n</code></pre>"},{"location":"reference/visualization/","title":"visualization","text":""},{"location":"reference/visualization/verify_io/","title":"verify_io","text":""},{"location":"reference/visualization/verify_io/#fmcib.visualization.verify_io.visualize_seed_point","title":"<code>visualize_seed_point(row)</code>","text":"<p>This function visualizes a seed point on an image.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row containing the information of the seed point, including the image path and the coordinates. The following columns are expected: \"image_path\", \"coordX\", \"coordY\", \"coordZ\".</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>fmcib/visualization/verify_io.py</code> <pre><code>def visualize_seed_point(row):\n    \"\"\"\n    This function visualizes a seed point on an image.\n\n    Args:\n        row (pandas.Series): A row containing the information of the seed point, including the image path and the coordinates.\n            The following columns are expected: \"image_path\", \"coordX\", \"coordY\", \"coordZ\".\n\n    Returns:\n        None\n    \"\"\"\n    # Define the transformation pipeline\n    is_label_provided = \"label_path\" in row\n    keys = [\"image_path\", \"label_path\"] if is_label_provided else [\"image_path\"]\n    all_keys = keys if is_label_provided else [\"image_path\", \"coordX\", \"coordY\", \"coordZ\"]\n\n    T = monai_transforms.Compose(\n        [\n            monai_transforms.LoadImaged(keys=keys, image_only=True, reader=\"ITKReader\"),\n            monai_transforms.EnsureChannelFirstd(keys=keys),\n            monai_transforms.Spacingd(keys=keys, pixdim=1, mode=\"bilinear\", align_corners=True, diagonal=True),\n            monai_transforms.ScaleIntensityRanged(keys=[\"image_path\"], a_min=-1024, a_max=3072, b_min=0, b_max=1, clip=True),\n            monai_transforms.Orientationd(keys=keys, axcodes=\"LPS\"),\n            monai_transforms.SelectItemsd(keys=all_keys),\n        ]\n    )\n\n    # Apply the transformation pipeline\n    out = T(row)\n\n    # Calculate the center of the image\n    image = out[\"image_path\"]\n    if not is_label_provided:\n        center = (-out[\"coordX\"], -out[\"coordY\"], out[\"coordZ\"])\n        center = np.linalg.inv(np.array(out[\"image_path\"].affine)) @ np.array(center + (1,))\n        center = [int(x) for x in center[:3]]\n\n        # Define the image and label\n        label = torch.zeros_like(image)\n\n        # Define the dimensions of the image and the patch\n        C, H, W, D = image.shape\n        Ph, Pw, Pd = 50, 50, 50\n\n        # Calculate and clamp the ranges for cropping\n        min_h, max_h = max(center[0] - Ph // 2, 0), min(center[0] + Ph // 2, H)\n        min_w, max_w = max(center[1] - Pw // 2, 0), min(center[1] + Pw // 2, W)\n        min_d, max_d = max(center[2] - Pd // 2, 0), min(center[2] + Pd // 2, D)\n\n        # Check if coordinates are valid\n        assert min_h &lt; max_h, \"Invalid coordinates: min_h &gt;= max_h\"\n        assert min_w &lt; max_w, \"Invalid coordinates: min_w &gt;= max_w\"\n        assert min_d &lt; max_d, \"Invalid coordinates: min_d &gt;= max_d\"\n\n        # Define the label for the cropped region\n        label[:, min_h:max_h, min_w:max_w, min_d:max_d] = 1\n    else:\n        label = out[\"label_path\"]\n        center = torch.nonzero(label).float().mean(dim=0)\n        center = [int(x) for x in center][1:]\n\n    # Blend the image and the label\n    ret = blend_images(image=image, label=label, alpha=0.3, cmap=\"hsv\", rescale_arrays=False)\n    ret = ret.permute(3, 2, 1, 0)\n\n    # Plot axial slice\n    plt.figure(figsize=(10, 10))\n    plt.subplot(1, 3, 1)\n    plt.imshow(ret[center[2], :, :])\n    plt.title(\"Axial\")\n    plt.axis(\"off\")\n\n    # Plot sagittal slice\n    plt.subplot(1, 3, 2)\n    plt.imshow(np.flipud(ret[:, center[1], :]))\n    plt.title(\"Coronal\")\n    plt.axis(\"off\")\n\n    # Plot coronal slice\n    plt.subplot(1, 3, 3)\n    plt.imshow(np.flipud(ret[:, :, center[0]]))\n    plt.title(\"Sagittal\")\n\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"replication-guide/analysis/","title":"Reproducing the analysis","text":"<p>We provide features and predictions extracted from each of the models investigated in this study to allow reproduction of our analyses as well as to allow further exploration of the feature space generated by the FM.</p>"},{"location":"replication-guide/analysis/#downloading-the-model-outputs","title":"Downloading the model outputs","text":"<p>All the model outputs are stored on hugging face at https://huggingface.co/surajpaib/FMCIB/tree/main</p> <p>To download these locally, you can run the following <pre><code>pip install -U \"huggingface_hub[cli]\"\n\nhuggingface-cli download surajpaib/fmcib outputs outputs # Run in the root of the repo\n</code></pre></p> <p>This will download all the outputs to a local folder outputs. </p> <p>The contents of the <code>outputs</code> folder are as below,</p> <ul> <li>The <code>features</code> folder contains features extracted from all the feature-based models (our pre-trained FM + SOTA baselines) on each of the three tasks. In addition to the baselines, features for pre-training comparisons performed on task 1 are also available.</li> <li>The <code>predictions</code> folder contains predictions made on each of the three tasks for our foundation model adaptation approaches, supervised baselines and pre-trained SOTA baselines. Predictions are also available for different data percentages used while fine-tuning for task 1 and task 2. These are indicated by a <code>_x_percent</code> suffix to the csv. </li> <li>The <code>stability</code> folder is specific to the analysis of input and test-retest stability and contains features and predictions from the pre-trained FM and our supervised finetuned baseline. </li> </ul>"},{"location":"replication-guide/analysis/#analysis","title":"Analysis","text":"<p>To support complete transparency of our work and allow you to have a better understanding of our analysis pipelines, the provided notebooks, with detailed walk-throughs, will help explore our analysis as well as reproduce the figures that are included in our research paper. </p> <p>The analysis notebooks are organized as follows,</p> <pre><code>analysis\n\u2514\u2500\u2500 task1.ipynb\n\u2514\u2500\u2500 task2.ipynb\n\u2514\u2500\u2500 task3.ipynb\n\u2514\u2500\u2500 stability.ipynb\n</code></pre> <p>Detailed walk-throughs are provided in the notebooks. These notebooks reproduce Figures 2, 3, 4 and several Extended Data Figures. Please ensure you download additional dependenices as described in the data section and that you have downloaded all the outputs from our pipelines.</p>"},{"location":"replication-guide/baselines/","title":"Reproduce Baselines","text":"<p>Reproducing baselines used in this study is very similar to the adaptation of the FM as essentially we are just using the FM weights for adaptation. </p>"},{"location":"replication-guide/baselines/#randomly-initialized","title":"Randomly initialized","text":"<p>We provide the YAML configuration to train the random init baseline at <code>experiments/baselines/supervised_training/supervised_random_init.yaml</code></p> <p>By default, we configure this for Task 1. You can adapt this for Task 2 and Task 3 by searching for <code>Note:</code> comments in the YAML that outline what must be changed. </p> <p>You can start training by running this in the root code folder, <pre><code>lighter fit --config_file=./experiments/baselines/supervised_training/supervised_random_init.yaml\n</code></pre></p>"},{"location":"replication-guide/baselines/#transfer-learning","title":"Transfer learning","text":"<p>We provide the YAML configuration to train the transfer learning baseline at <code>experiments/baselines/supervised_training/supervised_finetune.yaml</code></p> <p>This baseline is only used for Task 2 and Task 3 as we use the random init baseline from Task 1 for the transfer. Follow the <code>Note:</code> comments to switch between Task 2 and Task 3 configurations. </p> <p>You can start training by running this in the root code folder, <pre><code>lighter fit --config_file=./experiments/baselines/supervised_training/supervised_finetune.yaml\n</code></pre></p>"},{"location":"replication-guide/baselines/#med3d-medicalnet","title":"Med3D / MedicalNet","text":"<p>Original repo: https://github.com/Tencent/MedicalNet</p> <p>We have provided re-implementations of Med3D to fit into our YAML workflows at <code>experiments/baselines/med3d/finetune.yaml</code>. Again, the <code>Note:</code> comments help adapt for different tasks. </p> <p>You can start training by running this in the root code folder, <pre><code>lighter fit --config_file=./experiments/baselines/med3d/finetune.yaml\n</code></pre></p>"},{"location":"replication-guide/baselines/#models-genesis","title":"Models Genesis","text":"<p>Original repo: https://github.com/MrGiovanni/ModelsGenesis</p> <p>We have provided re-implementations of Models Genesis to fit into our YAML workflows at <code>experiments/baselines/models_genesis/finetune.yaml</code>. Again, the <code>Note:</code> comments help adapt for different tasks. </p> <p>You can start training by running this in the root code folder, <pre><code>lighter fit --config_file=./experiments/baselines/models_genesis/finetune.yaml\n</code></pre></p>"},{"location":"replication-guide/data/","title":"Data Download and Preprocessing","text":""},{"location":"replication-guide/data/#data","title":"Data","text":"<p>The majority of the datasets utilized in this study are openly accessible for both training and validation purposes: </p> DeepLesion   DeepLesion is a dataset comprising 32,735 lesions from 10,594 studies of 4,427 unique patients collected over two decades from the National Institute of Health Clinical Center PACS server. Various lesions, including kidney, bone, and liver lesions - as well as enlarged lymph nodes and lung nodules, are annotated. The lesions are identified through radiologist-bookmarked RECIST diameters across 32,120 CT slice . In our study we use this dataset both for our pre-training and use-case 1   LUNA16    LUNA16 is a curated version of the LIDC-IDRI dataset of 888 diagnostic and lung cancer screening thoracic CT scans obtained from seven academic centers and eight medical imaging companies comprising 1,186 nodules. The nodules are accompanied by annotations agreed upon by at least 3 out of 4 radiologists. Alongside nodule location annotations, radiologists also noted various observed attributes like internal composition, calcification, malignancy, suspiciousness, and more. We use this dataset to develop and validate our diagnostic image biomarker   LUNG1    LUNG1 is a cohort of 422 patients with stage I-IIIB NSCLC treated with radiation therapy at MAASTRO Clinic, Maastricht, The Netherlands. FDG PET-CT scans were acquired with or without contrast on the Siemens Biograph Scanner. Radiation oncologists used PET and CT images to delineate the gross tumor volume. Our prognostic image biomarker is validated using this cohort.    RADIO    RADIO (NSCLC-Radiogenomics)  dataset is a collection of 211 NSCLC stage I-IV patients who were referred for surgical treatment and underwent preoperative CT and PET/CT scans. These patients were recruited from the Stanford University School of Medicine and the Palo Alto Veterans Affairs Healthcare System. Scan scans were obtained using various scanners and protocols depending on the institution and physician. A subset of 144 patients in the cohort has available tumor segmentations independently reviewed by two thoracic radiologists. In addition to imaging data, the dataset includes molecular data from EGFR, KRAS, ALK mutational testing, gene expression microarrays, and RNA sequencing. We use this dataset for validation the performance of our prognostic biomarker and also for our biological analysis.   <p></p> <p>Note</p> <p>The training dataset for our prognostic biomarker model, HarvardRT, is internal and unavailable to the public. Nonetheless, our foundational model can be publicly accessed, and the results reproduced using the accessible test datasets. </p> <p>Along with this codebase, we provide scripts and code to easily download and pre-process this dataset to encourage reproduction of our study. This README contains detailed information on how to do so. </p>"},{"location":"replication-guide/data/#installing-additional-packages","title":"Installing additional packages","text":"<p>A few additional dependencies are required in order to download and preprocess data (which are not included in the package we have provided). You can install these using: <pre><code>pip install -r additional_requirements.txt\n</code></pre></p>"},{"location":"replication-guide/data/#downloading-the-datasets","title":"Downloading the datasets","text":"<p>All of the datasets can be downloaded using scripts provided in <code>data/download</code>. Note that these scripts are provided for a linux environment but can be simply adapted to other environments by downloading equivalent packages. </p> <p>Additionally, for the LUNG1 and RADIO datasets, we provide an end-to-end way to download and process them using the Imaging Data Commons infrastructure through Google Colab notebooks. This is preferred over the local install highlighted below</p>"},{"location":"replication-guide/data/#downloading-the-deeplesion-and-luna16-datasets","title":"Downloading the DeepLesion and LUNA16 datasets","text":"<p>Note</p> <p>Conda is required to perform this. Make sure you have enough space in the location you are downloading (~250GB for DeepLesion, ~60GB for LUNA16)</p> <p>For DeepLesion, <pre><code>cd data/download\nbash deeplesion.sh &lt;path_to_download&gt;\n</code></pre></p> <p>For LUNA16, <pre><code>cd data/download\nbash luna16.sh &lt;path_to_download&gt;\n</code></pre></p>"},{"location":"replication-guide/data/#downloading-the-lung1-and-radio-datasets","title":"Downloading the LUNG1 and RADIO datasets","text":"<p>The easiest way to download the LUNG1 and RADIO datasets is through s5cmd and IDC manifests For convenience, the manifests for each of the already been provided in <code>data/download</code> under <code>nsclc_radiomics.csv</code> for LUNG1 and <code>nsclc_radiogenomics.</code></p> <p>First, you'll need to install <code>s5cmd</code>. Follow the instructions [here]https://github.com/peak/s5cmd?tab=readme-ov-file#installation</p> <p>Once you have s5cmd installed, run </p> <pre><code>cd data/download\nbash lung1.sh &lt;path_to_download&gt;\n</code></pre> <p>Similarly for RADIO, <pre><code>cd data/download\nbash radio.sh &lt;path_to_download&gt;\n</code></pre> This should download all the DICOM files for the scan along with the annotations in the formats available. </p> <p>Note</p> <p>If you are using a non-Linux machine, you will can follow instructions you can use the \"command\" column of the csvs files to manually download the data as the bash script will not work there. </p>"},{"location":"replication-guide/data/#preprocessing-the-datasets","title":"Preprocessing the datasets","text":"<p>We provide simple linux shell scripts to reproduce the pre-processing pipeline. Incase, you have a different operating system, simply run the python scripts in these shell scripts individually in your environment.</p>"},{"location":"replication-guide/data/#pre-processing-the-deeplesion-dataset","title":"Pre-processing the DeepLesion dataset","text":"<p>Note: Conda is required to perform this <pre><code>cd data/preprocessing/deeplesion\nbash run.sh &lt;path_to_download&gt;\n</code></pre></p> <p>Once you run this successfully, you should see a file <code>data/processing/deeplesion/annotations/deeplesion_training_annotations.csv</code>. At this point you can run the notebook <code>data/processing/deeplesion/Process_Dataset.ipynb</code> to get the splits we use in our paper. For reference, we have already provided splits for comparison as generated by us.</p> <p>Note</p> <p>The pre-processing extracts the image files from zip files. Please delete the zip files from the path <code>&lt;path_to_download&gt;/DeepLesion/Images_png</code> using <code>rm &lt;path_to_download&gt;/DeepLesion/Images_png/*.zip</code>  path after these scripts are successfully run to not inflate your disk space.</p>"},{"location":"replication-guide/data/#pre-processing-the-luna16-dataset","title":"Pre-processing the LUNA16 dataset","text":"<pre><code>cd data/preprocessing/luna16\nbash run.sh &lt;path_to_download&gt;\n</code></pre> <p>Note</p> <p>The pre-processing extracts the image files from zip files. Please delete the zip files from the path <code>&lt;path_to_download&gt;/LUNA16</code> using <code>rm &lt;path_to_download&gt;/LUNA16/*.zip</code> after these scripts are successfully run to not inflate your disk space.</p> <p>Once you run this successfully, you should see a file <code>data/processing/deeplesion/annotations/luna16_training_annotations.csv</code>. At this point you can run the notebook <code>data/processing/deeplesion/Process_Dataset.ipynb</code> to get the splits we use in our paper. For reference, we have already provided splits for comparison as generated by us.</p>"},{"location":"replication-guide/data/#pre-processing-the-lung1-and-radio-dataset","title":"Pre-processing the LUNG1 and RADIO dataset","text":"<p>The directories <code>data/preprocessing/LUNG1</code> and <code>data/preprocessing/RADIO</code> contain Jupyter notebooks that provide instructions on how to pre-process the downloaded data from the above step to a csv format that is the central data format used in this study. </p>"},{"location":"replication-guide/download_models/","title":"Download Trained Models","text":"<p>All our models are will be made available to the public through Zenodo upon publication. Currently, we release these using Dropbox for the reviewers to use and test. Scripts for downloading these models are present under <code>models</code>. </p> <p>As part of our study we develop and share the following,</p>"},{"location":"replication-guide/download_models/#self-supervised-pre-training-model","title":"Self-supervised pre-training model","text":"<p>We developed the pretrained model using the DeepLesion dataset with 11,467 annotated CT lesions identified from 2,312 unique patients. Lesion findings were diverse and included multiple lesions, such as lung nodules, cysts, and breast lesions, among numerous others. A task-agnostic contrastive learning strategy was used to pre-train the model on these lesion findings. Refer to the methods section for more information or the reproducing our models section.</p> <p>To download these models run, <pre><code>cd models\nbash download_foundation_pretrained_model.sh\n</code></pre></p> <p>You can also extract the dropbox links and place them in the target location mentioned.</p> <p>The pre-trained model is implemented on downstreams task using supervised training or linear evaluation approaches. For these we develop,</p>"},{"location":"replication-guide/download_models/#supervised-models","title":"Supervised models","text":"<p>We developed several baseline training approaches,</p>  Supervised model trained from random initialization   Fine-tuning a trained supervised model   Fine-tuning a pre-trained foundation model  <p>To download these models run, <pre><code>cd models\nbash download_task1_baselines.sh\nbash download_task2_baselines.sh\nbash download_task3_baselines.sh\n</code></pre></p>"},{"location":"replication-guide/download_models/#linear-logistic-regression-models","title":"Linear (Logistic Regression) models","text":"<p>Our linear model takes features extracted from the pre-trained foundation model and builds a logistic regression classifer to predict outcome. </p> <p>\u2003 To download these models run, <pre><code>cd models\nbash download_linear_models.sh\n</code></pre></p> <p>These models can also be found at this link. In addition to providing our models, we also provide comprehensive documentation and ongoing support to users through project-lighter to reproduce our results and workflows.</p>"},{"location":"replication-guide/fm_adaptation/","title":"Adaptation of the FM","text":"<p>The FM was adapted by either fine-tuning all its weights or by freezing its weights and adding a linear model on top. </p>"},{"location":"replication-guide/fm_adaptation/#adaptation-through-fine-tuning","title":"Adaptation through fine-tuning","text":"<p>We provide the YAML configuration for this at <code>experiments/adaptation/fmcib_finetune.yaml</code>.</p> <p>By default, we configure this for Task 1. You can adapt this for Task 2 and Task 3 by searching for <code>Note:</code> comments in the YAML that outline what must be changed. Make sure you download the weights for the pre-trained foundation model before attempting to reproduce this training. </p> <p>You can start training by running this in the root code folder, <pre><code>lighter fit --config_file=./experiments/adaptation/fmcib_finetune.yaml\n</code></pre></p>"},{"location":"replication-guide/fm_adaptation/#adaptation-through-linear-evaluation","title":"Adaptation through linear evaluation","text":"<p>4096 features from the FM were for each data point and used to train a logistic regression model using the scikit-learn framework. A comprehensive parameter search for the logistic regression model was performed using the Optuna hyper-parameter optimization framework. The code and utilities for performing the logistic regression modelling is provided in <code>experiments/adaptation/linear</code></p> <p>In order to perform the modelling, you can run  <pre><code>cd experiments/adaptation\npython run.py &lt;features_folder&gt; &lt;label&gt;\n</code></pre></p> <p>The  must contain <code>train_features.csv</code>, <code>val_features.csv</code> and <code>test_features.csv</code> all extracted from our foundation model. The process to extract features from our foundation model is highlighted in this section <p>The  corresponds to the column in the csv files that contains the supervised label to predict. For example, in use-case 2 the label is <code>malignancy</code>.  <p>You can also provide scoring metrics, for instance,  using <code>--scoring roc_auc</code> where the scoring metric is a sklearn scorer. You can also provide the number of trials the optimization framework needs to be run for using <code>--trials</code>. </p> <p>The features folder is provided under <code>outputs/foundation_features</code> to try our the modelling process. Refer here</p>"},{"location":"replication-guide/inference/","title":"Model Inference","text":"<p>In this section, we detail how features (from the FM and pre-trained models) and predictions (from supervised models) can be extracted. </p>"},{"location":"replication-guide/inference/#extracting-features-from-fm-pre-trained-models","title":"Extracting features from FM / pre-trained models","text":"<p>In order to extract features from our models, you can use the following, (at root folder location) <pre><code>lighter predict --config_file=./experiments/inference/extract_features.yaml\n</code></pre></p> <p>Note</p> <p>While the above pipeline will allow you to extract features, we provide an easier and simpler, recommended API to do this. Please refer to Quick Start or Cloud Quick Start</p> <p>However, this method might be preferred when features need to be extracted from different models (used as baselines in our study). Follow the <code>Note:</code> in the corresponding config file to change model paths and use different baselines tested.</p>"},{"location":"replication-guide/inference/#running-predictions-from-our-supervised-models-finetuned-fm-baselines","title":"Running predictions from our supervised models (Finetuned FM/ Baselines)","text":"<p>To run predictions from our models (both supervised and self-supervised), we provide YAML files that can be run with the lighter interface. These are found in <code>experiments/inference</code>, namely <code>get_predictions.yaml</code> for getting the predictions. </p> <p>Beofre running the predictions config, if you haven't downloaded the <code>models</code> folder contraining all our baselines, you can do so using </p> <pre><code>pip install -U \"huggingface_hub[cli]\"\n\nhuggingface-cli download surajpaib/fmcib models models # Run in the root of the repo\n</code></pre> <p>This will pull all the models from hugging face. Following this you can use any of these models to get predictions on the dataset of choice. </p> <p>These can be run using (at root folder location)</p> <p><pre><code>lighter predict --config_file=./experiments/inference/get_predictions.yaml\n</code></pre> As with the previous YAMLS, please follow the 'Note:' tags to place appropriate data paths and change relevant parameters. This YAML is to be used if you want to get target predictions from the models.</p> <p>Note</p> <p>The predictions can be extracted for different tasks as well as different baselines by following the <code>Note:</code> comments. </p>"},{"location":"replication-guide/reproduce_fm/","title":"Reproducing Foundation Model Training","text":""},{"location":"replication-guide/reproduce_fm/#data-setup-for-the-models","title":"Data Setup for the Models","text":"<p>Make sure you download all the datasets before starting to train. If you are using your own datasets, then you will need to format them according to the structure as described to ensure the easiest translation to using our pipeline. </p> <p>Our framework ingests datasets as CSV files with <code>image_path</code> column providing location of the image (on your system) to be used, <code>coordX</code>, <code>coordY</code> and <code>coordZ</code> providing the global coordinates of the seed point around which a patch is cropped. </p> <p>We crop a <code>[50, 50, 50]</code> patch around the seed point. Please refer to our paper for more details on this. </p> <p>For supervised fine-tuning, along with these columns, label columns are needed as below, <pre><code>Task 1: Coarse_lesion_type\nTask 2: malignancy\nTask 3: survival\n</code></pre></p>"},{"location":"replication-guide/reproduce_fm/#reproducing-the-fm-pre-training","title":"Reproducing the FM pre-training","text":"<p>The crux of our study is the self-supervised/weakly supervised pre-training procedure. We implemented contrastive pre-training using a modified version of the SimCLR framework. The SimCLR framework's general principle involves transforming a single data piece (e.g., a patch taken from a CT scan) into two correlated and augmented samples (e.g., the same patch rotated 15 degrees clockwise and flipped horizontally). A convolutional encoder is then used to extract latent representations from these samples. Through a contrastive loss function, the model learns to identify similar representations from the same data sample and dissimilar representations from different data samples. The framework emphasizes effective transformation choices, convolutional encoder architectures, and contrastive loss functions for optimal self-supervised learning performance. To effectively represent the nature of medical images, we made modifications to each of these components. </p> <ol> <li>Medical image specific transformations implemented from Project-MONAI and custom implementations at <code>fmcib.ssl.transforms</code></li> <li>3D ResNet from Project-MONAI</li> <li>Custom implemented modified loss function and SimCLR architecture that can be found under <code>fmcib.ssl.losses.NTXentNegativeMinedLoss</code> and <code>fmcib.ssl.modules.ExNegSimCLR</code></li> </ol> <p>We use project-lighter developed internally within our lab to provide reproducible training for all the models used in this study. Project-lighter allows a YAML-based configuration system along with a python-based CLI to allow quick, easy and scalable experimentation.</p> <p>To pre-train on the DeepLesion pretraining dataset, you can find the YAML for the pre-training at <code>experiments/pretraining/fmcib_pretrain.yaml</code>. It is assumed that you have a GPU available. If you do not (not recommended and not tested and probably takes forever), then edit the following parameters <pre><code>  accelerator: cpu\n  strategy: auto\n  devices: 1\n</code></pre></p> <p>The default training assumes 2 GPUs as mentioned in the paper. You can change this by setting to your desired number of GPUs here: <pre><code>devices: 1\n</code></pre></p> <p>Change the path of the train dataset to the pre-train set generated earlier in the data-preprocessing step. Read the data section for more information.</p> <pre><code>datasets:\n  train:\n    _target_: fmcib.ssl.datasets.SSLRadiomicsDataset\n    path: \"your_pretrain_set_path_goes_here\" \n</code></pre> <p>Now you can start training by running this in the root code folder,</p> <pre><code>lighter fit --config_file=./experiments/pretraining/fmcib_pretrain.yaml\n</code></pre>"}]}